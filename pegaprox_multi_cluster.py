#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PegaProx Server - Cluster Management Backend for Proxmox VE
Version: 0.6.4 Beta

Copyright (C) 2025-2026 PegaProx Team

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program. If not, see <https://www.gnu.org/licenses/>.

═══════════════════════════════════════════════════════════════════════════════

Dev Team:
  NS - Nico Schmidt (Lead)
  MK - Marcus Kellermann (Backend)  
  LW - Laura Weber (Frontend, but helps here too sometimes)

Contributors:
  Florian Paul Azim Hoberg @gyptazy

Started as a weekend project in Sept 2025, got way out of hand lol
Now its a proper multi-cluster thing

IMPORTANT STUFF:
- Gevent monkey-patch MUST be first import or everything breaks (trust me - NS)
- Dont touch the session code unless you want to debug for 3 hours - MK
- The proxmox API is weird sometimes, we just work around it

Credits & Acknowledgments:
- ProxLB by gyptazy (https://github.com/gyptazy/ProxLB)
  The loadbalancing logic that powers our cluster balancing - saved us weeks of work!
- ProxSnap by gyptazy (https://github.com/gyptazy/ProxSnap)
  Inspiration for the snapshot overview feature - great tool for snapshot management

Security:
- Login rate limiting: ✓ (IP + username based)
- Session management: ✓ (auto-expiry, encrypted storage)
- Password hashing: ✓ (Argon2id / PBKDF2 fallback)
- Data encryption: ✓ (AES-256-GCM)
- CSRF/XSS protection: ✓ (CSP headers, secure cookies)

═══════════════════════════════════════════════════════════════════════════════

TODO: switch to FastAPI for async native? - NS (low priority, flask works fine)
TODO: unit tests (yeah we know...) - MK
TODO: CODE SPLITTING before v1.0!! 30k lines in one file is insane - NS
      -> split into: api/, core/, models/, utils/, migrations/
      -> MK will hate me for this but its necessary
DONE: general API rate limiting - LW jan 2026 (env: PEGAPROX_API_RATE_LIMIT)
DONE: disk bus type editing - MK jan 2026 (finally!)
DONE: EFI/TPM management in UI - LW jan 2026
DONE: SSH rate limiting for large clusters - NS jan 2026 (env: PEGAPROX_SSH_MAX_CONCURRENT, default: 25)
      -> Two-tier design: HA operations have PRIORITY (no limit), normal ops are rate-limited
      -> Prevents connection storms during rolling updates across multiple 15+ node clusters
      -> HA fencing/heartbeat can always execute immediately

═══════════════════════════════════════════════════════════════════════════════
"""

# CRITICAL: Gevent MUST be first!! dont move this!! - NS
# wasted 2 hours debugging why nothing worked, turns out the import order matters
import os
import sys

# env var to disable gevent for debugging (set PEGAPROX_NO_GEVENT=1)
USE_GEVENT = os.environ.get('PEGAPROX_NO_GEVENT', '').lower() not in ('1', 'true', 'yes')

if USE_GEVENT:
    try:
        from gevent import monkey
        # MK: thread=True is needed for paramiko or it throws some greenlet error
        monkey.patch_all()
        GEVENT_PATCHED = True
        print("Gevent monkey-patching applied")  # dont remove this, helps with debugging
    except ImportError:
        GEVENT_PATCHED = False
else:
    GEVENT_PATCHED = False

import json
import time
import logging
import threading
import subprocess
import uuid
import socket
import ssl
import base64
import hashlib
import hmac
import multiprocessing
import signal
import gc
import re
import shutil
import sqlite3
import warnings
# import asyncio  # TODO someday - MK

# shut up asyncio
warnings.filterwarnings('ignore', message='coroutine.*was never awaited')
warnings.filterwarnings('ignore', category=RuntimeWarning, module='asyncio')
from flask import Flask, jsonify, request, send_from_directory, Response, make_response, session
from flask_cors import CORS
from flask_sock import Sock
from flask_compress import Compress  # NS: Gzip compression for faster loading
from datetime import datetime, timedelta, date, timezone  # timedelta for session expiry calc, date for password expiry, timezone for snapshots
from pathlib import Path
from typing import Dict, Any, Optional, List
import requests
import urllib3
from urllib.parse import urlparse, urljoin, quote as url_quote, urlencode  # MK: quote/urlencode needed for encoding
import struct
# import redis  # for session storage, not implemented yet
from collections import defaultdict  # for some counters

# Production server imports
GEVENT_AVAILABLE = False
GUNICORN_AVAILABLE = False
GEVENT_POOL = None

try:
    from gevent.pywsgi import WSGIServer
    from gevent.pool import Pool as GeventPool
    GEVENT_AVAILABLE = True
    # Create a pool for concurrent API requests
    GEVENT_POOL = GeventPool(size=50)  # Max 50 concurrent greenlets
except ImportError:
    pass

try:
    import gunicorn
    GUNICORN_AVAILABLE = True
except ImportError:
    pass  # thats fine, we can use gevent or flask dev server

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# TOTP Support
try:
    import pyotp
    import qrcode
    import io
    TOTP_AVAILABLE = True
except ImportError:
    TOTP_AVAILABLE = False
    logging.warning("pyotp/qrcode not installed. 2FA will not be available. Install with: pip install pyotp qrcode[pil]")

# Encryption stuff - MK
# upgraded to AES-256 in jan 2026, old fernet stuff still works tho
try:
    from cryptography.fernet import Fernet
    from cryptography.hazmat.primitives import hashes
    from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
    from cryptography.hazmat.primitives.ciphers.aead import AESGCM
    from cryptography.hazmat.backends import default_backend
    ENCRYPTION_AVAILABLE = True
    LEGACY_ENCRYPTION = True
except ImportError:
    ENCRYPTION_AVAILABLE = False
    LEGACY_ENCRYPTION = False
    logging.warning("cryptography not installed, passwords wont be encrypted!")

# argon2 for pw hashing - way better than pbkdf2
try:
    import argon2
    from argon2 import PasswordHasher
    from argon2.exceptions import VerifyMismatchError
    ARGON2_AVAILABLE = True
except ImportError:
    ARGON2_AVAILABLE = False
    # not a big deal, pbkdf2 fallback works fine

# NS: External Authentication - LDAP support (Feb 2026)
LDAP_AVAILABLE = False
try:
    import ldap
    import ldap.filter
    LDAP_AVAILABLE = True
except ImportError:
    pass  # Optional - LDAP auth won't be available

# NS: External Authentication - OAuth2/OIDC support (Feb 2026)
OIDC_AVAILABLE = False
try:
    from authlib.integrations.requests_client import OAuth2Session
    from authlib.oauth2.rfc7636 import create_s256_code_challenge
    import jwt
    import httpx
    OIDC_AVAILABLE = True
except ImportError:
    pass  # Optional - OAuth2/OIDC auth won't be available

# from functools import lru_cache  # TODO add caching someday
# import aiohttp  # maybe for async stuff later idk

def get_paramiko():
    """lazy import for paramiko, its optional"""
    # MK: paramiko takes forever to import so we only do it when needed
    try:
        import paramiko
        return paramiko
    except ImportError:
        return None


# ============================================
# Concurrent API Helpers - added late 2025
# Use gevent pool for parallel requests when available
# MK: This made the dashboard like 5x faster, totally worth it
# ============================================

def run_concurrent(tasks: list, timeout: float = 30.0) -> list:
    """Run tasks concurrently with gevent pool"""
    # NS: chatgpt helped with this one, i was mass confused about greenlets
    # TODO: maybe add retry logic? - MK
    if not tasks:
        return []
    
    if GEVENT_POOL and GEVENT_AVAILABLE:
        # Use gevent pool for concurrent execution
        try:
            greenlets = [GEVENT_POOL.spawn(task) for task in tasks]
            # Wait for all with timeout
            from gevent import joinall
            joinall(greenlets, timeout=timeout)
            
            results = []
            for g in greenlets:
                try:
                    results.append(g.value if g.successful() else None)
                except Exception as e:
                    logging.error(f"Concurrent task failed: {e}")
                    results.append(None)
            return results
        except Exception as e:
            logging.error(f"Concurrent execution failed: {e}")
            # Fall through to sequential execution
    
    # Fallback: sequential execution (when gevent not available)
    results = []
    for task in tasks:
        try:
            results.append(task())
        except Exception as e:
            logging.error(f"Task failed: {e}")
            results.append(None)
    return results


def run_concurrent_dict(tasks: dict, timeout: float = 30.0) -> dict:
    """Run multiple tasks concurrently, returning results as dict
    
    Args:
        tasks: Dict of {key: callable}
        timeout: Maximum time to wait
    
    Returns:
        Dict of {key: result}
    
    Example:
        results = run_concurrent_dict({
            'node1': lambda: get_node_status('node1'),
            'node2': lambda: get_node_status('node2'),
        })
    """
    if not tasks:
        return {}
    
    keys = list(tasks.keys())
    callables = [tasks[k] for k in keys]
    results = run_concurrent(callables, timeout)
    
    return dict(zip(keys, results))


app = Flask(__name__)

# =============================================================================
# VERSION INFO
# =============================================================================
PEGAPROX_VERSION = "Beta 0.6.4"
PEGAPROX_BUILD = "2026.02.01"  # Year.Month.Day of release

# ============================================
# CORS Configuration (Security)
# ============================================
# For Open Source deployments: CORS is auto-configured based on how you access the server.
# 
# How it works:
# 1. Same-Origin requests (browser loads frontend from same server) → No CORS needed
# 2. First external origin that successfully logs in → Automatically allowed
# 3. Additional origins can be added via environment variable
#
# Manual configuration (optional):
#   export PEGAPROX_ALLOWED_ORIGINS="https://pegaprox.example.com,https://admin.example.com"
#
# Security note: Never use "*" in production - it allows any website to access your API

_cors_origins_env = os.environ.get('PEGAPROX_ALLOWED_ORIGINS', '')
_auto_allowed_origins = set()  # Populated dynamically from valid requests

def get_allowed_origins():
    """Get list of allowed CORS origins (dynamic for Open Source)"""
    origins = set()
    
    # 1. Environment variable origins (highest priority)
    if _cors_origins_env:
        for origin in _cors_origins_env.split(','):
            origin = origin.strip()
            if origin and origin != '*':
                origins.add(origin)
    
    # 2. Auto-detected origins from successful logins
    origins.update(_auto_allowed_origins)
    
    # 3. If nothing configured, allow requests without Origin header (same-origin)
    # This is safe because browsers always send Origin header for cross-origin requests
    if not origins:
        return None  # None = no CORS headers = same-origin only
    
    return list(origins)

def add_allowed_origin(origin: str):
    """Add an origin to the auto-allowed list (called on successful login)"""
    if origin and origin.startswith(('http://', 'https://')) and origin != '*':
        _auto_allowed_origins.add(origin)
        logging.info(f"Auto-allowed CORS origin: {origin}")

# Initial CORS setup - will be dynamically updated
if _cors_origins_env:
    ALLOWED_CORS_ORIGINS = [o.strip() for o in _cors_origins_env.split(',') if o.strip() and o.strip() != '*']
else:
    # Default: allow all for same-origin and local development
    # For production, set PEGAPROX_ALLOWED_ORIGINS env var
    ALLOWED_CORS_ORIGINS = ["*"]

# Standard Flask-CORS setup (this works reliably)
CORS(app, supports_credentials=True, resources={
    r"/api/*": {
        "origins": ALLOWED_CORS_ORIGINS,
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization", "X-Username", "X-Session-Id"],
        "expose_headers": ["Content-Type"],
        "supports_credentials": True
    }
})
sock = Sock(app)

# NS: Gzip compression - reduces transfer size by 70-80%
app.config['COMPRESS_MIMETYPES'] = [
    'text/html', 'text/css', 'text/xml', 'text/plain',
    'application/json', 'application/javascript', 'application/xml'
]
app.config['COMPRESS_LEVEL'] = 6  # Balance between speed and compression
app.config['COMPRESS_MIN_SIZE'] = 500  # Only compress if > 500 bytes
Compress(app)

# =============================================================================
# REQUEST VALIDATION & RATE LIMITING
# MK: added after some script kiddie tried sql injection lol
# NS: added general API rate limiting jan 2026
# =============================================================================
app.config['MAX_CONTENT_LENGTH'] = int(os.environ.get('PEGAPROX_MAX_REQUEST_SIZE', 10 * 1024 * 1024))

# General API rate limiting (separate from login rate limiting)
# Configurable via env vars
API_RATE_LIMIT = int(os.environ.get('PEGAPROX_API_RATE_LIMIT', 300))  # requests per window, bumped from 100
API_RATE_WINDOW = int(os.environ.get('PEGAPROX_API_RATE_WINDOW', 60))  # window in seconds
api_request_counts = {}  # ip -> {'count': int, 'window_start': timestamp}
api_rate_limit_lock = threading.Lock()

def check_api_rate_limit(client_ip: str) -> bool:
    """simple sliding window rate limiter, returns True if allowed"""
    if API_RATE_LIMIT <= 0:  # can be disabled via env var
        return True
    
    current_time = time.time()
    
    with api_rate_limit_lock:
        if client_ip not in api_request_counts:
            api_request_counts[client_ip] = {'count': 1, 'window_start': current_time}
            return True
        
        info = api_request_counts[client_ip]
        
        # Reset window if expired
        if current_time - info['window_start'] > API_RATE_WINDOW:
            info['count'] = 1
            info['window_start'] = current_time
            return True
        
        # Check limit
        if info['count'] >= API_RATE_LIMIT:
            return False
        
        info['count'] += 1
        return True

@app.before_request
def validate_request():
    """request validation + rate limiting"""
    # skip static stuff and websockets
    if request.path.startswith('/static/') or request.path.startswith('/images/'):
        return None
    if request.path.startswith('/ws'):
        return None
    
    # General API rate limiting (skip login - has its own stricter limiting)
    # NS: skip SSE/events and health checks - called way too often otherwise
    # chatgpt helped figure out which endpoints were hammering the limit
    if request.path.startswith('/api/'):
        skip_paths = ['/api/auth/login', '/api/auth/check', '/api/events', '/api/health', '/api/sse']
        if not any(request.path.startswith(p) for p in skip_paths):
            client_ip = request.headers.get('X-Forwarded-For', request.remote_addr)
            if client_ip:
                client_ip = client_ip.split(',')[0].strip()
            
            if not check_api_rate_limit(client_ip):
                logging.warning(f"Rate limit exceeded for {client_ip}")
                return jsonify({
                    'error': 'Rate limit exceeded. Please slow down.',
                    'retry_after': API_RATE_WINDOW
                }), 429
    
    # check content type
    if request.method in ['POST', 'PUT', 'PATCH'] and request.content_length:
        content_type = request.content_type or ''
        allowed_types = ['application/json', 'multipart/form-data', 'application/x-www-form-urlencoded']
        if not any(t in content_type for t in allowed_types):
            if request.content_length > 0:
                return jsonify({'error': 'Invalid Content-Type'}), 415
    
    return None

# security headers - NS
# LW kept bugging me about this after she read some owasp article
@app.after_request
def add_security_headers(response):
    """add security headers"""
    response.headers['X-Content-Type-Options'] = 'nosniff'
    response.headers['X-Frame-Options'] = 'DENY'
    response.headers['X-XSS-Protection'] = '1; mode=block'
    response.headers['Referrer-Policy'] = 'strict-origin-when-cross-origin'
    response.headers['Permissions-Policy'] = 'geolocation=(), microphone=(), camera=()'
    
    # csp - this took ages to get right
    csp = (
        "default-src 'self'; "
        "script-src 'self' 'unsafe-inline' 'unsafe-eval' "
            "https://cdn.jsdelivr.net https://cdn.tailwindcss.com "
            "https://cdnjs.cloudflare.com; "
        "style-src 'self' 'unsafe-inline' "
            "https://fonts.googleapis.com https://cdn.jsdelivr.net; "
        "font-src 'self' data: https://fonts.gstatic.com https://fonts.googleapis.com; "
        "img-src 'self' data: blob:; "
        "connect-src 'self' wss: ws: https://cdn.jsdelivr.net; "
        "frame-ancestors 'none'; "
        "base-uri 'self'; "
        "form-action 'self'"
    )
    response.headers['Content-Security-Policy'] = csp
    
    # hsts - careful, browser remembers this for a year
    if request.is_secure or request.headers.get('X-Forwarded-Proto') == 'https':
        response.headers['Strict-Transport-Security'] = 'max-age=31536000; includeSubDomains'
    
    return response

# ============================================
# File Paths & Directories
# MK: All config in one dir for easy backup
# Sensitive stuff is encrypted (*.enc files)
# moved to SQLite (MK suggested this after JSON corruption issues)
# ============================================
CONFIG_DIR = 'config'
Path(CONFIG_DIR).mkdir(exist_ok=True)

# NS: Set restrictive permissions on config directory
# Only owner can access - contains encryption keys and database
try:
    os.chmod(CONFIG_DIR, 0o700)
except:
    pass  # May fail on Windows, ignore

# Database file (encrypted SQLite)
DATABASE_FILE = os.path.join(CONFIG_DIR, 'pegaprox.db')

# Legacy configuration files (kept for migration)
CONFIG_FILE = os.path.join(CONFIG_DIR, 'clusters.json')  # Legacy unencrypted
CONFIG_FILE_ENCRYPTED = os.path.join(CONFIG_DIR, 'clusters.enc')
KEY_FILE = os.path.join(CONFIG_DIR, '.pegaprox.key')  # Auto-generated encryption key
USERS_FILE_ENCRYPTED = os.path.join(CONFIG_DIR, 'users.enc')
AUDIT_LOG_FILE = os.path.join(CONFIG_DIR, 'audit.log')  # Legacy
AUDIT_LOG_FILE_ENCRYPTED = os.path.join(CONFIG_DIR, 'audit.log.enc')
SESSIONS_FILE = os.path.join(CONFIG_DIR, 'sessions.json')  # Legacy unencrypted
SESSIONS_FILE_ENCRYPTED = os.path.join(CONFIG_DIR, 'sessions.enc')  # NS: now encrypted!
SERVER_SETTINGS_FILE = os.path.join(CONFIG_DIR, 'server_settings.json')
ADMIN_INITIALIZED_FILE = os.path.join(CONFIG_DIR, '.admin_initialized')

# NS: New feature config files - Dec 2025 (legacy, migrated to SQLite)
ALERTS_CONFIG_FILE = os.path.join(CONFIG_DIR, 'alerts.json')
SCHEDULED_TASKS_FILE = os.path.join(CONFIG_DIR, 'scheduled_tasks.json')
VM_TAGS_FILE = os.path.join(CONFIG_DIR, 'vm_tags.json')
AFFINITY_RULES_FILE = os.path.join(CONFIG_DIR, 'affinity_rules.json')
MIGRATION_HISTORY_FILE = os.path.join(CONFIG_DIR, 'migration_history.json')

# Other directories
SSL_CERT_FILE = 'ssl/cert.pem'
SSL_KEY_FILE = 'ssl/key.pem'
LOG_DIR = 'logs'
WEB_DIR = 'web'
SSL_DIR = 'ssl'

# Session configuration
SESSION_TIMEOUT = 28800  # 8 hours in seconds (HIPAA compliant default, can be configured)


# ============================================
# SQLITE DATABASE WITH ENCRYPTION
# replaces the old JSON mess
# MK: This was a big refactor but worth it - no more JSON corruption issues
# ============================================

class PegaProxDB:
    """
    SQLite database wrapper - MK
    
    switched from json files because they kept corrupting when multiple
    requests came in. wasted a whole weekend on that shit lol
    
    sensitive stuff (passwords etc) is encrypted, rest is plain text
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        # singleton - only one db connection
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.db_path = DATABASE_FILE
        self.fernet = None  # old encryption, keep for migration
        self.aesgcm = None  # new aes256
        self.aes_key = None  # raw key for HMAC signing
        self._conn = None
        self._local = threading.local()
        
        self._init_encryption()
        self._init_db()
        self._migrate_from_legacy()
        
        self._initialized = True
        logging.info(f"DB initialized: {self.db_path}")
    
    def _init_encryption(self):
        """setup encryption keys"""
        # MK: upgraded to aes256 in jan 2026, old fernet stuff still works
        if not ENCRYPTION_AVAILABLE:
            logging.warning("no encryption available!")
            return
        
        # AES-256 key file
        aes_key_file = os.path.join(CONFIG_DIR, '.pegaprox_aes256.key')
        
        # Load or generate AES-256 key
        if os.path.exists(aes_key_file):
            with open(aes_key_file, 'rb') as f:
                aes_key = f.read()
            if len(aes_key) != 32:
                logging.warning("Invalid AES key length, regenerating...")
                aes_key = os.urandom(32)  # 256 bits
                with open(aes_key_file, 'wb') as f:
                    f.write(aes_key)
        else:
            # Generate new 256-bit key
            aes_key = os.urandom(32)
            with open(aes_key_file, 'wb') as f:
                f.write(aes_key)
            try:
                os.chmod(aes_key_file, 0o600)
            except:
                pass
            logging.info("Generated new AES-256-GCM encryption key (Military Grade)")
        
        self.aesgcm = AESGCM(aes_key)
        self.aes_key = aes_key  # Store raw key for HMAC signing
        
        # Load legacy Fernet key for backwards compatibility
        if os.path.exists(KEY_FILE):
            try:
                with open(KEY_FILE, 'rb') as f:
                    fernet_key = f.read()
                self.fernet = Fernet(fernet_key)
                logging.debug("Loaded legacy Fernet key for migration support")
            except Exception as e:
                logging.warning(f"Could not load legacy Fernet key: {e}")
        else:
            # Generate Fernet key for potential fallback
            fernet_key = Fernet.generate_key()
            with open(KEY_FILE, 'wb') as f:
                f.write(fernet_key)
            try:
                os.chmod(KEY_FILE, 0o600)
            except:
                pass
            self.fernet = Fernet(fernet_key)
            logging.info("Generated legacy Fernet key (for compatibility)")
    
    def _get_connection(self):
        """Get thread-local database connection
        
        NS: Using thread-local storage because SQLite connections
        shouldn't be shared across threads. Each thread gets its own.
        """
        if not hasattr(self._local, 'conn') or self._local.conn is None:
            self._local.conn = sqlite3.connect(
                self.db_path,
                check_same_thread=False,  # We handle thread safety ourselves
                timeout=30.0
            )
            self._local.conn.row_factory = sqlite3.Row
            # Enable foreign keys
            self._local.conn.execute("PRAGMA foreign_keys = ON")
            # WAL mode for better concurrency (multiple readers, one writer)
            self._local.conn.execute("PRAGMA journal_mode = WAL")
        return self._local.conn
    
    @property
    def conn(self):
        return self._get_connection()
    
    def _init_db(self):
        """Initialize database schema
        
        NS: Also sets restrictive file permissions (0600) on the database file.
        This prevents other users on the system from reading the DB.
        """
        conn = self.conn
        cursor = conn.cursor()
        
        # NS: Set restrictive permissions on DB file - only owner can read/write
        # This is critical security - DB contains encrypted secrets and session data
        try:
            if os.path.exists(self.db_path):
                os.chmod(self.db_path, 0o600)
                logging.debug(f"Set database file permissions to 0600")
        except Exception as e:
            logging.warning(f"Could not set database file permissions: {e}")
        
        # Clusters table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS clusters (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                host TEXT NOT NULL,
                user TEXT NOT NULL,
                pass_encrypted TEXT NOT NULL,
                ssl_verification INTEGER DEFAULT 1,
                migration_threshold INTEGER DEFAULT 30,
                check_interval INTEGER DEFAULT 300,
                auto_migrate INTEGER DEFAULT 0,
                balance_containers INTEGER DEFAULT 0,
                balance_local_disks INTEGER DEFAULT 0,
                dry_run INTEGER DEFAULT 1,
                enabled INTEGER DEFAULT 1,
                ha_enabled INTEGER DEFAULT 0,
                fallback_hosts TEXT DEFAULT '[]',
                ssh_user TEXT DEFAULT '',
                ssh_key_encrypted TEXT DEFAULT '',
                ssh_port INTEGER DEFAULT 22,
                ha_settings TEXT DEFAULT '{}',
                created_at TEXT,
                updated_at TEXT
            )
        ''')
        
        # Users table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS users (
                username TEXT PRIMARY KEY,
                password_salt TEXT NOT NULL,
                password_hash TEXT NOT NULL,
                role TEXT DEFAULT 'viewer',
                permissions TEXT DEFAULT '[]',
                tenant TEXT,
                created_at TEXT,
                last_login TEXT,
                password_expiry TEXT,
                totp_secret_encrypted TEXT,
                totp_pending_secret_encrypted TEXT,
                totp_enabled INTEGER DEFAULT 0,
                force_password_change INTEGER DEFAULT 0,
                enabled INTEGER DEFAULT 1,
                theme TEXT DEFAULT '',
                language TEXT DEFAULT '',
                ui_layout TEXT DEFAULT 'modern'
            )
        ''')
        
        # Sessions table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS sessions (
                token TEXT PRIMARY KEY,
                username TEXT NOT NULL,
                created_at TEXT,
                expires_at TEXT,
                ip_address TEXT,
                user_agent TEXT
            )
        ''')
        
        # Audit log table with HMAC integrity verification
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS audit_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                user TEXT,
                action TEXT NOT NULL,
                details TEXT,
                ip_address TEXT,
                hmac_signature TEXT
            )
        ''')
        
        # Create index for audit log queries
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_audit_timestamp ON audit_log(timestamp DESC)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_audit_user ON audit_log(user)
        ''')
        
        # Alerts table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS alerts (
                id TEXT PRIMARY KEY,
                cluster_id TEXT,
                node TEXT,
                vmid INTEGER,
                type TEXT NOT NULL,
                threshold REAL,
                enabled INTEGER DEFAULT 1,
                notify_methods TEXT DEFAULT '[]',
                cooldown INTEGER DEFAULT 300,
                last_triggered TEXT,
                created_at TEXT
            )
        ''')
        
        # VM ACLs table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS vm_acls (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT NOT NULL,
                vmid TEXT NOT NULL,
                users TEXT DEFAULT '[]',
                permissions TEXT DEFAULT '[]',
                UNIQUE(cluster_id, vmid)
            )
        ''')
        
        # Affinity rules table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS affinity_rules (
                id TEXT PRIMARY KEY,
                cluster_id TEXT NOT NULL,
                name TEXT NOT NULL,
                type TEXT NOT NULL,
                vms TEXT NOT NULL DEFAULT '[]',
                enabled INTEGER DEFAULT 1,
                created_at TEXT
            )
        ''')
        
        # Tenants - requested on reddit
        # Someone on Reddit asked for multi-tenancy support, turns out its 
        # pretty useful for MSPs managing multiple customers
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS tenants (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                clusters TEXT DEFAULT '[]',
                created_at TEXT
            )
        ''')
        
        # Cluster Groups - organize clusters into collapsible groups with tenant assignment
        # NS: Jan 2026 - requested by user for better organization
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cluster_groups (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT DEFAULT '',
                color TEXT DEFAULT '#E86F2D',
                tenant_id TEXT,
                sort_order INTEGER DEFAULT 0,
                collapsed INTEGER DEFAULT 0,
                created_at TEXT,
                updated_at TEXT,
                FOREIGN KEY (tenant_id) REFERENCES tenants(id)
            )
        ''')
        
        # Custom roles table - need composite key for name + tenant_id
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS custom_roles (
                name TEXT NOT NULL,
                permissions TEXT NOT NULL DEFAULT '[]',
                description TEXT,
                tenant_id TEXT,
                created_at TEXT,
                PRIMARY KEY (name, tenant_id)
            )
        ''')
        
        # Migration: Recreate table with correct schema if needed
        try:
            cursor.execute("SELECT tenant_id FROM custom_roles LIMIT 1")
        except:
            # Old table without tenant_id - recreate
            cursor.execute("DROP TABLE IF EXISTS custom_roles")
            cursor.execute('''
                CREATE TABLE custom_roles (
                    name TEXT NOT NULL,
                    permissions TEXT NOT NULL DEFAULT '[]',
                    description TEXT,
                    tenant_id TEXT,
                    created_at TEXT,
                    PRIMARY KEY (name, tenant_id)
                )
            ''')
        
        # Scheduled tasks table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scheduled_tasks (
                id TEXT PRIMARY KEY,
                cluster_id TEXT,
                name TEXT NOT NULL,
                task_type TEXT NOT NULL,
                schedule TEXT NOT NULL,
                config TEXT DEFAULT '{}',
                enabled INTEGER DEFAULT 1,
                last_run TEXT,
                next_run TEXT,
                created_at TEXT
            )
        ''')
        
        # VM Tags table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS vm_tags (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT NOT NULL,
                vmid INTEGER NOT NULL,
                tag_name TEXT NOT NULL,
                tag_color TEXT,
                UNIQUE(cluster_id, vmid, tag_name)
            )
        ''')
        
        # Balancing excluded VMs table - MK Jan 2026
        # VMs that should not be automatically migrated during load balancing
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS balancing_excluded_vms (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT NOT NULL,
                vmid INTEGER NOT NULL,
                reason TEXT,
                created_by TEXT,
                created_at TEXT,
                UNIQUE(cluster_id, vmid)
            )
        ''')
        
        # Migration history table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS migration_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT NOT NULL,
                vmid INTEGER NOT NULL,
                vm_name TEXT,
                source_node TEXT NOT NULL,
                target_node TEXT NOT NULL,
                reason TEXT,
                status TEXT,
                duration_seconds REAL,
                timestamp TEXT NOT NULL
            )
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_migration_timestamp ON migration_history(timestamp DESC)
        ''')
        
        # Server settings table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS server_settings (
                key TEXT PRIMARY KEY,
                value TEXT
            )
        ''')
        
        # User favorites table - NS Jan 2026
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS user_favorites (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                username TEXT NOT NULL,
                cluster_id TEXT,
                vmid INTEGER,
                vm_type TEXT,
                vm_name TEXT,
                added_at TEXT
            )
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_favorites_user ON user_favorites(username)
        ''')
        
        # Scheduled actions table - NS Jan 2026
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scheduled_actions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT,
                vmid INTEGER,
                action TEXT NOT NULL,
                schedule_type TEXT NOT NULL,
                schedule_time TEXT,
                schedule_days TEXT,
                schedule_date TEXT,
                enabled INTEGER DEFAULT 1,
                last_run TEXT,
                created_by TEXT,
                created_at TEXT
            )
        ''')
        
        # Update schedules table - MK Jan 2026
        # For automatic rolling updates
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS update_schedules (
                cluster_id TEXT PRIMARY KEY,
                enabled INTEGER DEFAULT 0,
                schedule_type TEXT DEFAULT 'recurring',
                day TEXT DEFAULT 'sunday',
                time TEXT DEFAULT '03:00',
                include_reboot INTEGER DEFAULT 1,
                skip_evacuation INTEGER DEFAULT 0,
                skip_up_to_date INTEGER DEFAULT 1,
                evacuation_timeout INTEGER DEFAULT 1800,
                last_run TEXT,
                next_run TEXT,
                created_by TEXT,
                created_at TEXT,
                updated_at TEXT
            )
        ''')
        
        # Metrics history table - NS Jan 2026
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS metrics_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                data TEXT NOT NULL
            )
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_metrics_timestamp ON metrics_history(timestamp DESC)
        ''')
        
        # Custom Scripts table - MK Jan 2026
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS custom_scripts (
                id TEXT PRIMARY KEY,
                cluster_id TEXT NOT NULL,
                name TEXT NOT NULL,
                description TEXT DEFAULT '',
                type TEXT DEFAULT 'bash',
                content TEXT NOT NULL,
                target_nodes TEXT DEFAULT 'all',
                enabled INTEGER DEFAULT 1,
                last_run TEXT,
                last_status TEXT,
                created_at TEXT,
                updated_at TEXT
            )
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_scripts_cluster ON custom_scripts(cluster_id)
        ''')
        
        # NS: Additional tables for full JSON migration - Jan 2026
        # MK: finally got around to migrating all the random json files to sqlite
        # took way longer than expected but now everything is in one place
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cluster_alerts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT NOT NULL,
                alert_type TEXT NOT NULL,
                config TEXT DEFAULT '{}',
                enabled INTEGER DEFAULT 1,
                created_at TEXT,
                updated_at TEXT,
                UNIQUE(cluster_id, alert_type)
            )
        ''')
        
        # LW: ESXi integration was a pain, but people kept asking for it
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS esxi_storages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL UNIQUE,
                host TEXT NOT NULL,
                username TEXT,
                password_encrypted TEXT,
                datastore TEXT,
                enabled INTEGER DEFAULT 1,
                last_sync TEXT,
                config TEXT DEFAULT '{}'
            )
        ''')
        
        # NS: storage clusters for ceph/gluster/zfs pools shared across nodes
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS storage_clusters (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT NOT NULL,
                name TEXT NOT NULL,
                storage_type TEXT DEFAULT 'ceph',
                nodes TEXT DEFAULT '[]',
                config TEXT DEFAULT '{}',
                enabled INTEGER DEFAULT 1,
                UNIQUE(cluster_id, name)
            )
        ''')
        
        # MK: Pool Permissions - Jan 2026
        # Store permissions for Proxmox resource pools
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pool_permissions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT NOT NULL,
                pool_id TEXT NOT NULL,
                subject_type TEXT NOT NULL,
                subject_id TEXT NOT NULL,
                permissions TEXT DEFAULT '[]',
                created_at TEXT,
                updated_at TEXT,
                UNIQUE(cluster_id, pool_id, subject_type, subject_id)
            )
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_pool_perms_cluster ON pool_permissions(cluster_id)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_pool_perms_pool ON pool_permissions(cluster_id, pool_id)
        ''')

        # NS: External Authentication tables - Feb 2026
        # Support for LDAP (AD + OpenLDAP) and OAuth2/OIDC providers

        # Auth providers table - stores LDAP and OIDC provider configurations
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS auth_providers (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                type TEXT NOT NULL,
                enabled INTEGER DEFAULT 1,
                priority INTEGER DEFAULT 100,
                config_encrypted TEXT NOT NULL,
                default_role TEXT DEFAULT 'viewer',
                auto_create_users INTEGER DEFAULT 0,
                created_at TEXT,
                updated_at TEXT
            )
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_auth_providers_type ON auth_providers(type)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_auth_providers_enabled ON auth_providers(enabled)
        ''')

        # Group-to-role mappings for external providers
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS auth_group_mappings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                provider_id TEXT NOT NULL,
                external_group TEXT NOT NULL,
                role TEXT NOT NULL,
                tenant_id TEXT DEFAULT '_default',
                priority INTEGER DEFAULT 100,
                created_at TEXT,
                FOREIGN KEY (provider_id) REFERENCES auth_providers(id) ON DELETE CASCADE,
                UNIQUE(provider_id, external_group, tenant_id)
            )
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_group_mappings_provider ON auth_group_mappings(provider_id)
        ''')

        # User external identities - links PegaProx users to external IDs
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS user_external_identities (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                username TEXT NOT NULL,
                provider_id TEXT NOT NULL,
                external_id TEXT NOT NULL,
                external_username TEXT,
                external_email TEXT,
                external_groups TEXT DEFAULT '[]',
                last_sync TEXT,
                created_at TEXT,
                FOREIGN KEY (provider_id) REFERENCES auth_providers(id) ON DELETE CASCADE,
                UNIQUE(provider_id, external_id)
            )
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_external_identities_user ON user_external_identities(username)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_external_identities_provider ON user_external_identities(provider_id)
        ''')

        # Schema migrations for existing databases
        # Add password_salt column if it doesn't exist (for databases created before this fix)
        try:
            cursor.execute("PRAGMA table_info(users)")
            columns = [col[1] for col in cursor.fetchall()]
            
            if 'password_salt' not in columns:
                logging.info("Adding password_salt column to users table...")
                try:
                    cursor.execute("ALTER TABLE users ADD COLUMN password_salt TEXT DEFAULT ''")
                    logging.info("Added password_salt column to users table")
                    
                    # Force re-migration of users to populate password_salt
                    logging.info("Will re-migrate users from legacy files...")
                    conn.commit()
                    self._force_remigrate_users = True
                except Exception as e:
                    logging.error(f"Failed to add password_salt column: {e}")
            
            # user prefs columns
            if 'theme' not in columns:
                logging.info("Adding theme column to users table...")
                try:
                    cursor.execute("ALTER TABLE users ADD COLUMN theme TEXT DEFAULT ''")
                    logging.info("Added theme column to users table")
                except Exception as e:
                    logging.error(f"Failed to add theme column: {e}")
            
            if 'language' not in columns:
                logging.info("Adding language column to users table...")
                try:
                    cursor.execute("ALTER TABLE users ADD COLUMN language TEXT DEFAULT ''")
                    logging.info("Added language column to users table")
                except Exception as e:
                    logging.error(f"Failed to add language column: {e}")
            
            if 'ui_layout' not in columns:
                logging.info("Adding ui_layout column to users table...")
                try:
                    cursor.execute("ALTER TABLE users ADD COLUMN ui_layout TEXT DEFAULT 'modern'")
                    logging.info("Added ui_layout column to users table")
                except Exception as e:
                    logging.error(f"Failed to add ui_layout column: {e}")
            
            # NS: Add enabled column if missing (user disable feature)
            if 'enabled' not in columns:
                logging.info("Adding enabled column to users table...")
                try:
                    cursor.execute("ALTER TABLE users ADD COLUMN enabled INTEGER DEFAULT 1")
                    logging.info("Added enabled column to users table")
                except Exception as e:
                    logging.error(f"Failed to add enabled column: {e}")
            
            # MK: Add totp_pending_secret_encrypted column for 2FA setup
            if 'totp_pending_secret_encrypted' not in columns:
                logging.info("Adding totp_pending_secret_encrypted column to users table...")
                try:
                    cursor.execute("ALTER TABLE users ADD COLUMN totp_pending_secret_encrypted TEXT DEFAULT ''")
                    logging.info("Added totp_pending_secret_encrypted column to users table")
                except Exception as e:
                    logging.error(f"Failed to add totp_pending_secret_encrypted column: {e}")

            # NS: Add auth_source column for external authentication - Feb 2026
            if 'auth_source' not in columns:
                logging.info("Adding auth_source column to users table...")
                try:
                    cursor.execute("ALTER TABLE users ADD COLUMN auth_source TEXT DEFAULT 'local'")
                    logging.info("Added auth_source column to users table")
                except Exception as e:
                    logging.error(f"Failed to add auth_source column: {e}")

        except Exception as e:
            logging.error(f"Error checking users schema: {e}")
        
        # Schema migration for clusters table - add group_id
        try:
            cursor.execute("PRAGMA table_info(clusters)")
            cluster_columns = [col[1] for col in cursor.fetchall()]
            
            if 'group_id' not in cluster_columns:
                logging.info("Adding group_id column to clusters table...")
                try:
                    cursor.execute("ALTER TABLE clusters ADD COLUMN group_id TEXT DEFAULT NULL")
                    logging.info("Added group_id column to clusters table")
                except Exception as e:
                    logging.error(f"Failed to add group_id column: {e}")
            
            if 'display_name' not in cluster_columns:
                logging.info("Adding display_name column to clusters table...")
                try:
                    cursor.execute("ALTER TABLE clusters ADD COLUMN display_name TEXT DEFAULT ''")
                    logging.info("Added display_name column to clusters table for custom naming")
                except Exception as e:
                    logging.error(f"Failed to add display_name column: {e}")
            
            # MK: Add sort_order for consistent cluster ordering in sidebar
            if 'sort_order' not in cluster_columns:
                logging.info("Adding sort_order column to clusters table...")
                try:
                    cursor.execute("ALTER TABLE clusters ADD COLUMN sort_order INTEGER DEFAULT 0")
                    logging.info("Added sort_order column to clusters table")
                except Exception as e:
                    logging.error(f"Failed to add sort_order column: {e}")
            
            # LW: Add excluded_nodes for node exclusion from balancing (like ProxLB)
            if 'excluded_nodes' not in cluster_columns:
                logging.info("Adding excluded_nodes column to clusters table...")
                try:
                    cursor.execute("ALTER TABLE clusters ADD COLUMN excluded_nodes TEXT DEFAULT '[]'")
                    logging.info("Added excluded_nodes column to clusters table")
                except Exception as e:
                    logging.error(f"Failed to add excluded_nodes column: {e}")
                    
        except Exception as e:
            logging.error(f"Error checking clusters schema: {e}")
        
        # Add HMAC signature column to audit_log for integrity verification (Jan 2026)
        try:
            cursor.execute("PRAGMA table_info(audit_log)")
            audit_columns = [col[1] for col in cursor.fetchall()]
            
            if 'hmac_signature' not in audit_columns:
                logging.info("Adding hmac_signature column to audit_log table for integrity verification...")
                try:
                    cursor.execute("ALTER TABLE audit_log ADD COLUMN hmac_signature TEXT DEFAULT ''")
                    logging.info("Added hmac_signature column to audit_log table")
                except Exception as e:
                    logging.error(f"Failed to add hmac_signature column: {e}")
        except Exception as e:
            logging.error(f"Error checking audit_log schema: {e}")
        
        # MK: Migration - create balancing_excluded_vms table if not exists
        try:
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS balancing_excluded_vms (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cluster_id TEXT NOT NULL,
                    vmid INTEGER NOT NULL,
                    reason TEXT,
                    created_by TEXT,
                    created_at TEXT,
                    UNIQUE(cluster_id, vmid)
                )
            ''')
            logging.info("Ensured balancing_excluded_vms table exists")
        except Exception as e:
            logging.error(f"Error creating balancing_excluded_vms table: {e}")
        
        # MK: Migration - create update_schedules table if not exists
        try:
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS update_schedules (
                    cluster_id TEXT PRIMARY KEY,
                    enabled INTEGER DEFAULT 0,
                    schedule_type TEXT DEFAULT 'recurring',
                    day TEXT DEFAULT 'sunday',
                    time TEXT DEFAULT '03:00',
                    include_reboot INTEGER DEFAULT 1,
                    skip_evacuation INTEGER DEFAULT 0,
                    skip_up_to_date INTEGER DEFAULT 1,
                    evacuation_timeout INTEGER DEFAULT 1800,
                    last_run TEXT,
                    next_run TEXT,
                    created_by TEXT,
                    created_at TEXT,
                    updated_at TEXT
                )
            ''')
            logging.info("Ensured update_schedules table exists")
        except Exception as e:
            logging.error(f"Error creating update_schedules table: {e}")
        
        conn.commit()
        logging.info("DB schema initialized")
    
    def _encrypt(self, data: str) -> str:
        """encrypt sensitive stuff"""
        if not data:
            return data
        
        # try aes256 first (new way)
        if self.aesgcm:
            try:
                nonce = os.urandom(12)
                ciphertext = self.aesgcm.encrypt(nonce, data.encode('utf-8'), None)
                encrypted = base64.b64encode(nonce + ciphertext).decode('utf-8')
                return f"aes256:{encrypted}"
            except Exception as e:
                logging.error(f"aes encrypt failed: {e}")
        
        # fallback to old fernet
        if self.fernet:
            try:
                return self.fernet.encrypt(data.encode()).decode()
            except Exception as e:
                logging.error(f"fernet failed: {e}")
        
        # ugh no encryption
        logging.warning("no encryption, storing plaintext!!")
        return data
    
    def _decrypt(self, data: str) -> str:
        """decrypt - handles both old and new format"""
        # NS: handles aes256 and old fernet
        if not data:
            return data
        
        # Check for AES-256-GCM format
        if data.startswith('aes256:'):
            if not self.aesgcm:
                logging.error("AES-256-GCM data found but encryption not initialized!")
                return data
            try:
                encrypted = base64.b64decode(data[7:])  # Remove "aes256:" prefix
                nonce = encrypted[:12]  # First 12 bytes are nonce
                ciphertext = encrypted[12:]  # Rest is ciphertext + tag
                plaintext = self.aesgcm.decrypt(nonce, ciphertext, None)
                return plaintext.decode('utf-8')
            except Exception as e:
                logging.error(f"AES-256-GCM decryption failed: {e}")
                return data
        
        # Try Fernet (legacy)
        if self.fernet:
            try:
                # Fernet tokens start with 'gAAA' when base64 encoded
                return self.fernet.decrypt(data.encode()).decode()
            except Exception as e:
                # Not a valid Fernet token - might be plain text
                logging.debug(f"Fernet decryption failed (might be plain text): {e}")
                return data
        
        # Return as-is (probably plain text)
        return data
    
    def _needs_reencrypt(self, data: str) -> bool:
        """Check if data needs to be re-encrypted with AES-256-GCM
        
        NS: Returns True for legacy Fernet data
        """
        if not data or not self.aesgcm:
            return False
        # If it's not AES-256-GCM, it needs re-encryption
        return not data.startswith('aes256:')
    
    def _migrate_from_legacy(self):
        """Migrate data from legacy JSON/encrypted files to SQLite"""
        migrated_any = False
        
        # Check if already migrated
        cursor = self.conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM clusters")
        cluster_count = cursor.fetchone()[0]
        
        # Check if users have proper password_salt (fix for schema migration)
        needs_user_remigration = getattr(self, '_force_remigrate_users', False)
        
        if not needs_user_remigration:
            try:
                cursor.execute("SELECT username, password_salt FROM users LIMIT 1")
                row = cursor.fetchone()
                if row:
                    salt = row[1] if len(row) > 1 else None
                    if not salt or salt == '':  # password_salt is empty or missing
                        logging.warning("Users have empty password_salt - will re-migrate from legacy files")
                        needs_user_remigration = True
            except sqlite3.OperationalError as e:
                # Column might not exist
                logging.warning(f"Could not check password_salt: {e} - will re-migrate")
                needs_user_remigration = True
            except Exception as e:
                logging.error(f"Error checking users: {e}")
        
        if cluster_count > 0 and not needs_user_remigration:
            logging.info("Database already has data, skipping legacy migration")
            return
        
        # Migrate clusters (only if no clusters exist)
        if cluster_count == 0:
            if self._migrate_clusters():
                migrated_any = True
        
        # Migrate users (always if needs_user_remigration or no users)
        if needs_user_remigration or cluster_count == 0:
            # Clear existing users if re-migrating
            if needs_user_remigration:
                try:
                    cursor.execute("DELETE FROM users")
                    self.conn.commit()
                    logging.info("Cleared users table for re-migration")
                except Exception as e:
                    logging.error(f"Error clearing users: {e}")
            
            if self._migrate_users():
                migrated_any = True
        
        # Migrate sessions
        if self._migrate_sessions():
            migrated_any = True
        
        # Migrate audit log
        if self._migrate_audit_log():
            migrated_any = True
        
        # Migrate alerts
        if self._migrate_alerts():
            migrated_any = True
        
        # Migrate VM ACLs
        if self._migrate_vm_acls():
            migrated_any = True
        
        # Migrate affinity rules
        if self._migrate_affinity_rules():
            migrated_any = True
        
        # Migrate tenants
        if self._migrate_tenants():
            migrated_any = True
        
        # Migrate scheduled tasks
        if self._migrate_scheduled_tasks():
            migrated_any = True
        
        # Migrate VM tags
        if self._migrate_vm_tags():
            migrated_any = True
        
        # Migrate migration history
        if self._migrate_migration_history():
            migrated_any = True
        
        # Migrate server settings
        if self._migrate_server_settings():
            migrated_any = True
        
        # Migrate custom roles
        if self._migrate_custom_roles():
            migrated_any = True
        
        # NS: Migrate remaining JSON files - these were scattered everywhere lol
        # MK: should have done this from the start but hindsight is 20/20
        if self._migrate_cluster_alerts():
            migrated_any = True
        
        if self._migrate_esxi_storages():
            migrated_any = True
        
        if self._migrate_storage_clusters():
            migrated_any = True
        
        if self._migrate_cluster_affinity_rules():
            migrated_any = True
        
        # TODO: delete old json files after a few versions? or keep as backup idk - NS
        if migrated_any:
            logging.info("✓ Legacy data migration completed!")
            self.conn.commit()
    
    def _migrate_clusters(self) -> bool:
        """Migrate clusters from encrypted JSON"""
        fernet = get_fernet()
        data = None
        
        # Try encrypted file first
        if fernet and os.path.exists(CONFIG_FILE_ENCRYPTED):
            try:
                with open(CONFIG_FILE_ENCRYPTED, 'rb') as f:
                    encrypted_data = f.read()
                decrypted = fernet.decrypt(encrypted_data)
                data = json.loads(decrypted.decode('utf-8'))
            except Exception as e:
                logging.error(f"Failed to load encrypted clusters: {e}")
        
        # Try unencrypted
        if not data and os.path.exists(CONFIG_FILE):
            try:
                with open(CONFIG_FILE, 'r') as f:
                    data = json.load(f)
            except Exception as e:
                logging.error(f"Failed to load clusters.json: {e}")
        
        if not data:
            return False
        
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        for cluster_id, cluster in data.items():
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO clusters 
                    (id, name, host, user, pass_encrypted, ssl_verification, 
                     migration_threshold, check_interval, auto_migrate, 
                     balance_containers, balance_local_disks, dry_run, enabled, 
                     ha_enabled, fallback_hosts, ssh_user, ssh_key_encrypted, 
                     ssh_port, ha_settings, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    cluster_id,
                    cluster.get('name', ''),
                    cluster.get('host', ''),
                    cluster.get('user', ''),
                    self._encrypt(cluster.get('pass', '')),
                    1 if cluster.get('ssl_verification', True) else 0,
                    cluster.get('migration_threshold', 30),
                    cluster.get('check_interval', 300),
                    1 if cluster.get('auto_migrate', False) else 0,
                    1 if cluster.get('balance_containers', False) else 0,
                    1 if cluster.get('balance_local_disks', False) else 0,
                    1 if cluster.get('dry_run', True) else 0,
                    1 if cluster.get('enabled', True) else 0,
                    1 if cluster.get('ha_enabled', False) else 0,
                    json.dumps(cluster.get('fallback_hosts', [])),
                    cluster.get('ssh_user', ''),
                    self._encrypt(cluster.get('ssh_key', '')),
                    cluster.get('ssh_port', 22),
                    json.dumps(cluster.get('ha_settings', {})),
                    now, now
                ))
            except Exception as e:
                logging.error(f"Failed to migrate cluster {cluster_id}: {e}")
        
        logging.info(f"Migrated {len(data)} clusters to SQLite")
        return True
    
    def _migrate_users(self) -> bool:
        """Migrate users from encrypted file"""
        fernet = get_fernet()
        if not fernet or not os.path.exists(USERS_FILE_ENCRYPTED):
            return False
        
        try:
            with open(USERS_FILE_ENCRYPTED, 'rb') as f:
                encrypted_data = f.read()
            decrypted = fernet.decrypt(encrypted_data)
            data = json.loads(decrypted.decode('utf-8'))
        except Exception as e:
            logging.error(f"Failed to load users: {e}")
            return False
        
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        for username, user in data.items():
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO users
                    (username, password_salt, password_hash, role, permissions, tenant, 
                     created_at, last_login, password_expiry, 
                     totp_secret_encrypted, totp_enabled, force_password_change)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    username,
                    user.get('password_salt', ''),
                    user.get('password_hash', user.get('password', '')),
                    user.get('role', 'viewer'),
                    json.dumps(user.get('permissions', [])),
                    user.get('tenant'),
                    user.get('created_at', now),
                    user.get('last_login'),
                    user.get('password_expiry'),
                    self._encrypt(user.get('totp_secret', '')),
                    1 if user.get('totp_enabled', False) else 0,
                    1 if user.get('force_password_change', False) else 0
                ))
            except Exception as e:
                logging.error(f"Failed to migrate user {username}: {e}")
        
        logging.info(f"Migrated {len(data)} users to SQLite")
        return True
    
    def _migrate_sessions(self) -> bool:
        """Migrate sessions from encrypted file"""
        fernet = get_fernet()
        data = None
        
        if fernet and os.path.exists(SESSIONS_FILE_ENCRYPTED):
            try:
                with open(SESSIONS_FILE_ENCRYPTED, 'rb') as f:
                    encrypted_data = f.read()
                decrypted = fernet.decrypt(encrypted_data)
                data = json.loads(decrypted.decode('utf-8'))
            except:
                pass
        
        if not data and os.path.exists(SESSIONS_FILE):
            try:
                with open(SESSIONS_FILE, 'r') as f:
                    data = json.load(f)
            except:
                pass
        
        if not data:
            return False
        
        cursor = self.conn.cursor()
        
        for token, session in data.items():
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO sessions
                    (token, username, created_at, expires_at, ip_address, user_agent)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    token,
                    session.get('user', ''),
                    session.get('created', ''),
                    session.get('expires', ''),
                    session.get('ip', ''),
                    session.get('user_agent', '')
                ))
            except:
                pass
        
        logging.info(f"Migrated {len(data)} sessions to SQLite")
        return True
    
    def _migrate_audit_log(self) -> bool:
        """Migrate audit log from encrypted file"""
        fernet = get_fernet()
        data = None
        
        if fernet and os.path.exists(AUDIT_LOG_FILE_ENCRYPTED):
            try:
                with open(AUDIT_LOG_FILE_ENCRYPTED, 'rb') as f:
                    encrypted_data = f.read()
                decrypted = fernet.decrypt(encrypted_data)
                data = json.loads(decrypted.decode('utf-8'))
            except:
                pass
        
        if not data and os.path.exists(AUDIT_LOG_FILE):
            try:
                with open(AUDIT_LOG_FILE, 'r') as f:
                    data = json.load(f)
            except:
                pass
        
        if not data:
            return False
        
        cursor = self.conn.cursor()
        
        for entry in data:
            try:
                cursor.execute('''
                    INSERT INTO audit_log (timestamp, user, action, details, ip_address)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    entry.get('timestamp', ''),
                    entry.get('user', ''),
                    entry.get('action', ''),
                    entry.get('details', ''),
                    entry.get('ip', '')
                ))
            except:
                pass
        
        logging.info(f"Migrated {len(data)} audit entries to SQLite")
        return True
    
    def _migrate_alerts(self) -> bool:
        """Migrate alerts from JSON"""
        if not os.path.exists(ALERTS_CONFIG_FILE):
            return False
        
        try:
            with open(ALERTS_CONFIG_FILE, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        for alert_id, alert in data.items():
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO alerts
                    (id, cluster_id, node, vmid, type, threshold, enabled, 
                     notify_methods, cooldown, last_triggered, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    alert_id,
                    alert.get('cluster_id'),
                    alert.get('node'),
                    alert.get('vmid'),
                    alert.get('type', ''),
                    alert.get('threshold'),
                    1 if alert.get('enabled', True) else 0,
                    json.dumps(alert.get('notify_methods', [])),
                    alert.get('cooldown', 300),
                    alert.get('last_triggered'),
                    now
                ))
            except:
                pass
        
        logging.info(f"Migrated {len(data)} alerts to SQLite")
        return True
    
    def _migrate_vm_acls(self) -> bool:
        """Migrate VM ACLs from JSON"""
        vm_acls_file = os.path.join(CONFIG_DIR, 'vm_acls.json')
        if not os.path.exists(vm_acls_file):
            return False
        
        try:
            with open(vm_acls_file, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        
        for cluster_id, vms in data.items():
            for vmid, acl in vms.items():
                try:
                    cursor.execute('''
                        INSERT OR REPLACE INTO vm_acls (cluster_id, vmid, users, permissions)
                        VALUES (?, ?, ?, ?)
                    ''', (
                        cluster_id,
                        vmid,
                        json.dumps(acl.get('users', [])),
                        json.dumps(acl.get('permissions', []))
                    ))
                except:
                    pass
        
        logging.info(f"Migrated VM ACLs to SQLite")
        return True
    
    def _migrate_affinity_rules(self) -> bool:
        """Migrate affinity rules from JSON"""
        if not os.path.exists(AFFINITY_RULES_FILE):
            return False
        
        try:
            with open(AFFINITY_RULES_FILE, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        for cluster_id, rules in data.items():
            for rule in rules:
                try:
                    cursor.execute('''
                        INSERT OR REPLACE INTO affinity_rules
                        (id, cluster_id, name, type, vms, enabled, created_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        rule.get('id', str(uuid.uuid4())[:8]),
                        cluster_id,
                        rule.get('name', ''),
                        rule.get('type', 'affinity'),
                        json.dumps(rule.get('vms', [])),
                        1 if rule.get('enabled', True) else 0,
                        now
                    ))
                except:
                    pass
        
        logging.info(f"Migrated affinity rules to SQLite")
        return True
    
    def _migrate_tenants(self) -> bool:
        """Migrate tenants from JSON"""
        tenants_file = os.path.join(CONFIG_DIR, 'tenants.json')
        if not os.path.exists(tenants_file):
            return False
        
        try:
            with open(tenants_file, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        for tenant in data:
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO tenants (id, name, clusters, created_at)
                    VALUES (?, ?, ?, ?)
                ''', (
                    tenant.get('id', str(uuid.uuid4())[:8]),
                    tenant.get('name', ''),
                    json.dumps(tenant.get('clusters', [])),
                    now
                ))
            except:
                pass
        
        logging.info(f"Migrated {len(data)} tenants to SQLite")
        return True
    
    def _migrate_scheduled_tasks(self) -> bool:
        """Migrate scheduled tasks from JSON"""
        if not os.path.exists(SCHEDULED_TASKS_FILE):
            return False
        
        try:
            with open(SCHEDULED_TASKS_FILE, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        for task_id, task in data.items():
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO scheduled_tasks
                    (id, cluster_id, name, task_type, schedule, config, 
                     enabled, last_run, next_run, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    task_id,
                    task.get('cluster_id'),
                    task.get('name', ''),
                    task.get('task_type', ''),
                    task.get('schedule', ''),
                    json.dumps(task.get('config', {})),
                    1 if task.get('enabled', True) else 0,
                    task.get('last_run'),
                    task.get('next_run'),
                    now
                ))
            except:
                pass
        
        logging.info(f"Migrated {len(data)} scheduled tasks to SQLite")
        return True
    
    def _migrate_vm_tags(self) -> bool:
        """Migrate VM tags from JSON"""
        if not os.path.exists(VM_TAGS_FILE):
            return False
        
        try:
            with open(VM_TAGS_FILE, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        
        for key, tags in data.items():
            try:
                parts = key.split(':')
                if len(parts) == 2:
                    cluster_id, vmid = parts
                    for tag in tags:
                        tag_name = tag if isinstance(tag, str) else tag.get('name', '')
                        tag_color = tag.get('color', '') if isinstance(tag, dict) else ''
                        cursor.execute('''
                            INSERT OR IGNORE INTO vm_tags (cluster_id, vmid, tag_name, tag_color)
                            VALUES (?, ?, ?, ?)
                        ''', (cluster_id, int(vmid), tag_name, tag_color))
            except:
                pass
        
        logging.info(f"Migrated VM tags to SQLite")
        return True
    
    def _migrate_migration_history(self) -> bool:
        """Migrate migration history from JSON"""
        if not os.path.exists(MIGRATION_HISTORY_FILE):
            return False
        
        try:
            with open(MIGRATION_HISTORY_FILE, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        
        for entry in data:
            try:
                cursor.execute('''
                    INSERT INTO migration_history
                    (cluster_id, vmid, vm_name, source_node, target_node, 
                     reason, status, duration_seconds, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    entry.get('cluster_id', ''),
                    entry.get('vmid', 0),
                    entry.get('vm_name', ''),
                    entry.get('source_node', ''),
                    entry.get('target_node', ''),
                    entry.get('reason', ''),
                    entry.get('status', ''),
                    entry.get('duration', 0),
                    entry.get('timestamp', '')
                ))
            except:
                pass
        
        logging.info(f"Migrated {len(data)} migration history entries to SQLite")
        return True
    
    def _migrate_server_settings(self) -> bool:
        """Migrate server settings from JSON"""
        if not os.path.exists(SERVER_SETTINGS_FILE):
            return False
        
        try:
            with open(SERVER_SETTINGS_FILE, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        
        for key, value in data.items():
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO server_settings (key, value)
                    VALUES (?, ?)
                ''', (key, json.dumps(value) if not isinstance(value, str) else value))
            except:
                pass
        
        logging.info(f"Migrated server settings to SQLite")
        return True
    
    def _migrate_custom_roles(self) -> bool:
        """Migrate custom roles from JSON"""
        roles_file = os.path.join(CONFIG_DIR, 'custom_roles.json')
        if not os.path.exists(roles_file):
            return False
        
        try:
            with open(roles_file, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        for role_name, role_data in data.items():
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO custom_roles (name, permissions, description, created_at)
                    VALUES (?, ?, ?, ?)
                ''', (
                    role_name,
                    json.dumps(role_data.get('permissions', [])),
                    role_data.get('description', ''),
                    now
                ))
            except:
                pass
        
        logging.info(f"Migrated custom roles to SQLite")
        return True
    
    def _migrate_cluster_alerts(self) -> bool:
        """Migrate cluster alerts from JSON to SQLite
        
        NS: These were in cluster_alerts.json before, now in db
        MK: handles both old dict format and new list format
        """
        alerts_file = os.path.join(CONFIG_DIR, 'cluster_alerts.json')
        if not os.path.exists(alerts_file):
            return False
        
        try:
            with open(alerts_file, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        migrated = 0
        
        for cluster_id, alerts in data.items():
            # Handle list format (new style)
            if isinstance(alerts, list):
                for alert in alerts:
                    try:
                        alert_id = alert.get('id', str(uuid.uuid4())[:8])
                        cursor.execute('''
                            INSERT OR REPLACE INTO cluster_alerts 
                            (cluster_id, alert_type, config, enabled, created_at, updated_at)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', (
                            cluster_id,
                            alert_id,
                            json.dumps(alert),
                            1 if alert.get('enabled', True) else 0,
                            now,
                            now
                        ))
                        migrated += 1
                    except:
                        pass
            # Handle dict format (old style)
            elif isinstance(alerts, dict):
                for alert_type, config in alerts.items():
                    try:
                        cursor.execute('''
                            INSERT OR REPLACE INTO cluster_alerts 
                            (cluster_id, alert_type, config, enabled, created_at, updated_at)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', (
                            cluster_id,
                            alert_type,
                            json.dumps(config) if isinstance(config, dict) else str(config),
                            1,
                            now,
                            now
                        ))
                        migrated += 1
                    except:
                        pass
        
        logging.info(f"Migrated {migrated} cluster alerts to SQLite")
        return True
    
    def _migrate_esxi_storages(self) -> bool:
        """Migrate ESXi storage config from JSON to SQLite
        
        LW: this esxi stuff was added for vmware migration support
        but those who do really need it for vmware migrations
        """
        esxi_file = os.path.join(CONFIG_DIR, 'esxi_storages.json')
        if not os.path.exists(esxi_file):
            return False
        
        try:
            with open(esxi_file, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        
        storages = data.get('storages', [])
        for storage in storages:
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO esxi_storages 
                    (name, host, username, password_encrypted, datastore, enabled, config)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    storage.get('name', ''),
                    storage.get('host', ''),
                    storage.get('username', ''),
                    storage.get('password', ''),  # Already encrypted in JSON
                    storage.get('datastore', ''),
                    1 if storage.get('enabled', True) else 0,
                    json.dumps(storage.get('config', {}))
                ))
            except:
                pass  # old configs might have weird formats
        
        logging.info(f"Migrated {len(storages)} ESXi storages to SQLite")
        return True
    
    def _migrate_storage_clusters(self) -> bool:
        """Migrate storage clusters from JSON to SQLite
        
        NS: this file was in the wrong place for a while (root dir instead of config)
        so we check both locations just in case
        """
        storage_file = os.path.join(CONFIG_DIR, 'storage_clusters.json')
        if not os.path.exists(storage_file):
            storage_file = 'storage_clusters.json'  # Legacy location oops
        if not os.path.exists(storage_file):
            return False
        
        try:
            with open(storage_file, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        migrated = 0
        
        for cluster_id, config in data.items():
            clusters = config.get('clusters', [])
            for sc in clusters:
                try:
                    cursor.execute('''
                        INSERT OR REPLACE INTO storage_clusters 
                        (cluster_id, name, storage_type, nodes, config, enabled)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        cluster_id,
                        sc.get('name', ''),
                        sc.get('type', 'ceph'),
                        json.dumps(sc.get('nodes', [])),
                        json.dumps(sc.get('config', {})),
                        1 if sc.get('enabled', True) else 0
                    ))
                    migrated += 1
                except:
                    pass
        
        logging.info(f"Migrated {migrated} storage clusters to SQLite")
        return True
    
    def _migrate_cluster_affinity_rules(self) -> bool:
        """Migrate cluster affinity rules from JSON to SQLite
        
        MK: affinity rules keep VMs together or apart on hosts
        useful for HA setups where you dont want both replicas on same node
        """
        rules_file = os.path.join(CONFIG_DIR, 'cluster_affinity_rules.json')
        if not os.path.exists(rules_file):
            return False
        
        try:
            with open(rules_file, 'r') as f:
                data = json.load(f)
        except:
            return False
        
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        migrated = 0
        
        for cluster_id, rules in data.items():
            for rule in rules:
                try:
                    # some old rules might not have an id, generate one
                    rule_id = rule.get('id', str(uuid.uuid4()))
                    # NS: handle both 'vms' and 'vm_ids' field names
                    vms_data = rule.get('vms') or rule.get('vm_ids') or []
                    cursor.execute('''
                        INSERT OR REPLACE INTO affinity_rules 
                        (id, cluster_id, name, type, vms, enabled, created_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        rule_id,
                        cluster_id,
                        rule.get('name', ''),
                        rule.get('type', 'affinity'),
                        json.dumps(vms_data),
                        1 if rule.get('enabled', True) else 0,
                        rule.get('created_at', now)
                    ))
                    migrated += 1
                except:
                    pass
        
        logging.info(f"Migrated {migrated} cluster affinity rules to SQLite")
        return True
    
    # ========================================
    # CLUSTER OPERATIONS
    # ========================================
    
    def get_all_clusters(self) -> dict:
        """Get all clusters (returns dict like legacy format)"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM clusters')
        
        clusters = {}
        for row in cursor.fetchall():
            clusters[row['id']] = {
                'name': row['name'],
                'host': row['host'],
                'user': row['user'],
                'pass': self._decrypt(row['pass_encrypted']),
                'ssl_verification': bool(row['ssl_verification']),
                'migration_threshold': row['migration_threshold'],
                'check_interval': row['check_interval'],
                'auto_migrate': bool(row['auto_migrate']),
                'balance_containers': bool(row['balance_containers']),
                'balance_local_disks': bool(row['balance_local_disks']),
                'dry_run': bool(row['dry_run']),
                'enabled': bool(row['enabled']),
                'ha_enabled': bool(row['ha_enabled']),
                'fallback_hosts': json.loads(row['fallback_hosts'] or '[]'),
                'ssh_user': row['ssh_user'] or '',
                'ssh_key': self._decrypt(row['ssh_key_encrypted'] or ''),
                'ssh_port': row['ssh_port'] or 22,
                'ha_settings': json.loads(row['ha_settings'] or '{}'),
                'excluded_nodes': json.loads(row['excluded_nodes'] or '[]'),
            }
        
        return clusters
    
    def get_cluster(self, cluster_id: str) -> dict:
        """Get single cluster"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM clusters WHERE id = ?', (cluster_id,))
        row = cursor.fetchone()
        
        if not row:
            return None
        
        # NS: Auto-migrate encrypted fields to AES-256-GCM if needed - Jan 2026
        pass_encrypted = row['pass_encrypted']
        ssh_key_encrypted = row['ssh_key_encrypted'] or ''
        needs_migration = False
        
        if self._needs_reencrypt(pass_encrypted):
            needs_migration = True
        if ssh_key_encrypted and self._needs_reencrypt(ssh_key_encrypted):
            needs_migration = True
        
        # Decrypt values
        decrypted_pass = self._decrypt(pass_encrypted)
        decrypted_ssh_key = self._decrypt(ssh_key_encrypted) if ssh_key_encrypted else ''
        
        # If migration needed, re-encrypt and save
        if needs_migration and self.aesgcm:
            try:
                cursor.execute('''
                    UPDATE clusters SET 
                        pass_encrypted = ?,
                        ssh_key_encrypted = ?,
                        updated_at = ?
                    WHERE id = ?
                ''', (
                    self._encrypt(decrypted_pass),
                    self._encrypt(decrypted_ssh_key) if decrypted_ssh_key else '',
                    datetime.now().isoformat(),
                    cluster_id
                ))
                self.conn.commit()
                logging.info(f"Migrated cluster '{cluster_id}' encryption to AES-256-GCM (Military Grade)")
            except Exception as e:
                logging.warning(f"Failed to migrate cluster encryption: {e}")
        
        return {
            'name': row['name'],
            'host': row['host'],
            'user': row['user'],
            'pass': decrypted_pass,
            'ssl_verification': bool(row['ssl_verification']),
            'migration_threshold': row['migration_threshold'],
            'check_interval': row['check_interval'],
            'auto_migrate': bool(row['auto_migrate']),
            'balance_containers': bool(row['balance_containers']),
            'balance_local_disks': bool(row['balance_local_disks']),
            'dry_run': bool(row['dry_run']),
            'enabled': bool(row['enabled']),
            'ha_enabled': bool(row['ha_enabled']),
            'fallback_hosts': json.loads(row['fallback_hosts'] or '[]'),
            'ssh_user': row['ssh_user'] or '',
            'ssh_key': decrypted_ssh_key,
            'ssh_port': row['ssh_port'] or 22,
            'ha_settings': json.loads(row['ha_settings'] or '{}'),
            'excluded_nodes': json.loads(row['excluded_nodes'] or '[]'),
        }
    
    def save_cluster(self, cluster_id: str, data: dict):
        """Save or update cluster"""
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        cursor.execute('''
            INSERT OR REPLACE INTO clusters 
            (id, name, host, user, pass_encrypted, ssl_verification, 
             migration_threshold, check_interval, auto_migrate, 
             balance_containers, balance_local_disks, dry_run, enabled, 
             ha_enabled, fallback_hosts, ssh_user, ssh_key_encrypted, 
             ssh_port, ha_settings, excluded_nodes, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 
                    COALESCE((SELECT created_at FROM clusters WHERE id = ?), ?), ?)
        ''', (
            cluster_id,
            data.get('name', ''),
            data.get('host', ''),
            data.get('user', ''),
            self._encrypt(data.get('pass', '')),
            1 if data.get('ssl_verification', True) else 0,
            data.get('migration_threshold', 30),
            data.get('check_interval', 300),
            1 if data.get('auto_migrate', False) else 0,
            1 if data.get('balance_containers', False) else 0,
            1 if data.get('balance_local_disks', False) else 0,
            1 if data.get('dry_run', True) else 0,
            1 if data.get('enabled', True) else 0,
            1 if data.get('ha_enabled', False) else 0,
            json.dumps(data.get('fallback_hosts', [])),
            data.get('ssh_user', ''),
            self._encrypt(data.get('ssh_key', '')),
            data.get('ssh_port', 22),
            json.dumps(data.get('ha_settings', {})),
            json.dumps(data.get('excluded_nodes', [])),
            cluster_id, now, now
        ))
        self.conn.commit()
    
    def delete_cluster(self, cluster_id: str):
        """Delete cluster"""
        cursor = self.conn.cursor()
        cursor.execute('DELETE FROM clusters WHERE id = ?', (cluster_id,))
        self.conn.commit()
    
    # ========================================
    # USER OPERATIONS
    # ========================================
    
    def get_all_users(self) -> dict:
        """Get all users"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM users')
        
        users = {}
        for row in cursor.fetchall():
            # Handle both old schema (no password_salt) and new schema
            row_dict = dict(row)
            password_salt = row_dict.get('password_salt', '')
            password_hash = row_dict.get('password_hash', '')
            
            # If password_salt is missing or empty, check if there's a combined 'password' field
            # This handles migration edge cases
            if not password_salt and 'password' in row_dict:
                # Old format might have combined salt:hash
                combined = row_dict.get('password', '')
                if ':' in combined:
                    password_salt, password_hash = combined.split(':', 1)
            
            users[row['username']] = {
                'password_salt': password_salt,
                'password_hash': password_hash,
                'role': row['role'],
                'permissions': json.loads(row_dict.get('permissions') or '[]'),
                'tenant': row_dict.get('tenant'),
                'created_at': row_dict.get('created_at'),
                'last_login': row_dict.get('last_login'),
                'password_expiry': row_dict.get('password_expiry'),
                'totp_secret': self._decrypt(row_dict.get('totp_secret_encrypted') or ''),
                'totp_pending_secret': self._decrypt(row_dict.get('totp_pending_secret_encrypted') or ''),  # MK: Load pending 2FA secret
                'totp_enabled': bool(row_dict.get('totp_enabled', 0)),
                'force_password_change': bool(row_dict.get('force_password_change', 0)),
                'enabled': bool(row_dict.get('enabled', 1)),
            }
        
        return users
    
    def get_user(self, username: str) -> dict:
        """Get single user"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM users WHERE username = ?', (username,))
        row = cursor.fetchone()
        
        if not row:
            return None
        
        # Handle both old schema (no password_salt) and new schema
        row_dict = dict(row)
        password_salt = row_dict.get('password_salt', '')
        password_hash = row_dict.get('password_hash', '')
        
        # If password_salt is missing or empty, check if there's a combined 'password' field
        if not password_salt and 'password' in row_dict:
            combined = row_dict.get('password', '')
            if ':' in combined:
                password_salt, password_hash = combined.split(':', 1)
        
        return {
            'password_salt': password_salt,
            'password_hash': password_hash,
            'role': row_dict.get('role', 'viewer'),
            'permissions': json.loads(row_dict.get('permissions') or '[]'),
            'tenant': row_dict.get('tenant'),
            'created_at': row_dict.get('created_at'),
            'last_login': row_dict.get('last_login'),
            'password_expiry': row_dict.get('password_expiry'),
            'totp_secret': self._decrypt(row_dict.get('totp_secret_encrypted') or ''),
            'totp_pending_secret': self._decrypt(row_dict.get('totp_pending_secret_encrypted') or ''),  # MK: Load pending 2FA secret
            'totp_enabled': bool(row_dict.get('totp_enabled', 0)),
            'force_password_change': bool(row_dict.get('force_password_change', 0)),
            'enabled': bool(row_dict.get('enabled', 1)),
            'theme': row_dict.get('theme', ''),
            'language': row_dict.get('language', ''),
            'ui_layout': row_dict.get('ui_layout', 'modern'),
        }
    
    def save_user(self, username: str, data: dict):
        """Save or update user"""
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        cursor.execute('''
            INSERT OR REPLACE INTO users
            (username, password_salt, password_hash, role, permissions, tenant, 
             created_at, last_login, password_expiry, 
             totp_secret_encrypted, totp_pending_secret_encrypted, totp_enabled, force_password_change,
             enabled, theme, language, ui_layout)
            VALUES (?, ?, ?, ?, ?, ?, 
                    COALESCE((SELECT created_at FROM users WHERE username = ?), ?), 
                    ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            username,
            data.get('password_salt', ''),
            data.get('password_hash', ''),
            data.get('role', 'viewer'),
            json.dumps(data.get('permissions', [])),
            data.get('tenant'),
            username, now,
            data.get('last_login'),
            data.get('password_expiry'),
            self._encrypt(data.get('totp_secret', '')),
            self._encrypt(data.get('totp_pending_secret', '')),  # MK: Save pending 2FA secret
            1 if data.get('totp_enabled', False) else 0,
            1 if data.get('force_password_change', False) else 0,
            1 if data.get('enabled', True) else 0,
            data.get('theme', ''),
            data.get('language', ''),
            data.get('ui_layout', 'modern')
        ))
        self.conn.commit()
    
    def save_all_users(self, users: dict):
        """Save all users (for bulk operations)"""
        for username, data in users.items():
            self.save_user(username, data)
    
    def delete_user(self, username: str):
        """Delete user"""
        cursor = self.conn.cursor()
        cursor.execute('DELETE FROM users WHERE username = ?', (username,))
        self.conn.commit()
    
    # ========================================
    # SESSION OPERATIONS
    # ========================================
    
    def get_all_sessions(self) -> dict:
        """Get all sessions from database
        
        NOTE: Since v0.6.1, session tokens are stored as SHA-256 hashes.
        This means sessions loaded from DB cannot be validated against
        plaintext tokens - users must re-login after server restart.
        This is a SECURITY FEATURE, not a bug!
        """
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM sessions')
        
        # NS: Return empty dict - old hashed sessions can't be used anyway
        # This forces re-login after restart (more secure)
        sessions = {}
        # Note: We could load the hashes, but they're useless for validation
        # since we can't reverse SHA-256. Just return empty.
        logging.debug(f"Sessions in DB will be cleared (tokens are hashed, can't validate)")
        
        # Clean up old sessions from DB
        cursor.execute('DELETE FROM sessions')
        self.conn.commit()
        
        return sessions
    
    def get_session(self, token: str) -> dict:
        """Get single session"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM sessions WHERE token = ?', (token,))
        row = cursor.fetchone()
        
        if not row:
            return None
        
        return {
            'user': row['username'],
            'created': row['created_at'],
            'expires': row['expires_at'],
            'ip': row['ip_address'],
            'user_agent': row['user_agent'],
        }
    
    def save_session(self, token: str, data: dict):
        """Save session
        
        NS: Session tokens are hashed before storing in DB for security!
        If someone steals the DB, they can't hijack sessions.
        Trade-off: Sessions don't survive server restarts (users must re-login)
        """
        cursor = self.conn.cursor()
        
        # Hash the token - even if DB is stolen, tokens can't be used
        token_hash = hashlib.sha256(token.encode()).hexdigest()
        
        cursor.execute('''
            INSERT OR REPLACE INTO sessions
            (token, username, created_at, expires_at, ip_address, user_agent)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            token_hash,  # Store hash, not plaintext token!
            data.get('user', ''),
            data.get('created', ''),
            data.get('expires', ''),
            data.get('ip', ''),
            data.get('user_agent', '')
        ))
        self.conn.commit()
    
    def delete_session(self, token: str):
        """Delete session"""
        cursor = self.conn.cursor()
        token_hash = hashlib.sha256(token.encode()).hexdigest()
        cursor.execute('DELETE FROM sessions WHERE token = ?', (token_hash,))
        self.conn.commit()
    
    def delete_expired_sessions(self):
        """Delete expired sessions"""
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        cursor.execute('DELETE FROM sessions WHERE expires_at < ?', (now,))
        self.conn.commit()
    
    def save_all_sessions(self, sessions: dict):
        """Save all sessions"""
        for token, data in sessions.items():
            self.save_session(token, data)
    
    # ========================================
    # AUDIT LOG OPERATIONS (with HMAC Integrity)
    # ========================================
    
    def _generate_audit_hmac(self, timestamp: str, user: str, action: str, details: str, ip: str) -> str:
        """Generate HMAC signature for audit entry (tamper detection)"""
        if not self.aes_key:
            return ''
        
        # Create canonical string for signing
        data = f"{timestamp}|{user or ''}|{action}|{details or ''}|{ip or ''}"
        
        # Use HMAC-SHA256 with AES key as secret
        signature = hmac.new(
            self.aes_key,
            data.encode('utf-8'),
            hashlib.sha256
        ).hexdigest()
        
        return signature
    
    def _verify_audit_hmac(self, entry: dict) -> bool:
        """Verify HMAC signature of an audit entry"""
        if not self.aes_key:
            return True  # Can't verify without key
        
        stored_sig = entry.get('hmac_signature', '')
        if not stored_sig:
            return False  # No signature = potentially tampered or old entry
        
        # Regenerate signature
        expected_sig = self._generate_audit_hmac(
            entry.get('timestamp', ''),
            entry.get('user', ''),
            entry.get('action', ''),
            entry.get('details', ''),
            entry.get('ip_address', '')
        )
        
        # Constant-time comparison to prevent timing attacks
        return hmac.compare_digest(stored_sig, expected_sig)
    
    def add_audit_entry(self, user: str, action: str, details: str = '', ip: str = ''):
        """Add audit log entry with HMAC signature for integrity verification"""
        cursor = self.conn.cursor()
        timestamp = datetime.now().isoformat()
        
        # Generate HMAC signature for tamper detection
        signature = self._generate_audit_hmac(timestamp, user, action, details, ip)
        
        cursor.execute('''
            INSERT INTO audit_log (timestamp, user, action, details, ip_address, hmac_signature)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (timestamp, user, action, details, ip, signature))
        self.conn.commit()
    
    def get_audit_log(self, limit: int = 1000, user: str = None, action: str = None, verify_integrity: bool = False) -> list:
        """Get audit log entries, optionally verifying HMAC integrity"""
        cursor = self.conn.cursor()
        
        query = 'SELECT * FROM audit_log'
        params = []
        conditions = []
        
        if user:
            conditions.append('user = ?')
            params.append(user)
        if action:
            conditions.append('action LIKE ?')
            params.append(f'%{action}%')
        
        if conditions:
            query += ' WHERE ' + ' AND '.join(conditions)
        
        query += ' ORDER BY timestamp DESC LIMIT ?'
        params.append(limit)
        
        cursor.execute(query, params)
        
        entries = [dict(row) for row in cursor.fetchall()]
        
        # Optionally verify integrity
        if verify_integrity:
            for entry in entries:
                entry['integrity_verified'] = self._verify_audit_hmac(entry)
        
        return entries
    
    def verify_audit_log_integrity(self) -> dict:
        """Verify integrity of entire audit log - returns statistics"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM audit_log ORDER BY timestamp DESC')
        
        total = 0
        verified = 0
        unsigned = 0
        tampered = 0
        
        for row in cursor.fetchall():
            entry = dict(row)
            total += 1
            
            if not entry.get('hmac_signature'):
                unsigned += 1  # Old entry without signature
            elif self._verify_audit_hmac(entry):
                verified += 1
            else:
                tampered += 1
                logging.warning(f"AUDIT LOG INTEGRITY VIOLATION: Entry ID {entry.get('id')} may have been tampered!")
        
        return {
            'total_entries': total,
            'verified': verified,
            'unsigned': unsigned,
            'potentially_tampered': tampered,
            'integrity_percentage': round((verified / total * 100) if total > 0 else 100, 2)
        }
    
    def cleanup_audit_log(self, days: int = 90):
        """Remove audit entries older than specified days"""
        cursor = self.conn.cursor()
        cutoff = (datetime.now() - timedelta(days=days)).isoformat()
        cursor.execute('DELETE FROM audit_log WHERE timestamp < ?', (cutoff,))
        deleted = cursor.rowcount
        self.conn.commit()
        return deleted
    
    # ========================================
    # ALERT OPERATIONS
    # ========================================
    
    def get_all_alerts(self) -> dict:
        """Get all alerts"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM alerts')
        
        alerts = {}
        for row in cursor.fetchall():
            alerts[row['id']] = {
                'cluster_id': row['cluster_id'],
                'node': row['node'],
                'vmid': row['vmid'],
                'type': row['type'],
                'threshold': row['threshold'],
                'enabled': bool(row['enabled']),
                'notify_methods': json.loads(row['notify_methods'] or '[]'),
                'cooldown': row['cooldown'],
                'last_triggered': row['last_triggered'],
            }
        
        return alerts
    
    def save_alert(self, alert_id: str, data: dict):
        """Save alert"""
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        cursor.execute('''
            INSERT OR REPLACE INTO alerts
            (id, cluster_id, node, vmid, type, threshold, enabled, 
             notify_methods, cooldown, last_triggered, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 
                    COALESCE((SELECT created_at FROM alerts WHERE id = ?), ?))
        ''', (
            alert_id,
            data.get('cluster_id'),
            data.get('node'),
            data.get('vmid'),
            data.get('type', ''),
            data.get('threshold'),
            1 if data.get('enabled', True) else 0,
            json.dumps(data.get('notify_methods', [])),
            data.get('cooldown', 300),
            data.get('last_triggered'),
            alert_id, now
        ))
        self.conn.commit()
    
    def delete_alert(self, alert_id: str):
        """Delete alert"""
        cursor = self.conn.cursor()
        cursor.execute('DELETE FROM alerts WHERE id = ?', (alert_id,))
        self.conn.commit()
    
    def save_all_alerts(self, alerts: dict):
        """Save all alerts"""
        for alert_id, data in alerts.items():
            self.save_alert(alert_id, data)
    
    # ========================================
    # VM ACL OPERATIONS
    # ========================================
    
    def get_all_vm_acls(self) -> dict:
        """Get all VM ACLs"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM vm_acls')
        
        acls = {}
        for row in cursor.fetchall():
            cluster_id = row['cluster_id']
            if cluster_id not in acls:
                acls[cluster_id] = {}
            acls[cluster_id][row['vmid']] = {
                'users': json.loads(row['users'] or '[]'),
                'permissions': json.loads(row['permissions'] or '[]'),
            }
        
        return acls
    
    def save_vm_acl(self, cluster_id: str, vmid: str, data: dict):
        """Save VM ACL"""
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO vm_acls (cluster_id, vmid, users, permissions)
            VALUES (?, ?, ?, ?)
        ''', (
            cluster_id,
            vmid,
            json.dumps(data.get('users', [])),
            json.dumps(data.get('permissions', []))
        ))
        self.conn.commit()
    
    def save_all_vm_acls(self, acls: dict):
        """Save all VM ACLs"""
        for cluster_id, vms in acls.items():
            for vmid, data in vms.items():
                self.save_vm_acl(cluster_id, vmid, data)
    
    def delete_vm_acl(self, cluster_id: str, vmid: int) -> bool:
        """Delete a VM ACL entry from the database
        
        NS: This was missing! save_all_vm_acls only adds/updates, never deletes.
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute('DELETE FROM vm_acls WHERE cluster_id = ? AND vmid = ?',
                          (cluster_id, str(vmid)))
            self.conn.commit()
            return cursor.rowcount > 0
        except Exception as e:
            logging.error(f"Failed to delete VM ACL: {e}")
            return False
    
    # ========================================
    # POOL PERMISSIONS - MK Jan 2026
    # ========================================
    
    def get_pool_permissions(self, cluster_id: str, pool_id: str = None) -> List[Dict]:
        """Get pool permissions, optionally filtered by pool_id"""
        cursor = self.conn.cursor()
        if pool_id:
            cursor.execute('''
                SELECT * FROM pool_permissions 
                WHERE cluster_id = ? AND pool_id = ?
            ''', (cluster_id, pool_id))
        else:
            cursor.execute('''
                SELECT * FROM pool_permissions WHERE cluster_id = ?
            ''', (cluster_id,))
        
        rows = cursor.fetchall()
        result = []
        for row in rows:
            result.append({
                'id': row[0],
                'cluster_id': row[1],
                'pool_id': row[2],
                'subject_type': row[3],  # 'user' or 'group'
                'subject_id': row[4],    # username or group name
                'permissions': json.loads(row[5]) if row[5] else [],
                'created_at': row[6],
                'updated_at': row[7]
            })
        return result
    
    def save_pool_permission(self, cluster_id: str, pool_id: str, subject_type: str, 
                            subject_id: str, permissions: List[str]) -> bool:
        """Save or update pool permission"""
        try:
            cursor = self.conn.cursor()
            now = datetime.now().isoformat()
            cursor.execute('''
                INSERT INTO pool_permissions (cluster_id, pool_id, subject_type, subject_id, permissions, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(cluster_id, pool_id, subject_type, subject_id) 
                DO UPDATE SET permissions = ?, updated_at = ?
            ''', (cluster_id, pool_id, subject_type, subject_id, json.dumps(permissions), now, now,
                  json.dumps(permissions), now))
            self.conn.commit()
            return True
        except Exception as e:
            logging.error(f"Failed to save pool permission: {e}")
            return False
    
    def delete_pool_permission(self, cluster_id: str, pool_id: str, subject_type: str, subject_id: str) -> bool:
        """Delete a pool permission"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                DELETE FROM pool_permissions 
                WHERE cluster_id = ? AND pool_id = ? AND subject_type = ? AND subject_id = ?
            ''', (cluster_id, pool_id, subject_type, subject_id))
            self.conn.commit()
            return cursor.rowcount > 0
        except Exception as e:
            logging.error(f"Failed to delete pool permission: {e}")
            return False
    
    def get_user_pool_permissions(self, cluster_id: str, username: str, groups: List[str] = None) -> Dict[str, List[str]]:
        """Get all pool permissions for a user (including via group membership)
        Returns: {pool_id: [permissions]}
        """
        cursor = self.conn.cursor()
        
        # Get direct user permissions
        cursor.execute('''
            SELECT pool_id, permissions FROM pool_permissions 
            WHERE cluster_id = ? AND subject_type = 'user' AND subject_id = ?
        ''', (cluster_id, username))
        
        result = {}
        for row in cursor.fetchall():
            pool_id = row[0]
            perms = json.loads(row[1]) if row[1] else []
            result[pool_id] = perms
        
        # Get group permissions
        if groups:
            for group in groups:
                cursor.execute('''
                    SELECT pool_id, permissions FROM pool_permissions 
                    WHERE cluster_id = ? AND subject_type = 'group' AND subject_id = ?
                ''', (cluster_id, group))
                
                for row in cursor.fetchall():
                    pool_id = row[0]
                    perms = json.loads(row[1]) if row[1] else []
                    if pool_id in result:
                        # Merge permissions (union)
                        result[pool_id] = list(set(result[pool_id] + perms))
                    else:
                        result[pool_id] = perms
        
        return result
    
    # ========================================
    # KEY ROTATION (HIPAA/ISO Compliance)
    # ========================================
    
    def rotate_encryption_key(self) -> dict:
        """Rotate the AES-256 encryption key and re-encrypt all data
        
        This is required for HIPAA/ISO 27001 compliance (periodic key rotation).
        Process:
        1. Generate new AES-256 key
        2. Decrypt all encrypted data with old key
        3. Re-encrypt with new key
        4. Replace old key file
        
        Returns statistics about the rotation.
        """
        if not ENCRYPTION_AVAILABLE or not self.aesgcm:
            return {'error': 'Encryption not available'}
        
        aes_key_file = os.path.join(CONFIG_DIR, '.pegaprox_aes256.key')
        
        # Load old key
        with open(aes_key_file, 'rb') as f:
            old_key = f.read()
        old_aesgcm = AESGCM(old_key)
        
        # Generate new key
        new_key = os.urandom(32)  # 256 bits
        new_aesgcm = AESGCM(new_key)
        
        stats = {
            'users_rotated': 0,
            'clusters_rotated': 0,
            'sessions_rotated': 0,
            'errors': []
        }
        
        try:
            cursor = self.conn.cursor()
            
            # 1. Rotate user secrets (totp_secret_encrypted)
            cursor.execute('SELECT username, totp_secret_encrypted FROM users WHERE totp_secret_encrypted IS NOT NULL AND totp_secret_encrypted != ""')
            for row in cursor.fetchall():
                try:
                    encrypted = row['totp_secret_encrypted']
                    if encrypted and encrypted.startswith('aes256:'):
                        # Decrypt with old key
                        decrypted = self._decrypt_with_key(encrypted, old_aesgcm)
                        # Re-encrypt with new key
                        new_encrypted = self._encrypt_with_key(decrypted, new_aesgcm)
                        # Update
                        cursor.execute('UPDATE users SET totp_secret_encrypted = ? WHERE username = ?',
                                     (new_encrypted, row['username']))
                        stats['users_rotated'] += 1
                except Exception as e:
                    stats['errors'].append(f"User {row['username']}: {str(e)}")
            
            # 2. Rotate cluster credentials
            cursor.execute('SELECT id, password_encrypted FROM clusters WHERE password_encrypted IS NOT NULL AND password_encrypted != ""')
            for row in cursor.fetchall():
                try:
                    encrypted = row['password_encrypted']
                    if encrypted and encrypted.startswith('aes256:'):
                        decrypted = self._decrypt_with_key(encrypted, old_aesgcm)
                        new_encrypted = self._encrypt_with_key(decrypted, new_aesgcm)
                        cursor.execute('UPDATE clusters SET password_encrypted = ? WHERE id = ?',
                                     (new_encrypted, row['id']))
                        stats['clusters_rotated'] += 1
                except Exception as e:
                    stats['errors'].append(f"Cluster {row['id']}: {str(e)}")
            
            # Also rotate SSH keys and API tokens if present
            cursor.execute('SELECT id, ssh_key_encrypted, api_token_encrypted FROM clusters')
            for row in cursor.fetchall():
                try:
                    updated = False
                    ssh_key = row['ssh_key_encrypted']
                    api_token = row['api_token_encrypted']
                    
                    if ssh_key and ssh_key.startswith('aes256:'):
                        decrypted = self._decrypt_with_key(ssh_key, old_aesgcm)
                        new_encrypted = self._encrypt_with_key(decrypted, new_aesgcm)
                        cursor.execute('UPDATE clusters SET ssh_key_encrypted = ? WHERE id = ?',
                                     (new_encrypted, row['id']))
                        updated = True
                    
                    if api_token and api_token.startswith('aes256:'):
                        decrypted = self._decrypt_with_key(api_token, old_aesgcm)
                        new_encrypted = self._encrypt_with_key(decrypted, new_aesgcm)
                        cursor.execute('UPDATE clusters SET api_token_encrypted = ? WHERE id = ?',
                                     (new_encrypted, row['id']))
                        updated = True
                except Exception as e:
                    stats['errors'].append(f"Cluster secrets {row['id']}: {str(e)}")
            
            # 3. Rotate session data if encrypted
            cursor.execute('SELECT token, data_encrypted FROM sessions WHERE data_encrypted IS NOT NULL AND data_encrypted != ""')
            for row in cursor.fetchall():
                try:
                    encrypted = row['data_encrypted']
                    if encrypted and encrypted.startswith('aes256:'):
                        decrypted = self._decrypt_with_key(encrypted, old_aesgcm)
                        new_encrypted = self._encrypt_with_key(decrypted, new_aesgcm)
                        cursor.execute('UPDATE sessions SET data_encrypted = ? WHERE token = ?',
                                     (new_encrypted, row['token']))
                        stats['sessions_rotated'] += 1
                except Exception as e:
                    stats['errors'].append(f"Session: {str(e)}")
            
            self.conn.commit()
            
            # 4. Save new key (backup old key first)
            backup_file = aes_key_file + f'.backup.{datetime.now().strftime("%Y%m%d_%H%M%S")}'
            with open(backup_file, 'wb') as f:
                f.write(old_key)
            os.chmod(backup_file, 0o600)
            
            with open(aes_key_file, 'wb') as f:
                f.write(new_key)
            os.chmod(aes_key_file, 0o600)
            
            # 5. Update in-memory key
            self.aes_key = new_key
            self.aesgcm = new_aesgcm
            
            stats['success'] = True
            stats['key_backup'] = backup_file
            stats['rotated_at'] = datetime.now().isoformat()
            
            logging.info(f"Key rotation completed: {stats['users_rotated']} users, {stats['clusters_rotated']} clusters, {stats['sessions_rotated']} sessions")
            
        except Exception as e:
            stats['success'] = False
            stats['error'] = str(e)
            logging.error(f"Key rotation failed: {e}")
            self.conn.rollback()
        
        return stats
    
    def _encrypt_with_key(self, data: str, aesgcm) -> str:
        """Encrypt data with specific AESGCM key"""
        if not data:
            return data
        nonce = os.urandom(12)
        ciphertext = aesgcm.encrypt(nonce, data.encode('utf-8'), None)
        encrypted = base64.b64encode(nonce + ciphertext).decode('utf-8')
        return f"aes256:{encrypted}"
    
    def _decrypt_with_key(self, data: str, aesgcm) -> str:
        """Decrypt data with specific AESGCM key"""
        if not data:
            return data
        if data.startswith('aes256:'):
            encrypted_data = base64.b64decode(data[7:])
            nonce = encrypted_data[:12]
            ciphertext = encrypted_data[12:]
            return aesgcm.decrypt(nonce, ciphertext, None).decode('utf-8')
        return data
    
    def get_key_info(self) -> dict:
        """Get information about the current encryption key"""
        aes_key_file = os.path.join(CONFIG_DIR, '.pegaprox_aes256.key')
        
        if not os.path.exists(aes_key_file):
            return {'exists': False}
        
        stat = os.stat(aes_key_file)
        
        # Find backup files
        backups = []
        for f in os.listdir(CONFIG_DIR):
            if f.startswith('.pegaprox_aes256.key.backup'):
                backup_path = os.path.join(CONFIG_DIR, f)
                backup_stat = os.stat(backup_path)
                backups.append({
                    'filename': f,
                    'created': datetime.fromtimestamp(backup_stat.st_mtime).isoformat()
                })
        
        return {
            'exists': True,
            'created': datetime.fromtimestamp(stat.st_ctime).isoformat(),
            'last_modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
            'algorithm': 'AES-256-GCM',
            'key_size_bits': 256,
            'backups': sorted(backups, key=lambda x: x['created'], reverse=True)
        }
    
    # ========================================
    # AFFINITY RULES OPERATIONS
    # ========================================
    
    def get_affinity_rules(self, cluster_id: str = None) -> dict:
        """Get affinity rules"""
        cursor = self.conn.cursor()
        
        if cluster_id:
            cursor.execute('SELECT * FROM affinity_rules WHERE cluster_id = ?', (cluster_id,))
        else:
            cursor.execute('SELECT * FROM affinity_rules')
        
        rules = {}
        for row in cursor.fetchall():
            cid = row['cluster_id']
            if cid not in rules:
                rules[cid] = []
            rules[cid].append({
                'id': row['id'],
                'name': row['name'],
                'type': row['type'],
                'vms': json.loads(row['vms'] or '[]'),
                'enabled': bool(row['enabled']),
            })
        
        return rules
    
    def save_affinity_rule(self, rule_id: str, cluster_id: str, data: dict):
        """Save affinity rule"""
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        cursor.execute('''
            INSERT OR REPLACE INTO affinity_rules
            (id, cluster_id, name, type, vms, enabled, created_at)
            VALUES (?, ?, ?, ?, ?, ?, COALESCE((SELECT created_at FROM affinity_rules WHERE id = ?), ?))
        ''', (
            rule_id,
            cluster_id,
            data.get('name', ''),
            data.get('type', 'affinity'),
            json.dumps(data.get('vms', [])),
            1 if data.get('enabled', True) else 0,
            rule_id, now
        ))
        self.conn.commit()
    
    def delete_affinity_rule(self, rule_id: str):
        """Delete affinity rule"""
        cursor = self.conn.cursor()
        cursor.execute('DELETE FROM affinity_rules WHERE id = ?', (rule_id,))
        self.conn.commit()
    
    def save_all_affinity_rules(self, rules: dict):
        """Save all affinity rules"""
        for cluster_id, cluster_rules in rules.items():
            for rule in cluster_rules:
                self.save_affinity_rule(rule.get('id', str(uuid.uuid4())[:8]), cluster_id, rule)
    
    # ========================================
    # SERVER SETTINGS OPERATIONS
    # ========================================
    
    def get_server_settings(self) -> dict:
        """Get all server settings"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM server_settings')
        
        settings = {}
        for row in cursor.fetchall():
            try:
                settings[row['key']] = json.loads(row['value'])
            except:
                settings[row['key']] = row['value']
        
        return settings
    
    def get_server_setting(self, key: str, default=None):
        """Get single server setting"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT value FROM server_settings WHERE key = ?', (key,))
        row = cursor.fetchone()
        
        if not row:
            return default
        
        try:
            return json.loads(row['value'])
        except:
            return row['value']
    
    def save_server_setting(self, key: str, value):
        """Save server setting - always JSON encode to ensure consistent retrieval"""
        cursor = self.conn.cursor()
        # Always JSON encode the value for consistent storage and retrieval
        json_value = json.dumps(value)
        cursor.execute('''
            INSERT OR REPLACE INTO server_settings (key, value)
            VALUES (?, ?)
        ''', (key, json_value))
        self.conn.commit()
    
    def save_server_settings(self, settings: dict):
        """Save all server settings"""
        for key, value in settings.items():
            self.save_server_setting(key, value)

    # ========================================
    # AUTH PROVIDER OPERATIONS - Feb 2026
    # NS: LDAP and OAuth2/OIDC external authentication
    # ========================================

    def get_auth_providers(self, enabled_only: bool = False) -> list:
        """Get all authentication providers

        Args:
            enabled_only: If True, only return enabled providers

        Returns:
            List of provider dicts with decrypted config
        """
        cursor = self.conn.cursor()
        if enabled_only:
            cursor.execute('SELECT * FROM auth_providers WHERE enabled = 1 ORDER BY priority ASC')
        else:
            cursor.execute('SELECT * FROM auth_providers ORDER BY priority ASC')

        providers = []
        for row in cursor.fetchall():
            provider = {
                'id': row['id'],
                'name': row['name'],
                'type': row['type'],
                'enabled': bool(row['enabled']),
                'priority': row['priority'],
                'default_role': row['default_role'],
                'auto_create_users': bool(row['auto_create_users']),
                'created_at': row['created_at'],
                'updated_at': row['updated_at'],
            }
            # Decrypt config
            try:
                config_json = self._decrypt(row['config_encrypted'])
                provider['config'] = json.loads(config_json)
            except Exception as e:
                logging.error(f"Failed to decrypt config for provider {row['id']}: {e}")
                provider['config'] = {}
            providers.append(provider)

        return providers

    def get_auth_provider(self, provider_id: str) -> dict:
        """Get single auth provider by ID

        Returns:
            Provider dict with decrypted config, or None if not found
        """
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM auth_providers WHERE id = ?', (provider_id,))
        row = cursor.fetchone()

        if not row:
            return None

        provider = {
            'id': row['id'],
            'name': row['name'],
            'type': row['type'],
            'enabled': bool(row['enabled']),
            'priority': row['priority'],
            'default_role': row['default_role'],
            'auto_create_users': bool(row['auto_create_users']),
            'created_at': row['created_at'],
            'updated_at': row['updated_at'],
        }
        # Decrypt config
        try:
            config_json = self._decrypt(row['config_encrypted'])
            provider['config'] = json.loads(config_json)
        except Exception as e:
            logging.error(f"Failed to decrypt config for provider {provider_id}: {e}")
            provider['config'] = {}

        return provider

    def save_auth_provider(self, provider: dict) -> bool:
        """Save or update auth provider

        Args:
            provider: Provider dict with config to save

        Returns:
            True on success
        """
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()

        # Encrypt the config
        config_json = json.dumps(provider.get('config', {}))
        config_encrypted = self._encrypt(config_json)

        cursor.execute('''
            INSERT OR REPLACE INTO auth_providers
            (id, name, type, enabled, priority, config_encrypted, default_role,
             auto_create_users, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?,
                    COALESCE((SELECT created_at FROM auth_providers WHERE id = ?), ?), ?)
        ''', (
            provider['id'],
            provider.get('name', ''),
            provider.get('type', 'oidc'),
            1 if provider.get('enabled', True) else 0,
            provider.get('priority', 100),
            config_encrypted,
            provider.get('default_role', 'viewer'),
            1 if provider.get('auto_create_users', False) else 0,
            provider['id'], now, now
        ))
        self.conn.commit()
        return True

    def delete_auth_provider(self, provider_id: str) -> bool:
        """Delete auth provider and associated mappings

        Returns:
            True on success
        """
        cursor = self.conn.cursor()
        # Foreign key ON DELETE CASCADE will handle mappings and identities
        cursor.execute('DELETE FROM auth_providers WHERE id = ?', (provider_id,))
        self.conn.commit()
        return cursor.rowcount > 0

    def get_group_mappings(self, provider_id: str) -> list:
        """Get group-to-role mappings for a provider

        Returns:
            List of mapping dicts sorted by priority
        """
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT * FROM auth_group_mappings
            WHERE provider_id = ?
            ORDER BY priority ASC
        ''', (provider_id,))

        return [{
            'id': row['id'],
            'provider_id': row['provider_id'],
            'external_group': row['external_group'],
            'role': row['role'],
            'tenant_id': row['tenant_id'],
            'priority': row['priority'],
            'created_at': row['created_at'],
        } for row in cursor.fetchall()]

    def save_group_mapping(self, mapping: dict) -> int:
        """Save group-to-role mapping

        Args:
            mapping: Mapping dict with provider_id, external_group, role, etc.

        Returns:
            The mapping ID
        """
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()

        if mapping.get('id'):
            # Update existing
            cursor.execute('''
                UPDATE auth_group_mappings
                SET external_group = ?, role = ?, tenant_id = ?, priority = ?
                WHERE id = ?
            ''', (
                mapping['external_group'],
                mapping['role'],
                mapping.get('tenant_id', '_default'),
                mapping.get('priority', 100),
                mapping['id']
            ))
            return mapping['id']
        else:
            # Insert new
            cursor.execute('''
                INSERT INTO auth_group_mappings
                (provider_id, external_group, role, tenant_id, priority, created_at)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                mapping['provider_id'],
                mapping['external_group'],
                mapping['role'],
                mapping.get('tenant_id', '_default'),
                mapping.get('priority', 100),
                now
            ))
            self.conn.commit()
            return cursor.lastrowid

    def delete_group_mapping(self, mapping_id: int) -> bool:
        """Delete group mapping by ID

        Returns:
            True if deleted
        """
        cursor = self.conn.cursor()
        cursor.execute('DELETE FROM auth_group_mappings WHERE id = ?', (mapping_id,))
        self.conn.commit()
        return cursor.rowcount > 0

    def get_user_external_identities(self, username: str) -> list:
        """Get external identities linked to a user

        Returns:
            List of identity dicts
        """
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT ei.*, ap.name as provider_name, ap.type as provider_type
            FROM user_external_identities ei
            LEFT JOIN auth_providers ap ON ei.provider_id = ap.id
            WHERE ei.username = ?
        ''', (username,))

        return [{
            'id': row['id'],
            'username': row['username'],
            'provider_id': row['provider_id'],
            'provider_name': row['provider_name'],
            'provider_type': row['provider_type'],
            'external_id': row['external_id'],
            'external_username': row['external_username'],
            'external_email': row['external_email'],
            'external_groups': json.loads(row['external_groups'] or '[]'),
            'last_sync': row['last_sync'],
            'created_at': row['created_at'],
        } for row in cursor.fetchall()]

    def link_external_identity(self, username: str, provider_id: str, external_id: str,
                               external_username: str = None, external_email: str = None,
                               external_groups: list = None) -> bool:
        """Link external identity to a PegaProx user

        Args:
            username: PegaProx username
            provider_id: Auth provider ID
            external_id: External unique ID (LDAP DN or OIDC sub)
            external_username: Username from external provider
            external_email: Email from external provider
            external_groups: List of groups from external provider

        Returns:
            True on success
        """
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()

        cursor.execute('''
            INSERT OR REPLACE INTO user_external_identities
            (username, provider_id, external_id, external_username, external_email,
             external_groups, last_sync, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?,
                    COALESCE((SELECT created_at FROM user_external_identities
                              WHERE provider_id = ? AND external_id = ?), ?))
        ''', (
            username,
            provider_id,
            external_id,
            external_username,
            external_email,
            json.dumps(external_groups or []),
            now,
            provider_id, external_id, now
        ))
        self.conn.commit()
        return True

    def unlink_external_identity(self, identity_id: int) -> bool:
        """Remove external identity link

        Returns:
            True if deleted
        """
        cursor = self.conn.cursor()
        cursor.execute('DELETE FROM user_external_identities WHERE id = ?', (identity_id,))
        self.conn.commit()
        return cursor.rowcount > 0

    def find_user_by_external_id(self, provider_id: str, external_id: str) -> str:
        """Find PegaProx username by external ID

        Args:
            provider_id: Auth provider ID
            external_id: External unique ID

        Returns:
            Username if found, None otherwise
        """
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT username FROM user_external_identities
            WHERE provider_id = ? AND external_id = ?
        ''', (provider_id, external_id))
        row = cursor.fetchone()
        return row['username'] if row else None

    def update_external_identity_groups(self, provider_id: str, external_id: str,
                                        groups: list) -> bool:
        """Update external groups for an identity

        Args:
            provider_id: Auth provider ID
            external_id: External unique ID
            groups: Updated list of groups

        Returns:
            True if updated
        """
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        cursor.execute('''
            UPDATE user_external_identities
            SET external_groups = ?, last_sync = ?
            WHERE provider_id = ? AND external_id = ?
        ''', (json.dumps(groups), now, provider_id, external_id))
        self.conn.commit()
        return cursor.rowcount > 0

    # ========================================
    # TENANTS OPERATIONS
    # ========================================
    
    def get_all_tenants(self) -> list:
        """Get all tenants"""
        cursor = self.conn.cursor()
        cursor.execute('SELECT * FROM tenants')
        
        return [{
            'id': row['id'],
            'name': row['name'],
            'clusters': json.loads(row['clusters'] or '[]'),
        } for row in cursor.fetchall()]
    
    def save_tenant(self, tenant_id: str, data: dict):
        """Save tenant"""
        cursor = self.conn.cursor()
        now = datetime.now().isoformat()
        
        cursor.execute('''
            INSERT OR REPLACE INTO tenants (id, name, clusters, created_at)
            VALUES (?, ?, ?, COALESCE((SELECT created_at FROM tenants WHERE id = ?), ?))
        ''', (
            tenant_id,
            data.get('name', ''),
            json.dumps(data.get('clusters', [])),
            tenant_id, now
        ))
        self.conn.commit()
    
    def delete_tenant(self, tenant_id: str):
        """Delete tenant"""
        cursor = self.conn.cursor()
        cursor.execute('DELETE FROM tenants WHERE id = ?', (tenant_id,))
        self.conn.commit()
    
    def save_all_tenants(self, tenants: list):
        """Save all tenants"""
        for tenant in tenants:
            self.save_tenant(tenant.get('id', str(uuid.uuid4())[:8]), tenant)
    
    # Generic query methods for custom tables like scripts
    def execute(self, sql: str, params: tuple = ()):
        """Execute SQL statement (CREATE, INSERT, UPDATE, DELETE)"""
        cursor = self.conn.cursor()
        cursor.execute(sql, params)
        self.conn.commit()
    
    def query(self, sql: str, params: tuple = ()) -> list:
        """Execute SQL query and return all results as list of Row objects"""
        cursor = self.conn.cursor()
        cursor.row_factory = sqlite3.Row
        cursor.execute(sql, params)
        return cursor.fetchall()
    
    def query_one(self, sql: str, params: tuple = ()):
        """Execute SQL query and return first result or None"""
        cursor = self.conn.cursor()
        cursor.row_factory = sqlite3.Row
        cursor.execute(sql, params)
        return cursor.fetchone()


# Global database instance
_db = None

def get_db() -> PegaProxDB:
    """Get database instance (singleton)"""
    global _db
    if _db is None:
        _db = PegaProxDB()
    return _db


def get_session_timeout():
    # get timeout from settings
    try:
        settings = load_server_settings()
        return settings.get('session_timeout', SESSION_TIMEOUT)
    except:
        return SESSION_TIMEOUT  # fallback
SESSION_SECRET = None  # Will be generated/loaded

# Brute force protection (defaults, can be overridden via settings)
# NS: increased these after we noticed during testing about lockouts
LOGIN_MAX_ATTEMPTS = 5  # Max failed attempts before lockout
LOGIN_LOCKOUT_TIME = 300  # Lockout time in seconds (5 minutes)
LOGIN_ATTEMPT_WINDOW = 600  # Time window for counting attempts (10 minutes)
login_attempts_by_ip = {}  # ip -> {'attempts': [...timestamps], 'locked_until': timestamp}
login_attempts_by_user = {}  # username -> {'attempts': [...timestamps], 'locked_until': timestamp}
# MK: Security fix - track both IP and username to prevent distributed attacks


# helper for formatting - not sure if used anywhere anymore
def _fmt_size(size_bytes):
    # NS: simple bytes formatter, nothing fancy
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024**2:
        return f"{size_bytes/1024:.1f} KB"
    elif size_bytes < 1024**3:
        return f"{size_bytes/1024**2:.1f} MB"
    else:
        return f"{size_bytes/1024**3:.1f} GB"
    # TODO: add TB support? probably overkill


def get_login_settings():
    # MK: pulled these out to be configurable via settings
    try:
        settings = load_server_settings()
    except:
        settings = {}  # w/e just use defaults
    return {
        'max_attempts': settings.get('login_max_attempts', LOGIN_MAX_ATTEMPTS),
        'lockout_time': settings.get('login_lockout_time', LOGIN_LOCKOUT_TIME),
        'attempt_window': settings.get('login_attempt_window', LOGIN_ATTEMPT_WINDOW)
    }

# Audit log configuration
# LW: 90 days seems reasonable, adjust if disk space becomes an issue
AUDIT_RETENTION_DAYS = 90  # 3 months should be enough

# Ensure directories exist
Path(LOG_DIR).mkdir(exist_ok=True)
Path(WEB_DIR).mkdir(exist_ok=True)
Path(SSL_DIR).mkdir(exist_ok=True)

# Global cluster managers
cluster_managers = {}  # cluster_id -> PegaProxManager

# Global SSH connection management - NS Jan 2026
# 
# Design: Two-tier system with HA priority
# - HA operations (fencing, heartbeat): NO LIMIT - these are critical and short
# - Normal operations (updates, SMBIOS deploy): Limited by semaphore
#
# Default 25 allows comfortable handling of 2-3 clusters with 15+ nodes each
# doing simultaneous rolling updates. Increase for larger environments.
#
SSH_MAX_CONCURRENT = int(os.environ.get('PEGAPROX_SSH_MAX_CONCURRENT', 25))
_ssh_semaphore = threading.BoundedSemaphore(SSH_MAX_CONCURRENT)
_ssh_active_connections = {'normal': 0, 'ha': 0}
_ssh_connection_lock = threading.Lock()

def get_ssh_connection_stats():
    """Get current SSH connection statistics"""
    with _ssh_connection_lock:
        return {
            'max_concurrent': SSH_MAX_CONCURRENT,
            'active_normal': _ssh_active_connections['normal'],
            'active_ha': _ssh_active_connections['ha'],
            'total_active': _ssh_active_connections['normal'] + _ssh_active_connections['ha']
        }

def _ssh_track_connection(conn_type: str, delta: int):
    """Track SSH connection count"""
    with _ssh_connection_lock:
        _ssh_active_connections[conn_type] = max(0, _ssh_active_connections[conn_type] + delta)

# Global sessions store
# MK: this is in-memory, will be lost on restart
# TODO: persist to redis or file?
active_sessions = {}  # session_id -> {user, created_at, last_activity, role}

# Global audit log (in-memory cache, persisted to file)
audit_log = []
MAX_AUDIT_LOG_SIZE = 10000  # not actually enforced lol

# User roles
ROLE_ADMIN = 'admin'
ROLE_USER = 'user'
# ROLE_READONLY = 'readonly'  # TODO maybe add this? - LW
ROLE_VIEWER = 'viewer'

# =============================================================================
# PERMISSIONS - added oct 2025
# LW: asked chatgpt for ideas, NS fixed them to match proxmox
# this took way longer than expected lol
# =============================================================================

PERMISSIONS = {
    # vm stuff
    'vm.view': 'View VMs and containers',
    'vm.start': 'Start VMs and containers',
    'vm.stop': 'Stop VMs and containers',
    'vm.restart': 'Restart VMs and containers',
    'vm.console': 'Access VM console',
    'vm.migrate': 'Migrate VMs between nodes',
    'vm.clone': 'Clone VMs',
    'vm.delete': 'Delete VMs and containers',
    'vm.create': 'Create new VMs and containers',
    'vm.config': 'Modify VM configuration',
    'vm.snapshot': 'Create/delete snapshots',
    'vm.backup': 'Backup VMs',
    'vm.template': 'Convert to/from template',
    
    # Cluster permissions
    'cluster.view': 'View cluster info',
    'cluster.add': 'Add new clusters',
    'cluster.delete': 'Remove clusters',
    'cluster.config': 'Modify cluster settings',
    'cluster.join': 'Join nodes to cluster',
    'cluster.admin': 'Full cluster administration (join/remove nodes)',
    
    # Node permissions
    'node.view': 'View node status',
    'node.shell': 'Access node shell',
    'node.maintenance': 'Toggle maintenance mode',
    'node.update': 'Update node packages',
    'node.reboot': 'Reboot nodes',
    'node.network': 'Modify network config',
    'node.config': 'Change node options',
    'node.certificate': 'Manage SSL certificates',
    'node.power': 'Shutdown/wake nodes',  # WoL etc
    
    # Storage permissions
    'storage.view': 'View storage',
    'storage.upload': 'Upload files to storage',
    'storage.delete': 'Delete files from storage',
    'storage.config': 'Modify storage config',
    'storage.create': 'Add new storage',
    'storage.download': 'Download ISOs/templates',
    
    # Backup permissions
    'backup.view': 'View backups',
    'backup.create': 'Create backups',
    'backup.restore': 'Restore from backup',
    'backup.delete': 'Delete backups',
    'backup.schedule': 'Manage backup schedules',
    'backup.config': 'Configure backup storage',
    
    # HA permissions
    'ha.view': 'View HA status',
    'ha.config': 'Configure HA settings',
    'ha.groups': 'Manage HA groups',
    'ha.resources': 'Add/remove HA resources',
    
    # Firewall permissions - MK: needed for datacenter firewall
    'firewall.view': 'View firewall rules',
    'firewall.edit': 'Modify firewall rules',
    'firewall.aliases': 'Manage aliases/IPsets',
    
    # Pool/Resource permissions
    'pool.view': 'View resource pools',
    'pool.manage': 'Create/delete pools',
    'pool.assign': 'Assign VMs to pools',
    
    # Replication
    'replication.view': 'View replication jobs',
    'replication.manage': 'Create/delete replication',
    
    # Admin permissions
    'admin.users': 'Manage users',
    'admin.roles': 'Manage custom roles',  # NEW
    'admin.tenants': 'Manage tenants',
    'admin.groups': 'Manage cluster groups',  # NS: Cluster organization
    'admin.settings': 'Modify system settings',
    'admin.scripts': 'Manage and execute custom scripts',  # MK: Custom Scripts
    'admin.audit': 'View audit logs',
    'admin.api': 'Manage API tokens',
}

# builtin roles - these cant be deleted
BUILTIN_ROLES = [ROLE_ADMIN, ROLE_USER, ROLE_VIEWER]

# default permissions per role
# LW: easier to maintain than checking roles everywhere
ROLE_PERMISSIONS = {
    ROLE_ADMIN: list(PERMISSIONS.keys()),  # admin gets everything
    ROLE_USER: [
        # VMs
        'vm.view', 'vm.start', 'vm.stop', 'vm.restart', 'vm.console', 'vm.migrate',
        'vm.clone', 'vm.config', 'vm.snapshot', 'vm.backup',
        'cluster.view',
        'node.view',
        'storage.view', 'storage.upload', 'storage.download',
        'backup.view', 'backup.create', 'backup.restore', 'backup.delete',  # MK: users need to delete their own backups
        'ha.view',
        'firewall.view',
        'pool.view', 'pool.assign',  # MK: added assign so users can organize their VMs
        'replication.view',
    ],
    ROLE_VIEWER: [
        'vm.view', 'vm.console',
        'cluster.view',
        'node.view', 
        'storage.view',
        'backup.view',
        'ha.view',
        'firewall.view',
        'pool.view',
        'replication.view',
    ],
}

# =============================================================================
# CUSTOM ROLES - MK oct 2025
# Users can create their own roles - either global or per-tenant
# =============================================================================

CUSTOM_ROLES_FILE = os.path.join(CONFIG_DIR, 'custom_roles.json')

def load_custom_roles() -> dict:
    """Load custom roles from SQLite database
    
    moved to SQLite
    
    Structure:
    {
        "global": {
            "role_id": {"name": "...", "permissions": [...], "created_by": "..."}
        },
        "tenants": {
            "tenant_id": {
                "role_id": {"name": "...", "permissions": [...]}
            }
        }
    }
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM custom_roles')
        
        global_roles = {}
        tenant_roles = {}
        
        for row in cursor.fetchall():
            role_data = {
                'name': row['name'],
                'permissions': json.loads(row['permissions'] or '[]'),
                'description': row['description'] or ''
            }
            
            # Check if role has tenant_id (might not exist in old schema)
            tenant_id = None
            try:
                tenant_id = row['tenant_id']
            except (IndexError, KeyError):
                pass
            
            # Empty string or None means global role
            if tenant_id and tenant_id != '':
                # Tenant-specific role
                if tenant_id not in tenant_roles:
                    tenant_roles[tenant_id] = {}
                tenant_roles[tenant_id][row['name']] = role_data
            else:
                # Global role
                global_roles[row['name']] = role_data
        
        return {'global': global_roles, 'tenants': tenant_roles}
    except Exception as e:
        logging.error(f"Error loading custom roles from database: {e}")
        # Legacy fallback
        if os.path.exists(CUSTOM_ROLES_FILE):
            try:
                with open(CUSTOM_ROLES_FILE, 'r') as f:
                    return json.load(f)
            except:
                pass
    
    return {'global': {}, 'tenants': {}}


def save_custom_roles(roles: dict):
    """Save custom roles to SQLite database
    
    uses SQLite now
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        # Clear existing roles
        cursor.execute('DELETE FROM custom_roles')
        
        now = datetime.now().isoformat()
        
        # Save global roles (use empty string for tenant_id to work with composite key)
        for role_id, role_data in roles.get('global', {}).items():
            cursor.execute('''
                INSERT INTO custom_roles (name, permissions, description, tenant_id, created_at)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                role_id,
                json.dumps(role_data.get('permissions', [])),
                role_data.get('description', ''),
                '',  # Empty string for global roles
                now
            ))
        
        # Save tenant-specific roles
        for tenant_id, tenant_roles in roles.get('tenants', {}).items():
            for role_id, role_data in tenant_roles.items():
                cursor.execute('''
                    INSERT INTO custom_roles (name, permissions, description, tenant_id, created_at)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    role_id,
                    json.dumps(role_data.get('permissions', [])),
                    role_data.get('description', ''),
                    tenant_id,
                    now
                ))
        
        db.conn.commit()
    except Exception as e:
        logging.error(f"Failed to save custom roles: {e}")

# cache
_custom_roles_cache = None

def get_custom_roles():
    global _custom_roles_cache
    if _custom_roles_cache is None:
        _custom_roles_cache = load_custom_roles()
    return _custom_roles_cache

def invalidate_roles_cache():
    global _custom_roles_cache
    _custom_roles_cache = None

def get_role_permissions_for_user(user: dict, tenant_id: str = None) -> list:
    """Get permissions for a role, considering custom roles
    
    Priority:
    1. Builtin role (admin/user/viewer)
    2. Tenant-specific custom role
    3. Global custom role
    """
    role = user.get('role', ROLE_VIEWER)
    
    # builtin role?
    if role in ROLE_PERMISSIONS:
        return ROLE_PERMISSIONS[role].copy()
    
    # check custom roles
    custom = get_custom_roles()
    
    # tenant specific first
    if tenant_id:
        tenant_roles = custom.get('tenants', {}).get(tenant_id, {})
        if role in tenant_roles:
            return tenant_roles[role].get('permissions', []).copy()
    
    # global custom role
    global_roles = custom.get('global', {})
    if role in global_roles:
        return global_roles[role].get('permissions', []).copy()
    
    # fallback to viewer
    return ROLE_PERMISSIONS[ROLE_VIEWER].copy()

# =============================================================================
# MULTI-TENANCY
# Feature requested on Reddit (r/selfhosted) - MSPs wanted to manage multiple 
# customers from one PegaProx instance without them seeing each others VMs.
# Took about a weekend to implement properly.
#
# Tenants are like organizations - users belong to tenants
# Each tenant can only see clusters assigned to them
# =============================================================================

TENANTS_FILE = os.path.join(CONFIG_DIR, 'tenants.json')  # legacy, kept for migration
DEFAULT_TENANT_ID = 'default'  # fallback tenant for existing users

def load_tenants() -> dict:
    """Load tenants from SQLite database
    
    SQLite backend
    """
    try:
        db = get_db()
        tenants_list = db.get_all_tenants()
        
        if tenants_list:
            # Convert list to dict format
            return {t['id']: t for t in tenants_list}
    except Exception as e:
        logging.error(f"Error loading tenants from database: {e}")
        # Legacy fallback
        if os.path.exists(TENANTS_FILE):
            try:
                with open(TENANTS_FILE, 'r') as f:
                    return json.load(f)
            except:
                pass
    
    # Create default tenant
    default = {
        DEFAULT_TENANT_ID: {
            'id': DEFAULT_TENANT_ID,
            'name': 'Default',
            'clusters': [],  # empty = all clusters (for backwards compat)
            'created': datetime.now().isoformat(),
        }
    }
    save_tenants(default)
    return default


def save_tenants(tenants: dict):
    """Save tenants to SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        # Convert dict to list format
        tenants_list = list(tenants.values())
        db.save_all_tenants(tenants_list)
    except Exception as e:
        logging.error(f"Failed to save tenants: {e}")

# tenant cache - reloaded on changes
tenants_db = {}

def get_user_permissions(user: dict, tenant_id: str = None) -> list:
    """Get effective permissions for a user
    
    NS: Updated Dec 2025 - now supports tenant-specific permissions
    
    User can have different permissions per tenant via 'tenant_permissions' field:
    {
        "tenant_permissions": {
            "tenant_a": {"role": "custom_role", "extra": [...], "denied": [...]},
            "tenant_b": {"role": "viewer"}
        }
    }
    """
    # figure out which tenant we're checking for
    if not tenant_id:
        tenant_id = user.get('tenant_id', DEFAULT_TENANT_ID)
    
    # check if user has tenant-specific settings
    tenant_perms = user.get('tenant_permissions', {})
    
    if tenant_id in tenant_perms:
        # use tenant-specific role/permissions
        tp = tenant_perms[tenant_id]
        role = tp.get('role', user.get('role', ROLE_VIEWER))
        extra = tp.get('extra', [])
        denied = tp.get('denied', [])
    else:
        # use global user settings
        role = user.get('role', ROLE_VIEWER)
        extra = user.get('permissions', [])
        denied = user.get('denied_permissions', [])
    
    # get base permissions from role (supports custom roles now)
    base_perms = get_role_permissions_for_user({'role': role}, tenant_id)
    
    # add extra
    for p in extra:
        if p not in base_perms:
            base_perms.append(p)
    
    # remove denied
    base_perms = [p for p in base_perms if p not in denied]
    
    return base_perms

def has_permission(user: dict, permission: str, tenant_id: str = None) -> bool:
    """check if user has a specific permission
    
    NS: now tenant-aware
    """
    if not user:
        return False
    # admin always has access (safety net) - unless checking tenant-specific
    if user.get('role') == ROLE_ADMIN and not tenant_id:
        return True
    return permission in get_user_permissions(user, tenant_id)

def get_user_effective_role(user: dict, tenant_id: str = None) -> str:
    """Get the effective role for a user in a specific tenant"""
    if not tenant_id:
        tenant_id = user.get('tenant_id', DEFAULT_TENANT_ID)
    
    tenant_perms = user.get('tenant_permissions', {})
    if tenant_id in tenant_perms:
        return tenant_perms[tenant_id].get('role', user.get('role', ROLE_VIEWER))
    return user.get('role', ROLE_VIEWER)

def get_user_clusters(user: dict) -> list:
    """Get list of cluster IDs user can access based on tenant
    
    NS: Dec 2025 - Also checks role's tenant for tenant-specific roles
    NS: Jan 2026 - Added group-based access (tenant can be assigned to groups)
    """
    global tenants_db
    if not tenants_db:
        tenants_db = load_tenants()
    
    # admin sees all
    if user.get('role') == ROLE_ADMIN:
        return None  # None means all clusters
    
    tenant_id = user.get('tenant_id', DEFAULT_TENANT_ID)
    
    # MK: If user has default tenant but a tenant-specific role, use the role's tenant
    role = user.get('role', ROLE_VIEWER)
    if tenant_id == DEFAULT_TENANT_ID and role not in BUILTIN_ROLES:
        custom_roles = load_custom_roles()
        for tid, roles in custom_roles.get('tenants', {}).items():
            if role in roles:
                tenant_id = tid
                break
    
    tenant = tenants_db.get(tenant_id, {})
    clusters = tenant.get('clusters', [])
    
    # NS Jan 2026: Also include clusters from groups assigned to this tenant
    try:
        db = get_db()
        # Get groups assigned to this tenant
        groups = db.query('SELECT id FROM cluster_groups WHERE tenant_id = ?', (tenant_id,))
        if groups:
            group_ids = [g['id'] for g in groups]
            # Get clusters in those groups
            group_clusters = db.query('SELECT id FROM clusters WHERE group_id IN ({})'.format(
                ','.join(['?'] * len(group_ids))
            ), tuple(group_ids))
            if group_clusters:
                clusters = list(set(clusters + [c['id'] for c in group_clusters]))
    except Exception as e:
        logging.error(f"Error getting group clusters for tenant {tenant_id}: {e}")
    
    # empty list means all clusters (backwards compat) - but only for default tenant
    # LW: Changed this - non-default tenants with empty clusters should see nothing, not everything
    # was confusing before when new tenants could suddenly see everything
    if not clusters:
        if tenant_id == DEFAULT_TENANT_ID:
            return None  # default tenant can see all
        else:
            return []  # other tenants with no clusters assigned see nothing
    
    return clusters

def filter_clusters_for_user(clusters: dict, user: dict) -> dict:
    """Filter clusters dict to only show user's allowed clusters"""
    allowed = get_user_clusters(user)
    if allowed is None:
        return clusters  # user can see all
    
    return {k: v for k, v in clusters.items() if k in allowed}


# =============================================================================
# VM-LEVEL ACCESS CONTROL
# Fine-grained permissions for individual VMs/CTs
# Users can be granted or denied access to specific VMs
#
# AI-assisted: Initial structure suggested by Claude, then customized
# =============================================================================

VM_ACLS_FILE = os.path.join(CONFIG_DIR, 'vm_acls.json')

def load_vm_acls() -> dict:
    """Load VM access control lists from SQLite database
    
    SQLite migration
    
    Structure:
    {
        "cluster_id": {
            "100": {  # vmid
                "users": ["user1", "user2"],  # users with access
                "permissions": ["vm.view", "vm.console"],  # specific perms
                "inherit_role": true  # use user's role permissions
            }
        }
    }
    """
    try:
        db = get_db()
        return db.get_all_vm_acls()
    except Exception as e:
        logging.error(f"Failed to load VM ACLs from database: {e}")
        # Legacy fallback
        if os.path.exists(VM_ACLS_FILE):
            try:
                with open(VM_ACLS_FILE, 'r') as f:
                    return json.load(f)
            except:
                pass
    return {}


def save_vm_acls(acls: dict):
    """Save VM ACLs to SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        db.save_all_vm_acls(acls)
    except Exception as e:
        logging.error(f"Failed to save VM ACLs: {e}")

_vm_acls_cache = None

# MK: Pool membership cache - Jan 2026
# Structure: {cluster_id: {'data': {vmid: pool_id, ...}, 'timestamp': time, 'refreshing': bool}}
# TTL: 300 seconds (5 min) - pools don't change often
# Stale TTL: 30 seconds - return stale data while refreshing in background
_pool_membership_cache = {}
POOL_CACHE_TTL = 300  # 5 minutes - pools rarely change
POOL_CACHE_STALE_TTL = 30  # Return stale data for 30s while refreshing
_pool_cache_lock = threading.Lock()

def _refresh_pool_cache_async(cluster_id: str):
    """Background refresh of pool cache - doesn't block requests"""
    global _pool_membership_cache
    
    try:
        if cluster_id not in cluster_managers:
            return
        
        mgr = cluster_managers[cluster_id]
        pools = mgr.get_pools()
        
        membership = {}
        for pool in pools:
            pool_id = pool.get('poolid')
            if not pool_id:
                continue
            
            try:
                pool_data = mgr.get_pool_members(pool_id)
                members = pool_data.get('members', [])
                
                for member in members:
                    vmid = member.get('vmid')
                    mtype = member.get('type')
                    if vmid and mtype in ('qemu', 'lxc'):
                        membership[f"{vmid}:{mtype}"] = pool_id
            except Exception as e:
                logging.warning(f"[POOL-CACHE] Error getting members for pool {pool_id}: {e}")
                continue
        
        with _pool_cache_lock:
            _pool_membership_cache[cluster_id] = {
                'data': membership,
                'timestamp': time.time(),
                'refreshing': False
            }
        
        logging.info(f"[POOL-CACHE] Refreshed cache for cluster {cluster_id}: {len(membership)} VMs in pools")
        
    except Exception as e:
        logging.error(f"[POOL-CACHE] Error refreshing cache for {cluster_id}: {e}")
        with _pool_cache_lock:
            if cluster_id in _pool_membership_cache:
                _pool_membership_cache[cluster_id]['refreshing'] = False

def get_pool_membership_cache(cluster_id: str) -> dict:
    """Get cached pool memberships for a cluster
    
    Returns {vmid:type: pool_id, ...} mapping
    Uses stale-while-revalidate pattern for better performance
    """
    global _pool_membership_cache
    
    now = time.time()
    
    with _pool_cache_lock:
        cache_entry = _pool_membership_cache.get(cluster_id)
        
        # No cache at all - need synchronous refresh
        if not cache_entry:
            _pool_membership_cache[cluster_id] = {'data': {}, 'timestamp': 0, 'refreshing': True}
    
    if cache_entry:
        age = now - cache_entry.get('timestamp', 0)
        
        # Cache is fresh - return immediately
        if age < POOL_CACHE_TTL:
            return cache_entry.get('data', {})
        
        # Cache is stale but usable - return it and refresh in background
        if age < POOL_CACHE_TTL + POOL_CACHE_STALE_TTL:
            if not cache_entry.get('refreshing'):
                with _pool_cache_lock:
                    _pool_membership_cache[cluster_id]['refreshing'] = True
                threading.Thread(target=_refresh_pool_cache_async, args=(cluster_id,), daemon=True).start()
            return cache_entry.get('data', {})
    
    # Cache too old or missing - do synchronous refresh (only on first load)
    if cluster_id not in cluster_managers:
        return cache_entry.get('data', {}) if cache_entry else {}
    
    try:
        mgr = cluster_managers[cluster_id]
        pools = mgr.get_pools()
        
        membership = {}
        for pool in pools:
            pool_id = pool.get('poolid')
            if not pool_id:
                continue
            
            try:
                pool_data = mgr.get_pool_members(pool_id)
                members = pool_data.get('members', [])
                
                for member in members:
                    vmid = member.get('vmid')
                    mtype = member.get('type')
                    if vmid and mtype in ('qemu', 'lxc'):
                        membership[f"{vmid}:{mtype}"] = pool_id
            except:
                continue
        
        with _pool_cache_lock:
            _pool_membership_cache[cluster_id] = {
                'data': membership,
                'timestamp': now,
                'refreshing': False
            }
        
        logging.info(f"[POOL-CACHE] Initial cache for cluster {cluster_id}: {len(membership)} VMs in pools")
        return membership
        
    except Exception as e:
        logging.error(f"[POOL-CACHE] Error getting pool cache for {cluster_id}: {e}")
        return cache_entry.get('data', {}) if cache_entry else {}

def invalidate_pool_cache(cluster_id: str = None):
    """Invalidate pool membership cache"""
    global _pool_membership_cache
    with _pool_cache_lock:
        if cluster_id:
            _pool_membership_cache.pop(cluster_id, None)
        else:
            _pool_membership_cache = {}

def get_vm_pool_cached(cluster_id: str, vmid: int, vm_type: str = None) -> str:
    """Get pool for a VM using cache
    
    Much faster than direct API calls - uses cached membership data
    """
    membership = get_pool_membership_cache(cluster_id)
    
    if vm_type:
        # Exact match
        return membership.get(f"{vmid}:{vm_type}")
    else:
        # Try both types
        return membership.get(f"{vmid}:qemu") or membership.get(f"{vmid}:lxc")

def get_vm_acls():
    """Get VM ACLs - always reload from disk to avoid stale cache issues
    
    MK: Changed to always reload since ACLs are critical for security
    """
    global _vm_acls_cache
    # always reload from disk for security-critical data
    _vm_acls_cache = load_vm_acls()
    return _vm_acls_cache

def invalidate_vm_acls_cache():
    global _vm_acls_cache
    _vm_acls_cache = None

def user_can_access_vm(user: dict, cluster_id: str, vmid: int, permission: str = 'vm.view', vm_type: str = None) -> bool:
    """Check if user can access a specific VM
    
    NS: Dec 2025 - VM ACLs are ADDITIVE, not restrictive
    MK: Jan 2026 - Added Pool Permission support
    
    Logic:
    1. Admin always has access
    2. If user has VM-specific ACL entry:
       - inherit_role=True: User can do ALL VM operations (full access)
       - inherit_role=False: User can ONLY do operations listed in permissions
    3. Check Pool Permissions (if VM is in a pool)
    4. If user not in ACL: fall back to user's general role permissions
    
    LW: Changed inherit_role=True to mean "full VM access" instead of "use role perms"
    This is more intuitive - adding someone to a VM ACL should grant them access to that VM
    """
    if user.get('role') == ROLE_ADMIN:
        return True
    
    username = user.get('username', '')
    acls = get_vm_acls()
    
    # LW: Debug logging to help troubleshoot ACL issues
    logging.debug(f"[VM-ACL] Checking access for user={username}, cluster={cluster_id}, vmid={vmid}, perm={permission}")
    logging.debug(f"[VM-ACL] Available ACLs for cluster: {list(acls.get(cluster_id, {}).keys())}")
    
    # check VM-specific acl
    cluster_acls = acls.get(cluster_id, {})
    vm_acl = cluster_acls.get(str(vmid), {})
    
    if vm_acl:
        allowed_users = vm_acl.get('users', [])
        logging.debug(f"[VM-ACL] VM {vmid} ACL found, allowed users: {allowed_users}")
        
        # MK: If user is in the ACL whitelist, check their ACL permissions
        if username in allowed_users or '*' in allowed_users:
            if vm_acl.get('inherit_role', True):
                # inherit_role=True: FULL VM access (start, stop, console, etc.)
                # This means "this user has access to this VM"
                vm_permissions = ['vm.view', 'vm.start', 'vm.stop', 'vm.restart', 'vm.console', 
                                  'vm.snapshot', 'vm.migrate', 'vm.clone', 'vm.config', 'vm.backup']
                result = permission in vm_permissions
                logging.debug(f"[VM-ACL] User {username} in ACL with inherit_role=True, checking {permission}: {result}")
                return result
            else:
                # inherit_role=False: use ONLY the VM-specific permissions
                vm_perms = vm_acl.get('permissions', [])
                result = permission in vm_perms
                logging.debug(f"[VM-ACL] User {username} in ACL with custom perms {vm_perms}, checking {permission}: {result}")
                return result
        else:
            logging.debug(f"[VM-ACL] User {username} NOT in ACL whitelist {allowed_users}")
        
        # User not in ACL whitelist - fall through to check pool permissions
    else:
        logging.debug(f"[VM-ACL] No ACL found for VM {vmid} in cluster {cluster_id}")
    
    # MK: Check Pool Permissions - Jan 2026
    # If VM is in a pool, check if user has permission via pool
    # Uses cached pool membership data to avoid API calls on every permission check
    try:
        pool_id = get_vm_pool_cached(cluster_id, vmid, vm_type)
        
        if pool_id:
            logging.debug(f"[POOL-PERM] VM {vmid} is in pool '{pool_id}' (cached)")
            
            # Get user's groups
            user_groups = user.get('groups', [])
            
            # Get user's pool permissions from DB (not API)
            db = get_db()
            user_pool_perms = db.get_user_pool_permissions(cluster_id, username, user_groups)
            
            # Check if user has required permission for this pool
            pool_perms = user_pool_perms.get(pool_id, [])
            
            if pool_perms:
                # pool.admin grants all permissions
                if 'pool.admin' in pool_perms:
                    logging.debug(f"[POOL-PERM] User {username} has pool.admin for pool '{pool_id}'")
                    return True
                
                if permission in pool_perms:
                    logging.debug(f"[POOL-PERM] User {username} has {permission} for pool '{pool_id}'")
                    return True
                
                logging.debug(f"[POOL-PERM] User {username} has pool perms {pool_perms} but not {permission}")
            else:
                logging.debug(f"[POOL-PERM] User {username} has no permissions for pool '{pool_id}'")
    except Exception as e:
        logging.error(f"[POOL-PERM] Error checking pool permission: {e}")
    
    # no VM-specific ACL or pool permission - use general permissions
    result = has_permission(user, permission)
    logging.debug(f"[VM-ACL] Fallback to general permission check for {permission}: {result}")
    return result

def get_user_vms(user: dict, cluster_id: str) -> list:
    """Get list of VMIDs user can access in a cluster
    
    Returns None if user can access all VMs (admin or no restrictions)
    """
    if user.get('role') == ROLE_ADMIN:
        return None
    
    username = user.get('username', '')
    acls = get_vm_acls()
    cluster_acls = acls.get(cluster_id, {})
    
    # if no acls for this cluster, user can see all (based on general perms)
    if not cluster_acls:
        return None
    
    # collect VMs user has access to
    allowed_vms = []
    for vmid, acl in cluster_acls.items():
        users = acl.get('users', [])
        if username in users or '*' in users:
            allowed_vms.append(int(vmid))
    
    return allowed_vms if allowed_vms else None


# =============================================================================
# ROLE TEMPLATES - presets for common setups
# =============================================================================
# ROLE TEMPLATES - MK jan 2026
# MK: preset roles for common use cases
# LW: updated jan 2026 - tenant_admin was missing some perms
# =============================================================================

ROLE_TEMPLATES = {
    'tenant_admin': {
        'name': 'Tenant Administrator',
        'description': 'Full tenant access - everything except global settings',
        'permissions': [
            # VMs - full control
            'vm.view', 'vm.start', 'vm.stop', 'vm.restart', 'vm.console', 'vm.migrate',
            'vm.clone', 'vm.delete', 'vm.create', 'vm.config', 'vm.snapshot', 'vm.backup', 'vm.template',
            # cluster - no add/delete/join (thats global admin stuff)
            'cluster.view', 'cluster.config',
            # nodes - LW: added shell/reboot jan 2026
            'node.view', 'node.shell', 'node.maintenance', 'node.reboot', 'node.network', 'node.config',
            # storage
            'storage.view', 'storage.upload', 'storage.download', 'storage.delete', 'storage.config',
            # backup - MK: tenant admins need full backup control
            'backup.view', 'backup.create', 'backup.restore', 'backup.delete', 'backup.schedule', 'backup.config',
            # HA
            'ha.view', 'ha.config', 'ha.groups', 'ha.resources',
            # firewall
            'firewall.view', 'firewall.edit', 'firewall.aliases',
            # pools + replication
            'pool.view', 'pool.manage', 'pool.assign',
            'replication.view', 'replication.manage',
        ]
    },
    'tenant_operator': {
        'name': 'Tenant Operator',
        'description': 'Daily ops - VMs, backups, basic maintenance',
        'permissions': [
            # VMs - no delete/create/template
            'vm.view', 'vm.start', 'vm.stop', 'vm.restart', 'vm.console', 'vm.migrate',
            'vm.clone', 'vm.config', 'vm.snapshot', 'vm.backup',
            'cluster.view',
            'node.view', 'node.maintenance',
            'storage.view', 'storage.upload', 'storage.download',
            'backup.view', 'backup.create', 'backup.restore', 'backup.delete',  # LW: ops need to clean up old backups
            'ha.view',
            'firewall.view',
            'pool.view', 'pool.assign',
            'replication.view',
        ]
    },
    'tenant_user': {
        'name': 'Tenant User',
        'description': 'Basic VM stuff - start/stop/console',
        'permissions': [
            'vm.view', 'vm.start', 'vm.stop', 'vm.restart', 'vm.console', 'vm.snapshot',
            'cluster.view',
            'node.view',
            'storage.view',
            'backup.view', 'backup.create', 'backup.restore',  # LW: let users backup their own stuff
            'ha.view',
            'firewall.view',
            'pool.view',
        ]
    },
    'tenant_viewer': {
        'name': 'Tenant Viewer',
        'description': 'Read-only + console',
        'permissions': [
            'vm.view', 'vm.console',
            'cluster.view',
            'node.view',
            'storage.view',
            'backup.view',
            'ha.view',
            'firewall.view',
            'pool.view',
            'replication.view',
        ]
    },
    'vm_operator': {
        'name': 'VM Operator',
        'description': 'VMs only - no infra access',
        'permissions': [
            'vm.view', 'vm.start', 'vm.stop', 'vm.restart', 'vm.console',
            'vm.snapshot', 'vm.backup',
            'backup.view', 'backup.create', 'backup.restore',
            'storage.view',
        ]
    },
    'backup_operator': {
        'name': 'Backup Operator', 
        'description': 'Backups only - for backup admins',
        'permissions': [
            'vm.view',
            'storage.view', 'storage.upload',
            'backup.view', 'backup.create', 'backup.restore', 'backup.delete', 'backup.schedule', 'backup.config',
        ]
    },
    'storage_admin': {
        'name': 'Storage Administrator',
        'description': 'Storage + backup management',
        'permissions': [
            'vm.view',
            'cluster.view',
            'node.view',
            'storage.view', 'storage.upload', 'storage.delete', 'storage.config', 'storage.create', 'storage.download',
            'backup.view', 'backup.create', 'backup.restore', 'backup.delete', 'backup.config',
        ]
    },
    'network_admin': {
        'name': 'Network Administrator',
        'description': 'Network + firewall config',
        'permissions': [
            'vm.view', 'vm.config',  # need this for VM NICs
            'cluster.view',
            'node.view', 'node.network', 'node.config',
            'firewall.view', 'firewall.edit', 'firewall.aliases',
            'ha.view',
        ]
    },
    'monitoring': {
        'name': 'Monitoring',
        'description': 'Read-only for dashboards/alerting',
        'permissions': [
            'vm.view',
            'cluster.view',
            'node.view',
            'storage.view',
            'backup.view',
            'ha.view',
            'firewall.view',
            'pool.view',
            'replication.view',
            'admin.audit',  # MK: monitoring tools need audit logs
        ]
    },
    'group_manager': {
        'name': 'Group Manager',
        'description': 'Cluster groups + tenant management',
        'permissions': [
            'vm.view',
            'cluster.view',
            'node.view',
            'storage.view',
            'pool.view', 'pool.manage', 'pool.assign',
            'admin.groups', 'admin.tenants',
        ]
    },
    'helpdesk': {
        'name': 'Helpdesk',
        'description': 'Support staff - basic VM help',
        'permissions': [
            'vm.view', 'vm.start', 'vm.stop', 'vm.restart', 'vm.console', 'vm.snapshot',
            'cluster.view',
            'node.view',
            'storage.view',
            'backup.view', 'backup.restore',  # can restore for users
            'ha.view',
        ]
    },
    'developer': {
        'name': 'Developer',
        'description': 'Dev access - own VMs + snapshots',
        'permissions': [
            'vm.view', 'vm.start', 'vm.stop', 'vm.restart', 'vm.console',
            'vm.snapshot', 'vm.clone', 'vm.config',
            'cluster.view',
            'node.view',
            'storage.view', 'storage.upload',
            'backup.view', 'backup.create', 'backup.restore',
        ]
    },
    'auditor': {
        'name': 'Auditor',
        'description': 'Compliance - read-only + audit logs',
        'permissions': [
            'vm.view',
            'cluster.view',
            'node.view',
            'storage.view',
            'backup.view',
            'ha.view',
            'firewall.view',
            'pool.view',
            'replication.view',
            'admin.audit',
        ]
    },
}
class PegaProxConfig:
    """Configuration for a single Proxmox cluster
    
    NS: This started as just host/user/pass and grew into this monster
    MK: We should probably use dataclasses but I cant be bothered to refactor
    """
    
    def __init__(self, cluster_data):
        self.name = cluster_data['name']
        self.host = cluster_data['host']
        self.user = cluster_data['user']
        self.pass_ = cluster_data['pass']  # underscore because 'pass' is reserved
        self.ssl_verification = cluster_data.get('ssl_verification', False)
        self.migration_threshold = cluster_data.get('migration_threshold', 20)
        self.check_interval = cluster_data.get('check_interval', 300)
        self.auto_migrate = cluster_data.get('auto_migrate', True)
        self.balance_containers = cluster_data.get('balance_containers', False)  # Containers require restart!
        self.balance_local_disks = cluster_data.get('balance_local_disks', False)  # VMs with local disks (requires storage migration)
        self.dry_run = cluster_data.get('dry_run', False)
        self.enabled = cluster_data.get('enabled', True)
        self.ha_enabled = cluster_data.get('ha_enabled', False)
        # Fallback hosts for HA - if primary host fails, try these
        self.fallback_hosts = cluster_data.get('fallback_hosts', [])
        # SSH settings for node management
        self.ssh_user = cluster_data.get('ssh_user', '')  # Optional separate SSH user
        self.ssh_key = cluster_data.get('ssh_key', '')  # Private key content
        self.ssh_port = cluster_data.get('ssh_port', 22)
        # HA Settings (Split-Brain Prevention, 2-Node Mode, etc.) - NS Jan 2026
        self.ha_settings = cluster_data.get('ha_settings', {})
        # LW: Excluded nodes - these nodes will never be targets for balancing
        # Similar to ProxLB's exclude hosts feature
        self.excluded_nodes = cluster_data.get('excluded_nodes', [])

class MaintenanceTask:
    """Tracks a node evacuation/maintenance task
    
    LW: This is used when putting a node into maintenance mode
    All VMs get migrated off before the node can be updated/rebooted
    """
    
    def __init__(self, node: str):
        self.node = node
        self.started_at = datetime.now()
        self.total_vms = 0
        self.migrated_vms = 0
        self.failed_vms = []
        self.pending_vms = []
        self.status = 'starting'  # starting, evacuating, completed, completed_with_errors, failed
        self.current_vm = None
        self.error = None
        self.acknowledged = False  # User acknowledged warning about failed migrations

    def to_dict(self):
        # MK: convert to dict for JSON serialization
        return {
            'node': self.node,
            'started_at': self.started_at.isoformat(),
            'total_vms': self.total_vms,
            'migrated_vms': self.migrated_vms,
            'failed_vms': self.failed_vms,
            'pending_vms': [{'vmid': vm.get('vmid'), 'name': vm.get('name', 'unnamed')} for vm in self.pending_vms],
            'status': self.status,
            'current_vm': self.current_vm,
            'progress_percent': round((self.migrated_vms / self.total_vms * 100) if self.total_vms > 0 else 0, 1),
            'error': self.error,
            'acknowledged': self.acknowledged
        }


class UpdateTask:
    
    def __init__(self, node: str, reboot: bool = True):
        self.node = node
        self.reboot = reboot
        self.started_at = datetime.now()
        self.status = 'starting'  # starting, updating, rebooting, waiting_online, completed, failed
        self.phase = 'init'  # init, apt_update, apt_upgrade, reboot, wait_online
        self.output_lines = []
        self.error = None
        self.packages_upgraded = 0
        self.completed_at = None

    def add_output(self, line: str):
        self.output_lines.append({
            'timestamp': datetime.now().isoformat(),
            'text': line
        })
        # Keep only last 100 lines
        if len(self.output_lines) > 100:
            self.output_lines = self.output_lines[-100:]

    def to_dict(self):
        return {
            'node': self.node,
            'reboot': self.reboot,
            'started_at': self.started_at.isoformat(),
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'status': self.status,
            'phase': self.phase,
            'output_lines': self.output_lines[-20:],  # Last 20 lines for UI
            'error': self.error,
            'packages_upgraded': self.packages_upgraded,
            'duration_seconds': (datetime.now() - self.started_at).total_seconds()
        }

class PegaProxManager:
    """
    main cluster manager - NS
    
    handles all the proxmox api stuff, loadbalancing, HA etc
    each cluster runs in its own thread
    
    this class is kinda big, should probably split it up someday - MK
    """
    
    def __init__(self, cluster_id: str, config: PegaProxConfig):
        self.id = cluster_id
        self.config = config
        self.running = False
        self.thread = None
        self.stop_event = threading.Event()
        self.last_run = None
        self.last_migration_log = []
        
        # maintenance mode
        self.nodes_in_maintenance = {}
        self.maintenance_lock = threading.Lock()
        
        # update tracking
        self.nodes_updating = {}
        self.update_lock = threading.Lock()
        
        # HA stuff - MK added this
        self.ha_enabled = getattr(config, 'ha_enabled', False)
        self.ha_check_interval = 10
        self.ha_thread = None
        self.ha_node_status = {}  # node -> status dict
        self.ha_lock = threading.Lock()
        self.ha_recovery_in_progress = {}
        
        # load saved HA settings
        saved_ha = getattr(config, 'ha_settings', {}) or {}
        
        self.ha_failure_threshold = saved_ha.get('failure_threshold', 3)
        
        # split-brain stuff (complicated, dont touch) - NS
        self.ha_config = {
            'quorum_enabled': saved_ha.get('quorum_enabled', True),
            'quorum_hosts': saved_ha.get('quorum_hosts', []),
            'quorum_gateway': saved_ha.get('quorum_gateway', ''),
            'quorum_required_votes': saved_ha.get('quorum_required_votes', 2),
            
            # self-fencing
            'self_fence_enabled': saved_ha.get('self_fence_enabled', True),
            'watchdog_enabled': saved_ha.get('watchdog_enabled', False),
            
            # network checks
            'verify_network_before_recovery': saved_ha.get('verify_network', True),
            'network_check_hosts': saved_ha.get('network_check_hosts', []),
            'network_check_required': saved_ha.get('network_check_required', 1),
            
            # storage fencing
            'storage_fence_enabled': saved_ha.get('storage_fence_enabled', False),
            
            # storage heartbeat - safest for 2-node clusters
            # NS: spent forever getting this to work right
            'storage_heartbeat_enabled': saved_ha.get('storage_heartbeat_enabled', False),
            'storage_heartbeat_path': saved_ha.get('storage_heartbeat_path', ''),
            'storage_heartbeat_interval': saved_ha.get('storage_heartbeat_interval', 5),
            'storage_heartbeat_timeout': saved_ha.get('storage_heartbeat_timeout', 30),
            'poison_pill_enabled': saved_ha.get('poison_pill_enabled', True),
            
            # ═══════════════════════════════════════════════════════════════
            # DUAL-NETWORK PROTECTION - NS Jan 2026
            # For setups with separate Server and Storage networks!
            # Auto-installs a small agent on each node that communicates
            # via the storage network (survives server network failures)
            # ═══════════════════════════════════════════════════════════════
            'dual_network_mode': saved_ha.get('dual_network_mode', False),
            'node_agent_installed': saved_ha.get('node_agent_installed', {}),  # node -> True/False
            'self_fence_installed': saved_ha.get('self_fence_installed', False),  # MK: was missing, status got lost on restart
            'self_fence_nodes': saved_ha.get('self_fence_nodes', []),  # NS: list of nodes with agent installed
            
            # Timing
            'recovery_delay': saved_ha.get('recovery_delay', 30),  # Wait before recovery
            'node_timeout': saved_ha.get('node_timeout', 60),  # How long node must be dead before recovery
            
            # ═══════════════════════════════════════════════════════════════
            # 2-NODE CLUSTER MODE - Automatic quorum handling
            # Uses cluster credentials (same as Proxmox API login) for SSH
            # ═══════════════════════════════════════════════════════════════
            'two_node_mode': saved_ha.get('two_node_mode', False),
            'force_quorum_on_failure': saved_ha.get('force_quorum_on_failure', False),
            
            # ═══════════════════════════════════════════════════════════════
            # STRICT MODE - Maximum safety, may cause false positives
            # ═══════════════════════════════════════════════════════════════
            'strict_fencing': saved_ha.get('strict_fencing', False),  # Require successful fencing before recovery
            'require_storage_heartbeat_confirm': saved_ha.get('require_storage_heartbeat_confirm', False),  # Must confirm via storage
            
            # Node IPs (auto-discovered but can be overridden)
            'node_ips': saved_ha.get('node_ips', {}),  # node_name -> ip
        }
        
        # Storage heartbeat tracking
        self.ha_heartbeat_thread = None
        self.ha_heartbeat_stop = threading.Event()
        self.ha_last_heartbeat_write = None
        
        # Quorum tracking - NS Jan 2026
        self.ha_have_quorum = True  # Assume quorum until proven otherwise
        self.ha_last_quorum_check = None
        
        # Setup logging
        self.logger = logging.getLogger(f"PegaProx_{config.name}")
        self.logger.setLevel(logging.DEBUG)  # File gets everything
        self.logger.propagate = False  # MK: Don't propagate to root logger (prevents DEBUG spam)
        
        # Clear existing handlers to prevent duplicates - NS Jan 2026
        if self.logger.handlers:
            self.logger.handlers.clear()
        
        # File handler - DEBUG level (for troubleshooting)
        fh = logging.FileHandler(f"{LOG_DIR}/{cluster_id}.log")
        fh.setLevel(logging.DEBUG)
        
        # Console handler - INFO level (no DEBUG spam)
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter('[%(asctime)s] [%(name)s] %(levelname)s: %(message)s')
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)
        
        self.logger.addHandler(fh)
        self.logger.addHandler(ch)
        
        # Proxmox API credentials (stored, not session)
        self._ticket = None
        self._csrf_token = None
        self._api_token = None  # NS: for API token auth (user@realm!tokenid=secret)
        self._using_api_token = False
        self.current_host = None  # Track which host we're connected to
        self._ssl_verify = False
        
        # Connection state tracking
        self.is_connected = False
        self.last_successful_request = None
        self.connection_error = None
        self._consecutive_failures = 0  # NS: track failed requests for smarter disconnect detection
        self._disabled_check_counter = 0  # LW: for checking connection even when disabled
        
        # Default timeout for API requests
        self.api_timeout = 10
        
        # Lock for connection operations
        self._connect_lock = threading.Lock()
    
    def _create_session(self):
        """
        Create a new requests session with auth - thread safe
        
        MK: Each request gets a fresh session to avoid threading issues
        Tried session pooling but gevent + requests was causing deadlocks
        
        NS: Added API token support Jan 2026 (GitHub Issue #5)
        Token auth uses Authorization header, password auth uses cookies
        """
        session = requests.Session()
        session.verify = self._ssl_verify
        
        if getattr(self, '_api_token', None):
            # API Token auth - use Authorization header
            session.headers.update({'Authorization': f'PVEAPIToken={self._api_token}'})
        else:
            # Password auth - use ticket cookie
            if self._ticket:
                session.cookies.set('PVEAuthCookie', self._ticket)
            if self._csrf_token:
                session.headers.update({'CSRFPreventionToken': self._csrf_token})
        return session
    
    @property
    def host(self) -> str:
        return self.current_host or self.config.host
    
    @property
    def ticket(self) -> str:
        return self._ticket
    
    # LW: All API methods go through these wrappers for consistent error handling
    # MK: Jan 2026 - Fixed timeout handling, was marking cluster offline too eagerly
    def _api_get(self, url, **kwargs):
        kwargs.setdefault('timeout', self.api_timeout)
        try:
            session = self._create_session()
            response = session.get(url, **kwargs)
            # Track connection state for UI
            self.is_connected = True
            self.last_successful_request = datetime.now()
            self.connection_error = None
            self._consecutive_failures = 0  # reset failure counter on success
            return response
        except requests.exceptions.Timeout as e:
            # MK: Timeout != offline. Proxmox might just be slow (happens a lot with ZFS)
            self.connection_error = f"Request timed out: {e}"
            raise
        except requests.exceptions.ConnectionError as e:
            # LW: Only mark disconnected after 3 consecutive failures to avoid flapping
            self._consecutive_failures += 1
            if self._consecutive_failures >= 3:
                self.is_connected = False
                self.connection_error = str(e)
            raise
    
    def _api_post(self, url, **kwargs):
        kwargs.setdefault('timeout', self.api_timeout)
        try:
            sess = self._create_session()
            resp = sess.post(url, **kwargs)
            self.is_connected = True
            self.last_successful_request = datetime.now()
            self.connection_error = None
            self._consecutive_failures = 0
            return resp
        except requests.exceptions.Timeout as e:
            # MK: Timeout does NOT mean cluster is offline
            self.connection_error = f"Request timed out: {e}"
            raise
        except requests.exceptions.ConnectionError as e:
            self._consecutive_failures += 1
            if self._consecutive_failures >= 3:
                self.is_connected = False
                self.connection_error = str(e)
            raise
    
    def _api_put(self, url, **kwargs):
        kwargs.setdefault('timeout', self.api_timeout)
        try:
            session = self._create_session()
            response = session.put(url, **kwargs)
            self.is_connected = True
            self.last_successful_request = datetime.now()
            self.connection_error = None
            self._consecutive_failures = 0
            return response
        except requests.exceptions.Timeout as e:
            # MK: Timeout does NOT mean cluster is offline - operation might have succeeded
            self.connection_error = f"Request timed out: {e}"
            self.logger.warning(f"[WARN] API PUT timeout (not marking offline): {e}")
            raise
        except requests.exceptions.ConnectionError as e:
            # Real connection error - only mark offline after multiple failures
            self._consecutive_failures += 1
            if self._consecutive_failures >= 3:
                self.is_connected = False
                self.connection_error = str(e)
            raise
    
    def _api_delete(self, url, **kwargs):
        # same as put but delete
        kwargs.setdefault('timeout', self.api_timeout)
        try:
            session = self._create_session()
            r = session.delete(url, **kwargs)
            self.is_connected = True
            self.last_successful_request = datetime.now()
            self.connection_error = None
            self._consecutive_failures = 0
            return r
        except requests.exceptions.Timeout as e:
            # MK: Timeout does NOT mean cluster is offline
            self.connection_error = f"Request timed out: {e}"
            self.logger.warning(f"[WARN] API DELETE timeout (not marking offline): {e}")
            raise
        except requests.exceptions.ConnectionError as e:
            # Real connection error - only mark offline after multiple failures
            self._consecutive_failures += 1
            if self._consecutive_failures >= 3:
                self.is_connected = False
                self.connection_error = str(e)
            raise
        
    def connect_to_proxmox(self) -> bool:
        # connect with fallback
        with self._connect_lock:
            # Build list of hosts to try: primary first, then fallbacks
            hosts_to_try = [self.config.host] + (self.config.fallback_hosts or [])
            
            self._ssl_verify = self.config.ssl_verification
            # self._ssl_verify = False  # tmp disable for debugging cert issues
            
            # NS: Check if using API Token (format: user@realm!tokenid)
            # API tokens have ! in the username, passwords don't
            self._using_api_token = '!' in self.config.user
            
            for host in hosts_to_try:
                try:
                    # Create a temporary session just for login
                    session = requests.Session()
                    session.verify = self._ssl_verify
                    
                    if self._using_api_token:
                        # API Token auth - no ticket needed!
                        # Token goes in Authorization header: PVEAPIToken=user@realm!tokenid=secret
                        self._api_token = f"{self.config.user}={self.config.pass_}"
                        self._ticket = None
                        self._csrf_token = None
                        
                        # Test the token by making a simple API call
                        test_url = f"https://{host}:8006/api2/json/version"
                        headers = {'Authorization': f'PVEAPIToken={self._api_token}'}
                        resp = session.get(test_url, headers=headers, timeout=10)
                        
                        if resp.status_code == 200:
                            self.current_host = host
                            self.is_connected = True
                            self.last_successful_request = datetime.now()
                            self.connection_error = None
                            self.session = True
                            
                            self.logger.info(f"Connected to Proxmox at {host} using API Token")
                            
                            if not self.config.fallback_hosts:
                                self._auto_discover_fallback_hosts()
                            
                            return True
                        else:
                            self.logger.warning(f"API Token auth failed at {host}: {resp.status_code}")
                    else:
                        # Password auth - get ticket from /access/ticket
                        login_data = {
                            'username': self.config.user,
                            'password': self.config.pass_
                        }
                        # print(f"DEBUG: trying {host}")  # dont commit this
                        
                        login_url = f"https://{host}:8006/api2/json/access/ticket"
                        resp = session.post(login_url, data=login_data, timeout=10)
                        
                        if resp.status_code == 200:
                            data = resp.json()['data']
                            self._ticket = data['ticket']
                            self._csrf_token = data['CSRFPreventionToken']
                            self._api_token = None
                            
                            self.current_host = host
                            self.is_connected = True
                            self.last_successful_request = datetime.now()
                            self.connection_error = None
                            
                            # For backward compatibility - some code still checks self.session
                            # NS: this is ugly but works, passt eh
                            self.session = True  # Just a truthy value
                            
                            if host != self.config.host:
                                self.logger.warning(f"Connected to FALLBACK host {host} (primary {self.config.host} unavailable)")
                            else:
                                self.logger.info(f"Successfully connected to Proxmox at {host}")
                            
                            # Auto-discover fallback hosts if not already set
                            if not self.config.fallback_hosts:
                                self._auto_discover_fallback_hosts()
                            
                            return True
                        else:
                            self.logger.warning(f"Failed to login to Proxmox at {host}: {resp.status_code}")
                            # self.logger.debug(f"Response body: {resp.text}")  # too verbose
                        
                except requests.exceptions.Timeout:
                    self.logger.warning(f"Connection timeout to {host}")
                    self.is_connected = False
                    self.connection_error = f"Timeout connecting to {host}"
                except requests.exceptions.ConnectionError:
                    self.logger.warning(f"Cannot connect to {host}")
                    self.is_connected = False
                    self.connection_error = f"Cannot connect to {host}"
                except Exception as e:
                    self.logger.warning(f"Error connecting to {host}: {e}")
                    self.is_connected = False
                    self.connection_error = str(e)
            
            self.logger.error(f"Failed to connect to any Proxmox host (tried {len(hosts_to_try)} hosts)")
            self.is_connected = False
            return False
    
    def _auto_discover_fallback_hosts(self):
        # find other nodes in cluster
        try:
            h = self.current_host or self.config.host
            url = f"https://{h}:8006/api2/json/nodes"
            r = self._create_session().get(url, timeout=10)
            
            if r.status_code != 200:
                return
            
            nodes = r.json().get('data', [])
            discovered = []
            
            for node in nodes:
                node_name = node.get('node')
                if node.get('status') == 'online':
                    node_ip = self._get_node_ip(node_name)
                    if node_ip and node_ip != self.config.host:
                        discovered.append(node_ip)
            
            if discovered:
                self.config.fallback_hosts = discovered
                self.logger.info(f"Auto-discovered {len(discovered)} fallback hosts: {discovered}")
                save_config()
                
        except Exception as e:
            self.logger.debug(f"Could not auto-discover fallback hosts: {e}")
    
    def _get_api_url(self, path: str) -> str:
        host = self.current_host or self.config.host
        return f"https://{host}:8006/api2/json{path}"
    
    def get_node_status(self) -> Dict[str, Any]:
        # gets node status, calculates load score
        # MK: made this parallel, was super slow before
        if not self.is_connected or not self.session:
            # return cached ha status if disconnected
            if self.ha_node_status:
                return {
                    name: {
                        'status': data.get('status', 'offline'),
                        'cpu_percent': 0,
                        'mem_percent': 0,
                        'mem_used': 0,
                        'mem_total': 0,
                        'disk_percent': 0,
                        'disk_used': 0,
                        'disk_total': 0,
                        'netin': 0,
                        'netout': 0,
                        'score': 0,
                        'uptime': 0,
                        'offline': data.get('status') == 'offline',
                        'last_seen': data.get('last_seen').isoformat() if data.get('last_seen') else None
                    }
                    for name, data in self.ha_node_status.items()
                }
            return {}
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes"
            response = self._api_get(url)
            
            if response.status_code == 200:
                nodes = response.json()['data']
                node_status = {}
                
                api_nodes = set()
                
                # fetch node details (parallel if gevent available)
                def fetch_node_details(node):
                    node_name = node['node']
                    try:
                        status_url = f"https://{host}:8006/api2/json/nodes/{node_name}/status"
                        status_response = self._create_session().get(status_url, timeout=10)
                        if status_response.status_code == 200:
                            return (node_name, node, status_response.json()['data'])
                        else:
                            return (node_name, node, None)
                    except Exception as e:
                        # self.logger.debug(f"node {node_name} error: {e}")  # too noisy
                        return (node_name, node, None)
                
                # parallel if available
                if GEVENT_AVAILABLE and GEVENT_POOL:
                    tasks = [lambda n=node: fetch_node_details(n) for node in nodes]
                    results = run_concurrent(tasks, timeout=15.0)
                else:
                    results = [fetch_node_details(node) for node in nodes]
                
                # Process results
                for result in results:
                    if result is None:
                        continue
                    
                    node_name, node, status_data = result
                    api_nodes.add(node_name)
                    
                    if status_data:
                        self.logger.debug(f"Raw status data for {node_name}: {status_data}")
                        
                        # Calculate percentages
                        cpu_percent = status_data.get('cpu', 0) * 100
                        mem_used = status_data.get('memory', {}).get('used', 0)
                        mem_total = status_data.get('memory', {}).get('total', 1)
                        mem_percent = (mem_used / mem_total) * 100 if mem_total > 0 else 0
                        
                        # Disk stats from rootfs
                        rootfs = status_data.get('rootfs', {})
                        disk_used = rootfs.get('used', 0)
                        disk_total = rootfs.get('total', 1)
                        disk_percent = (disk_used / disk_total) * 100 if disk_total > 0 else 0
                        
                        # Network stats (cumulative bytes)
                        netin = status_data.get('netin', 0)
                        netout = status_data.get('netout', 0)
                        
                        # Calculate simple score (lower is better)
                        score = cpu_percent + mem_percent
                        
                        # Check maintenance status
                        in_maintenance = node_name in self.nodes_in_maintenance
                        maintenance_task = None
                        maintenance_acknowledged = False
                        if in_maintenance:
                            task_obj = self.nodes_in_maintenance[node_name]
                            maintenance_task = task_obj.to_dict()
                            maintenance_acknowledged = task_obj.acknowledged
                        
                        # Check update status
                        is_updating = node_name in self.nodes_updating
                        update_task = None
                        if is_updating:
                            update_task = self.nodes_updating[node_name].to_dict()
                        
                        node_status[node_name] = {
                            'status': node.get('status', 'unknown'),
                            'cpu_percent': round(cpu_percent, 2),
                            'mem_percent': round(mem_percent, 2),
                            'mem_used': mem_used,
                            'mem_total': mem_total,
                            'disk_percent': round(disk_percent, 2),
                            'disk_used': disk_used,
                            'disk_total': disk_total,
                            'netin': netin,
                            'netout': netout,
                            'score': round(score, 2),
                            'uptime': status_data.get('uptime', 0),
                            'maintenance_mode': in_maintenance,
                            'maintenance_task': maintenance_task,
                            'maintenance_acknowledged': maintenance_acknowledged,
                            'is_updating': is_updating,
                            'update_task': update_task,
                            'offline': False
                        }
                        
                        maintenance_str = " [MAINTENANCE]" if in_maintenance else ""
                        update_str = " [UPDATING]" if is_updating else ""
                        self.logger.info(f"Node {node_name}: CPU {cpu_percent:.2f}%, RAM {mem_percent:.2f}% ({self._format_bytes(mem_used)}/{self._format_bytes(mem_total)}), Score {score:.2f}, Status: {node['status']}{maintenance_str}{update_str}")
                    else:
                        # Node exists but we couldn't get status - might be offline
                        node_status[node_name] = {
                            'status': node.get('status', 'unknown'),
                            'cpu_percent': 0,
                            'mem_percent': 0,
                            'mem_used': 0,
                            'mem_total': 0,
                            'disk_percent': 0,
                            'disk_used': 0,
                            'disk_total': 0,
                            'netin': 0,
                            'netout': 0,
                            'score': 0,
                            'uptime': 0,
                            'offline': node.get('status') != 'online'
                        }
                
                # Add offline nodes from HA tracking that weren't in API response
                for ha_node, ha_data in self.ha_node_status.items():
                    if ha_node not in api_nodes and ha_data.get('status') == 'offline':
                        node_status[ha_node] = {
                            'status': 'offline',
                            'cpu_percent': 0,
                            'mem_percent': 0,
                            'mem_used': 0,
                            'mem_total': 0,
                            'disk_percent': 0,
                            'disk_used': 0,
                            'disk_total': 0,
                            'netin': 0,
                            'netout': 0,
                            'score': 0,
                            'uptime': 0,
                            'offline': True,
                            'last_seen': ha_data.get('last_seen').isoformat() if ha_data.get('last_seen') else None
                        }
                        self.logger.info(f"Node {ha_node}: OFFLINE (from HA tracking)")
                
                return node_status
            else:
                # Session might have expired - don't immediately mark as disconnected
                # LW: the _api_get already handles connection state
                self.logger.warning(f"Failed to get nodes (status {response.status_code})")
                return {}
        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError):
            # Connection error - _api_get handles the failure counter
            return {}
        except Exception as e:
            self.logger.error(f"Error getting node status: {e}")
            return {}
    
    def get_vm_resources(self) -> list:
        # NS: fetches VMs + CTs, adds computed mem/cpu percentages
        if not self.is_connected or not self.session: return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/cluster/resources"
            resp = self._create_session().get(url, params={'type': 'vm'}, timeout=10)
            
            if resp.status_code != 200: return []
            
            # NS: success - reset failure counter
            self._consecutive_failures = 0
            
            resources = resp.json()['data']
            resources.sort(key=lambda x: x.get('vmid', 0))  # consistent order
            
            # add percentage values for UI
            for r in resources:
                maxmem = r.get('maxmem', 0)
                r['mem_percent'] = round((r.get('mem', 0) / maxmem) * 100, 1) if maxmem > 0 else 0
                r['cpu_percent'] = round(r.get('cpu', 0) * 100, 1)
            
            return resources
        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError):
            # LW: don't immediately mark disconnected, use failure counter
            return []
        except:
            return []
    
    # MK: old version, keeping around just in case
    def get_vm_resources_v1(self) -> list:
        if not self.is_connected: return []
        try:
            url = f"https://{self.host}:8006/api2/json/cluster/resources"
            r = self._create_session().get(url, params={'type': 'vm'}, timeout=10)
            return r.json().get('data', []) if r.status_code == 200 else []
        except:
            return []
    
    def _format_bytes(self, bytes_value: int) -> str:
        # NS: quick helper, nothing fancy
        gb = bytes_value / (1024 ** 3)
        if gb >= 1: return f"{gb:.2f} GB"
        mb = bytes_value / (1024 ** 2)
        return f"{mb:.2f} MB"
    
    # same thing but different format, used somewhere?
    def _fmt_bytes(self, b):
        if b >= 1024**3: return f"{b/1024**3:.1f}G"
        if b >= 1024**2: return f"{b/1024**2:.1f}M"
        return f"{b/1024:.1f}K"
    
    def check_balance_needed(self, node_status: Dict[str, Any]) -> tuple:
        """
        Check if cluster needs rebalancing based on node scores.
        
        NS: The scoring algorithm here is inspired by ProxLB by gyptazy
        (https://github.com/gyptazy/ProxLB) - great project, thanks for 
        open-sourcing it! We adapted it for our multi-cluster setup.
        
        LW: Now also excludes nodes configured in excluded_nodes setting
        """
        if not node_status:
            return False, None, None
        
        # LW: Get excluded nodes from config
        config_excluded = getattr(self.config, 'excluded_nodes', []) or []
        
        # Exclude nodes in maintenance AND configured excluded nodes from balancing decisions
        # MK: This was a bug before - we'd try to migrate TO maintenance nodes
        active_nodes = {
            node: data for node, data in node_status.items() 
            if data['status'] == 'online' 
            and not data.get('maintenance_mode', False)
            and node not in config_excluded
        }
        
        if config_excluded:
            self.logger.debug(f"Excluding configured nodes from balance check: {config_excluded}")
        
        scores = [(node, data['score']) for node, data in active_nodes.items()]
        if len(scores) < 2:
            self.logger.info("Not enough online nodes for balancing (excluding maintenance and excluded nodes)")
            return False, None, None
        
        scores.sort(key=lambda x: x[1])
        min_node, min_score = scores[0]
        max_node, max_score = scores[-1]
        
        score_diff = max_score - min_score
        threshold_value = self.config.migration_threshold
        
        # self.logger.debug(f"scores: {scores}")  # very spammy
        self.logger.info(f"Score difference: {score_diff:.2f} (Min: {min_node}={min_score:.2f}, Max: {max_node}={max_score:.2f})")
        self.logger.info(f"Migration threshold: {threshold_value}%")
        
        if score_diff > threshold_value:
            self.logger.info(f"[WARN] Balance needed! Score difference {score_diff:.2f} > threshold {threshold_value}")
            return True, max_node, min_node
        else:
            self.logger.info(f"[OK] Cluster is balanced. Score difference {score_diff:.2f} <= threshold {threshold_value}")
            return False, None, None
    
    def find_migration_candidate(self, source_node: str, target_node: str) -> Optional[Dict]:
        """
        Find the best VM to migrate from source to target node.
        
        NS: This logic is based on ProxLB's approach but we've added:
        - Affinity rule checking
        - HA status awareness  
        - Multi-cluster considerations
        
        Priority order:
        1. VMs on shared storage (easiest to migrate)
        2. VMs on local storage (only if balance_local_disks enabled)
        3. Smaller VMs first (less impact during migration)
        4. Prefer QEMU VMs over containers (containers need restart)
        
        MK: Container migrations are tricky - they ALWAYS restart.
        We learned this the hard way in production...
        """
        vms = self.get_vm_resources()
        if not vms:
            return None
        
        # MK: Get excluded VMs for this cluster
        excluded_vmids = self.get_balancing_excluded_vms()
        if excluded_vmids:
            self.logger.info(f"VMs excluded from balancing: {excluded_vmids}")
        
        # Filter VMs on source node that are running
        candidates = [
            vm for vm in vms 
            if vm.get('node') == source_node and 
            vm.get('status') == 'running' and
            vm.get('type') in ['qemu', 'lxc'] and
            vm.get('vmid') not in excluded_vmids  # MK: Skip excluded VMs
        ]
        
        # Log if any VMs were excluded
        excluded_on_node = [vm for vm in vms if vm.get('node') == source_node and vm.get('vmid') in excluded_vmids]
        if excluded_on_node:
            self.logger.info(f"Skipping {len(excluded_on_node)} excluded VM(s) on {source_node}: {[vm.get('vmid') for vm in excluded_on_node]}")
        
        # Filter out containers if balance_containers is disabled
        if not getattr(self.config, 'balance_containers', False):
            original_count = len(candidates)
            candidates = [vm for vm in candidates if vm.get('type') != 'lxc']
            skipped = original_count - len(candidates)
            if skipped > 0:
                self.logger.info(f"Skipping {skipped} container(s) - container balancing is disabled")
        
        if not candidates:
            self.logger.info(f"No running VMs found on {source_node} for migration")
            return None
        
        # Check setting for local disk migration
        balance_local_disks = getattr(self.config, 'balance_local_disks', False)
        
        # Check each candidate for local disks and filter accordingly
        migratable_candidates = []
        local_disk_candidates = []  # VMs with local disks (need special handling)
        
        for vm in candidates:
            vmid = vm.get('vmid')
            vm_type = vm.get('type')
            storage_type = self.check_vm_storage_type(source_node, vmid, vm_type)
            
            if storage_type == 'local':
                if balance_local_disks:
                    # Mark as local disk VM for migration with --with-local-disks
                    vm['_has_local_disks'] = True
                    local_disk_candidates.append(vm)
                    self.logger.info(f"Found {vm.get('name', 'unnamed')} (VMID {vmid}) with local storage - eligible for migration (balance_local_disks enabled)")
                else:
                    self.logger.info(f"Skipping {vm.get('name', 'unnamed')} (VMID {vmid}) - uses local storage (enable 'Balance Local Disks' to include)")
                continue
            elif storage_type == 'unknown':
                self.logger.warning(f"Could not determine storage type for {vm.get('name', 'unnamed')} (VMID {vmid}) - skipping to be safe")
                continue
            
            vm['_has_local_disks'] = False
            migratable_candidates.append(vm)
        
        # Prefer VMs on shared storage, but include local disk VMs if enabled and no shared ones available
        all_candidates = migratable_candidates + local_disk_candidates
        
        if not all_candidates:
            self.logger.info(f"No migratable VMs found on {source_node}")
            return None
        
        # Sort by: shared storage first, then VMs over containers, then by memory (smallest first)
        all_candidates.sort(key=lambda x: (x.get('_has_local_disks', False), x.get('type') == 'lxc', x.get('mem', 0)))
        
        for vm in all_candidates:
            vm_type = 'CT' if vm.get('type') == 'lxc' else 'VM'
            local_warning = ' [LOCAL DISKS]' if vm.get('_has_local_disks') else ''
            restart_warning = ' (will restart!)' if vm.get('type') == 'lxc' else ''
            self.logger.info(f"Potential migration candidate: {vm.get('name', 'unnamed')} ({vm_type} {vm.get('vmid')}, RAM: {self._format_bytes(vm.get('mem', 0))}){local_warning}{restart_warning}")
        
        # Return the best candidate
        selected = all_candidates[0]
        vm_type = 'CT' if selected.get('type') == 'lxc' else 'VM'
        if selected.get('_has_local_disks'):
            self.logger.warning(f"Selected for migration: {selected.get('name', 'unnamed')} ({vm_type} {selected.get('vmid')}) - HAS LOCAL DISKS (will use --with-local-disks)")
        elif selected.get('type') == 'lxc':
            self.logger.warning(f"Selected container for migration: {selected.get('name', 'unnamed')} ({vm_type} {selected.get('vmid')}) - WILL CAUSE DOWNTIME!")
        else:
            self.logger.info(f"Selected for migration: {selected.get('name', 'unnamed')} ({vm_type} {selected.get('vmid')})")
        return selected
    
    def get_best_target_node(self, exclude_nodes: List[str] = None) -> Optional[str]:
        """Find the best target node for migration
        
        LW: Now also excludes nodes configured in excluded_nodes (like ProxLB)
        """
        if exclude_nodes is None:
            exclude_nodes = []
        
        # LW: Also exclude nodes configured in cluster settings
        config_excluded = getattr(self.config, 'excluded_nodes', []) or []
        all_excluded = list(set(exclude_nodes + config_excluded))
        
        if config_excluded:
            self.logger.debug(f"Excluding configured nodes from balancing: {config_excluded}")
        
        node_status = self.get_node_status()
        
        # Filter available nodes
        available_nodes = [
            (node, data) for node, data in node_status.items()
            if data['status'] == 'online' 
            and not data.get('maintenance_mode', False)
            and node not in all_excluded
        ]
        
        if not available_nodes:
            return None
        
        # Sort by score (lowest first)
        available_nodes.sort(key=lambda x: x[1]['score'])
        
        return available_nodes[0][0]
    
    def migrate_vm(self, vm: Dict, target_node: str, dry_run: bool = None) -> bool:
        """migrate vm to another node"""
        # NS: this handles the proxmox api call
        # MK: had to add iso unmount, was breaking migrations silently for weeks
        if dry_run is None:
            dry_run = self.config.dry_run
        
        # dry run = just log, dont actually do it
        if dry_run:
            self.logger.info(f"[DRY RUN] Would migrate {vm.get('name', 'unnamed')} ({vm.get('vmid')}) to {target_node}")
            self.last_migration_log.append({
                'timestamp': datetime.now().isoformat(),
                'vm': vm.get('name', 'unnamed'),
                'vmid': vm.get('vmid'),
                'from_node': vm.get('node'),
                'to_node': target_node,
                'dry_run': True,
                'success': True
            })
            return True
        
        try:
            vmid = vm.get('vmid')
            source_node = vm.get('node')
            vm_type = vm.get('type')
            # target_storage = vm.get('_target_storage')  # old code, not needed
            
            # unmount iso first or migration fails (found this out the hard way)
            if vm_type == 'qemu':
                config_url = f"https://{self.host}:8006/api2/json/nodes/{source_node}/qemu/{vmid}/config"
                config_response = self._create_session().get(config_url, timeout=15)
                if config_response.status_code == 200:
                    config = config_response.json().get('data', {})
                    for key in ['ide2', 'cdrom']:
                        if key in config and 'iso' in str(config.get(key, '')).lower():
                            iso_value = config[key]
                            self.logger.info(f"unmounting iso from {key}")
                            unmount_response = self._create_session().put(config_url, data={key: 'none,media=cdrom'})
                            if unmount_response.status_code != 200:
                                self.logger.warning(f"couldnt unmount iso: {unmount_response.text}")
            
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{source_node}/qemu/{vmid}/migrate"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{source_node}/lxc/{vmid}/migrate"
            
            data = {
                'target': target_node,
                'online': 1
            }
            
            # local disk handling
            has_local_disks = vm.get('_has_local_disks', False)
            if has_local_disks and vm_type == 'qemu':
                data['with-local-disks'] = 1
                self.logger.info(f"using with-local-disks flag")
            
            local_info = ' (local disks)' if has_local_disks else ''
            self.logger.info(f"migrating {vm.get('name', 'unnamed')} ({vmid}) {source_node} -> {target_node}{local_info}")
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                task_id = response.json().get('data')
                self.logger.info(f"[OK] Migration initiated for {vm.get('name', 'unnamed')} to {target_node} (Task: {task_id})")
                
                # Wait for migration to complete
                if task_id:
                    success = self._wait_for_task(source_node, task_id)
                    if success:
                        self.logger.info(f"[OK] Successfully migrated {vm.get('name', 'unnamed')} to {target_node}")
                        self.last_migration_log.append({
                            'timestamp': datetime.now().isoformat(),
                            'vm': vm.get('name', 'unnamed'),
                            'vmid': vmid,
                            'from_node': source_node,
                            'to_node': target_node,
                            'dry_run': False,
                            'success': True
                        })
                        return True
                    else:
                        self.logger.error(f"[ERROR] Migration task failed for {vm.get('name', 'unnamed')}")
                        self.last_migration_log.append({
                            'timestamp': datetime.now().isoformat(),
                            'vm': vm.get('name', 'unnamed'),
                            'vmid': vmid,
                            'from_node': source_node,
                            'to_node': target_node,
                            'dry_run': False,
                            'success': False,
                            'error': 'Task failed'
                        })
                        return False
                
                return True
            else:
                self.logger.error(f"[ERROR] Failed to migrate {vm.get('name', 'unnamed')}: {response.status_code} - {response.text}")
                self.last_migration_log.append({
                    'timestamp': datetime.now().isoformat(),
                    'vm': vm.get('name', 'unnamed'),
                    'vmid': vmid,
                    'from_node': source_node,
                    'to_node': target_node,
                    'dry_run': False,
                    'success': False,
                    'error': response.text
                })
                return False
                
        except Exception as e:
            self.logger.error(f"[ERROR] Error migrating VM: {e}")
            return False
    
    def _wait_for_task(self, node: str, task_id: str, timeout: int = 600) -> bool:
        """
        Wait for a Proxmox task to complete.
        
        MK: This polls the task status endpoint until the task is done or times out.
        Used after migrations, backups, etc. to ensure they complete before proceeding.
        
        Polling interval: 2s normally, 5s after errors (to avoid hammering a failing node)
        Default timeout: 10 minutes (should be enough for most migrations)
        
        Returns True only if task completed with exitstatus 'OK'.
        """
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/tasks/{task_id}/status"
                response = self._api_get(url)
                
                if response.status_code == 200:
                    task_status = response.json()['data']
                    status = task_status.get('status')
                    
                    if status == 'stopped':
                        # Task finished - check if successful
                        exit_status = task_status.get('exitstatus', '')
                        return exit_status == 'OK'
                    
                time.sleep(2)  # Poll every 2 seconds
                
            except Exception as e:
                self.logger.error(f"Error checking task status: {e}")
                time.sleep(5)  # Back off on errors
        
        self.logger.error(f"Task {task_id} timed out after {timeout} seconds")
        return False
    
    def enter_maintenance_mode(self, node_name: str, skip_evacuation: bool = False) -> MaintenanceTask:
        """Enter maintenance mode for a node
        
        Args:
            node_name: Name of the node
            skip_evacuation: If True, skip VM evacuation (NOT RECOMMENDED - use only for non-reboot updates)
        """
        with self.maintenance_lock:
            if node_name in self.nodes_in_maintenance:
                return self.nodes_in_maintenance[node_name]
            
            task = MaintenanceTask(node_name)
            self.nodes_in_maintenance[node_name] = task
        
        self.logger.info(f"[MAINT] Entering maintenance mode for node: {node_name}")
        
        if skip_evacuation:
            # MK: Skip evacuation - for non-reboot updates where user accepts the risk
            self.logger.warning(f"[MAINT] Skipping VM evacuation for {node_name} - VMs may be affected if update fails!")
            task.status = 'completed'
            task.total_vms = 0
            task.migrated_vms = 0
        else:
            # Start evacuation in background thread
            t = threading.Thread(target=self._evacuate_node, args=(node_name, task))
            t.daemon = True
            t.start()
        
        return task
    
    def _evacuate_node(self, node_name: str, task: MaintenanceTask):
        # move all VMs off this node
        try:
            task.status = 'evacuating'
            
            # Get all VMs on this node
            vms = self.get_vm_resources()
            node_vms = [
                vm for vm in vms 
                if vm.get('node') == node_name and 
                vm.get('status') == 'running' and
                vm.get('type') in ['qemu', 'lxc']
            ]
            
            task.total_vms = len(node_vms)
            task.pending_vms = node_vms.copy()
            
            if task.total_vms == 0:
                self.logger.info(f"[OK] No running VMs on {node_name}, maintenance mode ready")
                task.status = 'completed'
                return
            
            self.logger.info(f"[PKG] Found {task.total_vms} VMs to evacuate from {node_name}")
            
            # Sort VMs by memory (smallest first for faster evacuation)
            node_vms.sort(key=lambda x: x.get('mem', 0))
            
            for vm in node_vms:
                vm_name = vm.get('name', 'unnamed')
                vmid = vm.get('vmid')
                
                task.current_vm = {'vmid': vmid, 'name': vm_name}
                
                # Find best target node
                target_node = self.get_best_target_node(exclude_nodes=[node_name])
                
                if not target_node:
                    self.logger.error(f"[ERROR] No available target node for {vm_name}")
                    task.failed_vms.append({'vmid': vmid, 'name': vm_name, 'error': 'No target node available'})
                    task.pending_vms = [v for v in task.pending_vms if v.get('vmid') != vmid]
                    continue
                
                self.logger.info(f"[SYNC] Evacuating {vm_name} (VMID: {vmid}) to {target_node}")
                
                # Perform migration (never dry run during evacuation)
                success = self.migrate_vm(vm, target_node, dry_run=False)
                
                if success:
                    task.migrated_vms += 1
                    self.logger.info(f"[OK] Evacuated {vm_name} to {target_node} ({task.migrated_vms}/{task.total_vms})")
                else:
                    task.failed_vms.append({'vmid': vmid, 'name': vm_name, 'error': 'Migration failed'})
                    self.logger.error(f"[ERROR] Failed to evacuate {vm_name}")
                
                task.pending_vms = [v for v in task.pending_vms if v.get('vmid') != vmid]
            
            task.current_vm = None
            
            if len(task.failed_vms) == 0:
                task.status = 'completed'
                self.logger.info(f"[OK] Maintenance mode ready for {node_name} - all VMs evacuated")
            else:
                task.status = 'completed_with_errors'
                self.logger.warning(f"[WARN] Maintenance mode for {node_name} completed with {len(task.failed_vms)} failed migrations")
                
        except Exception as e:
            self.logger.error(f"[ERROR] Error during evacuation: {e}")
            task.status = 'failed'
            task.error = str(e)
    
    def exit_maintenance_mode(self, node_name: str) -> bool:
        with self.maintenance_lock:
            if node_name in self.nodes_in_maintenance:
                del self.nodes_in_maintenance[node_name]
                self.logger.info(f"[OK] Exited maintenance mode for node: {node_name}")
                return True
            return False
    
    def get_maintenance_status(self, node_name: str) -> Optional[Dict]:
        with self.maintenance_lock:
            if node_name in self.nodes_in_maintenance:
                return self.nodes_in_maintenance[node_name].to_dict()
            return None
    
    # =====================================================
    # HIGH AVAILABILITY (HA) FUNCTIONS
    # NS: Our own HA implementation - doesn't use Proxmox HA
    # Monitors nodes and can restart VMs on other nodes if one fails
    # Oct 2025: Added quorum host checking for 2-node clusters
    # =====================================================
    
    def _ha_discover_fallback_hosts(self):
        # find all node IPs for fallback
        self.logger.info("[HA] Auto-discovering cluster node IPs for fallback...")
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return
        
        try:
            h = self.current_host or self.config.host
            url = f"https://{h}:8006/api2/json/nodes"
            r = self._create_session().get(url, timeout=10)
            
            if r.status_code != 200:
                self.logger.error(f"[HA] Failed to get nodes for auto-discovery")
                return
            
            nodes = r.json().get('data', [])
            discovered_hosts = []
            
            for node in nodes:
                node_name = node.get('node')
                node_ip = self._get_node_ip(node_name)
                
                if node_ip and node_ip != self.config.host:
                    discovered_hosts.append(node_ip)
                    self.logger.info(f"[HA] Discovered fallback host: {node_name} -> {node_ip}")
            
            # Update fallback hosts
            self.config.fallback_hosts = discovered_hosts
            self.logger.info(f"[HA] Auto-configured {len(discovered_hosts)} fallback hosts: {discovered_hosts}")
            
            # Save config
            save_config()
            
        except Exception as e:
            self.logger.error(f"[HA] Error discovering fallback hosts: {e}")
    
    def _ha_update_fallback_hosts(self):
        # update fallback hosts
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes"
            response = self._create_session().get(url, timeout=10)
            
            if response.status_code != 200:
                return
            
            nodes = response.json().get('data', [])
            current_hosts = set()
            
            for node in nodes:
                node_name = node.get('node')
                if node.get('status') == 'online':
                    node_ip = self._get_node_ip(node_name)
                    if node_ip and node_ip != self.config.host:
                        current_hosts.add(node_ip)
            
            # Update if changed
            if set(self.config.fallback_hosts or []) != current_hosts:
                old_hosts = self.config.fallback_hosts
                self.config.fallback_hosts = list(current_hosts)
                self.logger.info(f"[HA] Updated fallback hosts: {old_hosts} -> {list(current_hosts)}")
                save_config()
                
        except Exception as e:
            self.logger.debug(f"[HA] Error updating fallback hosts: {e}")
    
    def start_ha_monitor(self):
        # start HA thread
        if self.ha_thread and self.ha_thread.is_alive():
            self.logger.info("HA monitor already running")
            return
        
        # Auto-discover fallback hosts
        self._ha_discover_fallback_hosts()
        
        # Check cluster size and warn about 2-node clusters
        try:
            if self.is_connected or self.connect_to_proxmox():
                host = self.current_host or self.config.host
                url = f"https://{host}:8006/api2/json/nodes"
                resp = self._create_session().get(url, timeout=10)
                if resp.status_code == 200:
                    nodes = resp.json().get('data', [])
                    node_count = len(nodes)
                    
                    if node_count == 2:
                        self.logger.warning("[HA] ════════════════════════════════════════════════════════")
                        self.logger.warning("[HA] 2-NODE CLUSTER DETECTED!")
                        self.logger.warning("[HA] ")
                        self.logger.warning("[HA] In a 2-node Proxmox cluster, when one node fails,")
                        self.logger.warning("[HA] the surviving node loses quorum and cannot start VMs!")
                        self.logger.warning("[HA] ")
                        self.logger.warning("[HA] RECOMMENDED SOLUTIONS:")
                        self.logger.warning("[HA] 1. Add a QDevice: pvecm qdevice setup <third-machine-IP>")
                        self.logger.warning("[HA] 2. Configure fencing in HA settings (IPMI/iLO)")
                        self.logger.warning("[HA] 3. Enable 'force_quorum_on_failure' (DANGEROUS!)")
                        self.logger.warning("[HA] ════════════════════════════════════════════════════════")
                    elif node_count == 1:
                        self.logger.warning("[HA] Single-node cluster - HA has limited functionality")
                    else:
                        self.logger.info(f"[HA] Cluster has {node_count} nodes - quorum should work normally")
        except Exception as e:
            self.logger.debug(f"[HA] Could not check cluster size: {e}")
        
        self.ha_enabled = True
        self.config.ha_enabled = True
        self.ha_thread = threading.Thread(target=self._ha_monitor_loop, daemon=True)
        self.ha_thread.start()
        self.logger.info("[HA] High Availability monitor started (checking every 10s)")  # 10s hardcoded for now
        
        # ═══════════════════════════════════════════════════════════════
        # AUTOMATIC SPLIT-BRAIN PROTECTION SETUP - NS Jan 2026
        # No manual configuration needed!
        # ═══════════════════════════════════════════════════════════════
        
        # Auto-discover shared storage if not already configured
        if not self.ha_config.get('storage_heartbeat_path'):
            self.logger.info("[HA] 🔍 Auto-discovering shared storages...")
            storage_path = self._ha_get_best_shared_storage_path()
            
            if storage_path:
                self.logger.info(f"[HA] ✓ Found shared storage: {storage_path}")
                self.ha_config['storage_heartbeat_path'] = storage_path
                self.ha_config['storage_heartbeat_enabled'] = True
                self.ha_config['dual_network_mode'] = True
                
                # Auto-install agents in background
                def auto_install():
                    time.sleep(5)  # Wait for HA to fully start
                    self.logger.info("[HA] 🔧 Auto-installing node agents...")
                    results = self._ha_install_agents_on_all_nodes()
                    success = sum(1 for v in results.values() if v)
                    self.logger.info(f"[HA] ✓ Node agents: {success}/{len(results)} installed")
                
                threading.Thread(target=auto_install, daemon=True).start()
            else:
                self.logger.warning("[HA] ⚠️ No shared storage found - SSH-only protection mode")
                self.logger.warning("[HA] ⚠️ Add shared storage (NFS/CephFS) for full dual-network protection")
        
        # Start storage-based heartbeat if configured
        if self.ha_config.get('storage_heartbeat_enabled') and self.ha_config.get('storage_heartbeat_path'):
            self._ha_storage_heartbeat_init()
            self.logger.info("[HA] ✓ Storage-based split-brain protection ACTIVE")
            self.logger.info(f"[HA] ✓ Heartbeat path: {self.ha_config.get('storage_heartbeat_path')}")
        else:
            self.logger.info("[HA] Split-brain protection: SSH verification active")
        
        # Restart self-fence agents if they were installed - NS Jan 2026
        if self.ha_config.get('self_fence_installed'):
            self.logger.info("[HA] 🛡️ Restarting self-fence agents on nodes...")
            threading.Thread(target=self._ha_start_self_fence_agents, daemon=True).start()
    
    def stop_ha_monitor(self):
        self.ha_enabled = False
        self.config.ha_enabled = False
        
        # Stop storage heartbeat thread
        if self.ha_heartbeat_thread and self.ha_heartbeat_thread.is_alive():
            self.ha_heartbeat_stop.set()
            self.ha_heartbeat_thread.join(timeout=5)
            self.logger.info("[HA] Storage heartbeat thread stopped")
        
        # Stop self-fence agents on nodes (but don't uninstall) - NS Jan 2026
        if self.ha_config.get('self_fence_installed'):
            self.logger.info("[HA] Stopping self-fence agents on nodes...")
            threading.Thread(target=self._ha_stop_self_fence_agents, daemon=True).start()
        
        self.logger.info("[HA] High Availability monitor stopped")
    
    def _ha_monitor_loop(self):
        # main loop - checks every 10s
        self.logger.info("[HA] HA monitor loop started")
        update_counter = 0
        
        while self.ha_enabled and not self.stop_event.is_set():
            try:
                self._ha_check_nodes()
                
                # Update fallback hosts every 60 seconds (6 iterations)
                update_counter += 1
                if update_counter >= 6:
                    self._ha_update_fallback_hosts()
                    update_counter = 0
                    
            except Exception as e:
                self.logger.error(f"[HA] Error in HA monitor: {e}")
            
            # Wait 10 seconds between checks
            for _ in range(10):
                if not self.ha_enabled or self.stop_event.is_set():
                    break
                time.sleep(1)
        
        self.logger.info("[HA] HA monitor loop ended")
    
    def _ha_check_nodes(self):
        if not self.is_connected:
            if not self.connect_to_proxmox():
                self.logger.error("[HA] Cannot connect to Proxmox for HA check")
                return
        
        try:
            # Use current connected host
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes"
            resp = self._create_session().get(url, timeout=10)
            
            if resp.status_code != 200:
                self.logger.error(f"[HA] Failed to get nodes: {resp.status_code}")
                # Try to reconnect (might switch to fallback host)
                self.session = None
                self.connect_to_proxmox()
                return
            
            nodes = resp.json().get('data', [])
            current_time = datetime.now()
            
            with self.ha_lock:
                for node in nodes:
                    node_name = node.get('node')
                    node_status = node.get('status', 'unknown')
                    
                    # init tracking for new nodes
                    if node_name not in self.ha_node_status:
                        self.ha_node_status[node_name] = {
                            'last_seen': current_time,
                            'status': 'online',
                            'consecutive_failures': 0,
                            'last_status': node_status
                        }
                    
                    prev_status = self.ha_node_status[node_name]['status']
                    
                    if node_status == 'online':
                        # Node is healthy
                        self.ha_node_status[node_name]['last_seen'] = current_time
                        self.ha_node_status[node_name]['consecutive_failures'] = 0
                        
                        if prev_status == 'offline':
                            self.logger.info(f"[HA] ✓ Node {node_name} is back ONLINE")
                            self.ha_node_status[node_name]['status'] = 'online'
                            # Clear recovery flag
                            self.ha_recovery_in_progress.pop(node_name, None)
                            
                            # NS: Restore quorum if all nodes are back online
                            self._ha_check_restore_quorum()
                            
                            # Broadcast node online event
                            try:
                                broadcast_sse('node_status', {
                                    'node': node_name,
                                    'status': 'online',
                                    'event': 'node_online',
                                    'message': f'Node {node_name} is back online',
                                    'cluster_id': self.id
                                }, self.id)
                            except Exception as e:
                                self.logger.error(f"[HA] Failed to broadcast node online: {e}")
                    else:
                        # Node appears offline
                        self.ha_node_status[node_name]['consecutive_failures'] += 1
                        failures = self.ha_node_status[node_name]['consecutive_failures']
                        
                        self.logger.warning(f"[HA] ⚠ Node {node_name} check failed ({failures}/{self.ha_failure_threshold})")
                        
                        if failures >= self.ha_failure_threshold:
                            if prev_status == 'online':
                                self.logger.error(f"[HA] ✗ Node {node_name} declared OFFLINE after {failures} failures!")
                                self.ha_node_status[node_name]['status'] = 'offline'
                                
                                # Broadcast node offline event immediately
                                try:
                                    broadcast_sse('node_status', {
                                        'node': node_name,
                                        'status': 'offline',
                                        'event': 'node_offline',
                                        'message': f'Node {node_name} is offline!',
                                        'cluster_id': self.id,
                                        'severity': 'critical'
                                    }, self.id)
                                except Exception as e:
                                    self.logger.error(f"[HA] Failed to broadcast node offline: {e}")
                                
                                # Skip if in maintenance or already recovering
                                if node_name in self.nodes_in_maintenance:
                                    self.logger.info(f"[HA] Node {node_name} is in maintenance, skipping HA recovery")
                                elif node_name in self.ha_recovery_in_progress:
                                    self.logger.info(f"[HA] Recovery already in progress for {node_name}")
                                else:
                                    # Trigger HA recovery
                                    self._ha_trigger_recovery(node_name)
                    
                    self.ha_node_status[node_name]['last_status'] = node_status
                    
        except Exception as e:
            self.logger.error(f"[HA] Error checking nodes: {e}")
    
    def _ha_trigger_recovery(self, failed_node: str):
        """Trigger HA recovery for a failed node - restart VMs on other nodes
        
        IMPORTANT: This HA feature requires SHARED STORAGE (NFS, Ceph, iSCSI, etc.)
        VMs on local storage cannot be automatically recovered!
        
        For 2-node clusters:
        - Both nodes must have access to the same shared storage
        - VMs will be fenced and restarted on the surviving node
        - Consider using a witness/quorum device for proper split-brain handling
        """
        self.logger.info(f"[HA] ========== STARTING HA RECOVERY FOR {failed_node} ==========")
        
        # Mark recovery in progress
        self.ha_recovery_in_progress[failed_node] = True
        
        # Start recovery in background thread
        recovery_thread = threading.Thread(
            target=self._ha_recovery_worker,
            args=(failed_node,),
            daemon=True
        )
        recovery_thread.start()
    
    def _ha_recovery_worker(self, failed_node: str):
        
        try:
            # ============================================
            # STEP 0: Try to acquire recovery lock (if storage configured)
            # ============================================
            if self.ha_config.get('storage_heartbeat_enabled'):
                if not self._ha_acquire_recovery_lock(failed_node):
                    self.logger.warning(f"[HA] Another instance is already recovering {failed_node}")
                    return
            
            # ============================================
            # SPLIT-BRAIN PREVENTION STEP 1: Wait period
            # ============================================
            recovery_delay = self.ha_config.get('recovery_delay', 30)
            self.logger.info(f"[HA] Waiting {recovery_delay}s before recovery (split-brain prevention)...")
            time.sleep(recovery_delay)
            
            # Check if node came back online during wait
            with self.ha_lock:
                if failed_node in self.ha_node_status:
                    if self.ha_node_status[failed_node].get('status') == 'online':
                        self.logger.info(f"[HA] Node {failed_node} came back online - cancelling recovery")
                        self._ha_release_recovery_lock(failed_node)
                        return
            
            # ============================================
            # SPLIT-BRAIN PREVENTION STEP 2: SSH CHECK (AUTOMATIC!)
            # ============================================
            # This is the KEY to automatic split-brain prevention:
            # If we can SSH to the "dead" node, it's actually alive!
            # This means we have a NETWORK SPLIT, not a node failure.
            
            self.logger.info(f"[HA] ═══════════════════════════════════════════════════════")
            self.logger.info(f"[HA] STEP 2: AUTOMATIC SPLIT-BRAIN CHECK")
            self.logger.info(f"[HA] ═══════════════════════════════════════════════════════")
            
            node_is_alive = False
            running_vms = []
            running_cts = []
            
            # Method 1: SSH Check (works if server network is up)
            self.logger.info(f"[HA] 2a. Checking via SSH...")
            ssh_check = self._ha_check_node_via_ssh(failed_node)
            
            if ssh_check['reachable']:
                node_is_alive = True
                running_vms = ssh_check.get('running_vms', [])
                running_cts = ssh_check.get('running_cts', [])
                self.logger.warning(f"[HA] ⚠️ Node {failed_node} IS REACHABLE via SSH!")
            
            # Method 2: Storage Heartbeat Check (works even if server network is down!)
            # This is CRITICAL for dual-network setups!
            if self.ha_config.get('dual_network_mode') or self.ha_config.get('storage_heartbeat_enabled'):
                self.logger.info(f"[HA] 2b. Checking via STORAGE HEARTBEAT (survives server network failure)...")
                
                heartbeat = self._ha_check_node_agent_heartbeat(failed_node)
                
                if heartbeat.get('alive'):
                    age = heartbeat.get('age_seconds', 0)
                    self.logger.critical(f"[HA] ════════════════════════════════════════════════════════")
                    self.logger.critical(f"[HA] ⚠️ NODE {failed_node} STORAGE HEARTBEAT IS ACTIVE!")
                    self.logger.critical(f"[HA] ⚠️ Heartbeat age: {age:.1f}s (timeout: {self.ha_config.get('storage_heartbeat_timeout', 30)}s)")
                    self.logger.critical(f"[HA] ⚠️ Node is ALIVE on storage network!")
                    self.logger.critical(f"[HA] ════════════════════════════════════════════════════════")
                    node_is_alive = True
                    # Get VMs from heartbeat if we didn't get them from SSH
                    if not running_vms:
                        running_vms = heartbeat.get('running_vms', [])
                    if not running_cts:
                        running_cts = heartbeat.get('running_cts', [])
                elif heartbeat.get('age_seconds') is not None:
                    self.logger.info(f"[HA] ✓ Storage heartbeat is STALE ({heartbeat.get('age_seconds'):.1f}s old)")
                else:
                    self.logger.info(f"[HA] No storage heartbeat found for {failed_node}")
            
            # Now handle based on combined results
            if node_is_alive:
                # NODE IS ALIVE! This is a network split!
                self.logger.critical(f"[HA] ☠️ SPLIT-BRAIN RISK DETECTED!")
                self.logger.critical(f"[HA] Node {failed_node} is ALIVE but not responding to Proxmox API")
                self.logger.critical(f"[HA] This is a NETWORK PARTITION, not a node failure!")
                
                has_running_vms = bool(running_vms or running_cts)
                
                if has_running_vms:
                    # CRITICAL: Must stop VMs on the partitioned node first!
                    self.logger.critical(f"[HA] ═══════════════════════════════════════════════════════")
                    self.logger.critical(f"[HA] STOPPING VMs ON PARTITIONED NODE TO PREVENT CORRUPTION")
                    self.logger.critical(f"[HA] Running VMs: {running_vms}")
                    self.logger.critical(f"[HA] Running CTs: {running_cts}")
                    self.logger.critical(f"[HA] ═══════════════════════════════════════════════════════")
                    
                    vms_stopped = False
                    
                    # Try SSH method first
                    if ssh_check['reachable']:
                        vms_stopped = self._ha_ssh_stop_vms_on_node(
                            failed_node, 
                            vmids=running_vms,
                            ctids=running_cts,
                            reachable_ips=ssh_check.get('reachable_ips', [])
                        )
                    
                    # If SSH didn't work, use poison pill via storage
                    if not vms_stopped and (self.ha_config.get('dual_network_mode') or self.ha_config.get('storage_heartbeat_enabled')):
                        self.logger.info(f"[HA] SSH stop failed, using POISON PILL via storage...")
                        if self._ha_write_poison_pill(failed_node, "Recovery initiated - stop all VMs"):
                            # Wait for the node agent to see the poison and stop VMs
                            self.logger.info(f"[HA] Waiting 30s for node agent to stop VMs...")
                            time.sleep(30)
                            
                            # Check if VMs stopped
                            heartbeat = self._ha_check_node_agent_heartbeat(failed_node)
                            if not heartbeat.get('running_vms') and not heartbeat.get('running_cts'):
                                vms_stopped = True
                                self.logger.info(f"[HA] ✓ Poison pill worked - VMs stopped")
                            else:
                                self.logger.warning(f"[HA] ⚠️ VMs may still be running after poison pill")
                    
                    if not vms_stopped:
                        strict_mode = self.ha_config.get('strict_fencing', False)
                        if strict_mode:
                            self.logger.error(f"[HA] ✗ STRICT MODE: Could not stop VMs on {failed_node} - ABORTING!")
                            self._ha_release_recovery_lock(failed_node)
                            return
                        else:
                            self.logger.warning(f"[HA] ⚠️ Could not confirm VMs stopped on {failed_node}")
                            self.logger.warning(f"[HA] ⚠️ Proceeding anyway - SPLIT-BRAIN RISK EXISTS!")
                    else:
                        self.logger.info(f"[HA] ✓ VMs stopped on {failed_node}")
                        
                    # Wait a moment for VMs to fully stop
                    self.logger.info(f"[HA] Waiting 10s for VMs to fully stop...")
                    time.sleep(10)
                else:
                    self.logger.info(f"[HA] No running VMs on {failed_node} - safe to proceed")
            else:
                # Node is truly unreachable on ALL networks - safe to proceed
                self.logger.info(f"[HA] ✓ Node {failed_node} confirmed UNREACHABLE (SSH failed, no storage heartbeat)")
                self.logger.info(f"[HA] ✓ Safe to proceed with recovery")
            
            # ============================================
            # ENSURE CONNECTION (might need to switch hosts)
            # ============================================
            if self.current_host and failed_node in self.current_host:
                self.logger.warning(f"[HA] Currently connected to failing node {self.current_host}, reconnecting...")
                self.session = None
                self.is_connected = False
            
            if not self.is_connected or not self.session:
                if not self.connect_to_proxmox():
                    self.logger.error(f"[HA] Cannot connect to any Proxmox node for recovery!")
                    self._ha_release_recovery_lock(failed_node)
                    return
                self.logger.info(f"[HA] Connected to {self.current_host} for recovery operations")
            
            # ============================================
            # SPLIT-BRAIN PREVENTION STEP 3: Quorum check
            # ============================================
            if self.ha_config.get('quorum_enabled', True):
                if not self._ha_check_quorum():
                    self.logger.error(f"[HA] ✗ NO QUORUM - Cannot proceed with recovery!")
                    self.logger.error(f"[HA] This prevents split-brain: we might be the isolated node")
                    self.logger.error(f"[HA] Configure quorum_hosts in HA config or disable quorum check")
                    self._ha_release_recovery_lock(failed_node)
                    return
                self.logger.info(f"[HA] ✓ Quorum confirmed - safe to proceed with recovery")
            
            # ============================================
            # SPLIT-BRAIN PREVENTION STEP 4: Network check
            # ============================================
            if self.ha_config.get('verify_network_before_recovery', True):
                if not self._ha_verify_network():
                    self.logger.error(f"[HA] ✗ Network verification failed - we might be isolated!")
                    self.logger.error(f"[HA] Not proceeding with recovery to prevent split-brain")
                    self._ha_release_recovery_lock(failed_node)
                    return
                self.logger.info(f"[HA] ✓ Network connectivity confirmed")
            
            # ============================================
            # Optional: Hardware fencing (IPMI/iLO if configured)
            # ============================================
            fenced = self._ha_fence_node(failed_node)
            if fenced:
                self.logger.info(f"[HA] ✓ Hardware fencing successful for {failed_node}")
            
            # Get list of VMs that were on the failed node
            vms_on_failed_node = self._ha_get_vms_on_node(failed_node)
            
            if not vms_on_failed_node:
                self.logger.info(f"[HA] No VMs found on failed node {failed_node}")
                self._ha_release_recovery_lock(failed_node)
                return
            
            self.logger.info(f"[HA] Found {len(vms_on_failed_node)} VMs on failed node {failed_node}")
            
            # Get available target nodes
            available_nodes = self._ha_get_available_nodes(exclude_node=failed_node)
            
            if not available_nodes:
                self.logger.error(f"[HA] No available nodes for HA recovery!")
                return
            
            # Special handling for 2-node cluster
            if len(available_nodes) == 1:
                self.logger.warning(f"[HA] 2-NODE CLUSTER DETECTED - Only {available_nodes[0]} available for recovery")
                self.logger.warning(f"[HA] All VMs will be started on {available_nodes[0]}")
            
            self.logger.info(f"[HA] Available target nodes: {available_nodes}")
            
            # For each VM, check storage type and try to recover
            recovered = 0
            failed = 0
            skipped_local = 0
            
            for vm in vms_on_failed_node:
                vmid = vm.get('vmid')
                vm_name = vm.get('name', f'VM {vmid}')
                vm_type = vm.get('type', 'qemu')
                
                # check VM uses shared storage
                storage_type = self._ha_check_vm_storage(vmid, vm_type, failed_node)
                
                if storage_type == 'local':
                    self.logger.warning(f"[HA] ⚠ SKIPPING {vm_name} ({vmid}) - Uses LOCAL storage, cannot recover!")
                    self.logger.warning(f"[HA]   → To enable HA for this VM, move its disks to shared storage")
                    skipped_local += 1
                    continue
                
                # Select target node (round-robin or least loaded)
                target_node = self._ha_select_target_node(available_nodes, vm)
                
                if not target_node:
                    self.logger.error(f"[HA] No target node available for {vm_name}")
                    failed += 1
                    continue
                
                self.logger.info(f"[HA] Attempting to recover {vm_name} ({vmid}) to {target_node}")
                
                # Try to start the VM on the target node
                # Note: This relies on shared storage - the VM config should already be available
                success = self._ha_start_vm_on_node(vmid, vm_type, target_node, failed_node)
                
                if success:
                    self.logger.info(f"[HA] ✓ Successfully recovered {vm_name} on {target_node}")
                    recovered += 1
                else:
                    self.logger.error(f"[HA] ✗ Failed to recover {vm_name}")
                    failed += 1
                
                # Small delay between VM starts
                time.sleep(2)
            
            self.logger.info(f"[HA] ========== HA RECOVERY COMPLETE ==========")
            self.logger.info(f"[HA] Recovered: {recovered}, Failed: {failed}, Skipped (local storage): {skipped_local}")
            
            if skipped_local > 0:
                self.logger.warning(f"[HA] {skipped_local} VMs were skipped because they use local storage!")
                self.logger.warning(f"[HA] Move these VMs to shared storage for full HA protection.")
            
        except Exception as e:
            self.logger.error(f"[HA] Error in recovery worker: {e}")
        finally:
            # Release recovery lock
            self._ha_release_recovery_lock(failed_node)
            
            # Keep recovery flag for a while to prevent duplicate recovery
            time.sleep(60)  # 60s cooldown, maybe make this configurable?
            self.ha_recovery_in_progress.pop(failed_node, None)
    
    def _ha_check_vm_storage(self, vmid: int, vm_type: str, node: str) -> str:
        """
        Check if VM uses shared or local storage
        
        Returns: 'shared', 'local', or 'unknown'
        
        MK: This was tricky - can't just check storage type because LVM/ZFS
        can be either local or shared depending on backend (SAN vs local disk).
        We now check the 'shared' flag from Proxmox which the admin sets.
        
        Claude helped optimize this logic, seems solid now.
        """
        try:
            host = self.current_host or self.config.host
            
            # Get VM config
            if vm_type == 'qemu':
                url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            else:
                url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
            
            response = self._create_session().get(url, timeout=10)
            
            if response.status_code != 200:
                return 'unknown'
            
            config = response.json().get('data', {})
            
            # Get storage configurations
            storage_url = f"https://{host}:8006/api2/json/storage"
            storage_response = self._create_session().get(storage_url, timeout=10)
            storage_configs = {}
            if storage_response.status_code == 200:
                for s in storage_response.json().get('data', []):
                    storage_configs[s['storage']] = s
            
            # Storage types that are ALWAYS shared (network-based)
            always_shared_types = ['nfs', 'cifs', 'glusterfs', 'cephfs', 'rbd', 'iscsi', 'iscsidirect', 'drbd', 'pbs']
            
            # Storage types that CAN be shared (with proper SAN/cluster setup)
            # - LVM: Can be shared on SAN with clvm/lvmlockd
            # - ZFS: Can be shared via iSCSI target
            # NOTE: lvmthin CANNOT be shared - it's always local!
            can_be_shared_types = ['lvm', 'zfspool', 'zfs']
            
            # Storage types that are ALWAYS local
            always_local_types = ['dir', 'lvmthin']  # dir and lvmthin are always local
            
            has_local = False
            has_shared = False
            
            for key, value in config.items():
                # Check QEMU disks (scsi0, virtio0, ide0, sata0, etc.)
                if vm_type == 'qemu' and any(key.startswith(p) for p in ['scsi', 'virtio', 'ide', 'sata', 'efidisk', 'tpmstate']):
                    if isinstance(value, str) and ':' in value:
                        storage_name = value.split(':')[0]
                        if storage_name in storage_configs:
                            storage_cfg = storage_configs[storage_name]
                            storage_type = storage_cfg.get('type', '')
                            # IMPORTANT: Check 'shared' flag FIRST - this is set by admin
                            # and indicates whether the storage is accessible from multiple nodes
                            is_shared_flag = storage_cfg.get('shared', 0)
                            
                            if is_shared_flag:
                                # Admin explicitly marked as shared
                                has_shared = True
                            elif storage_type in always_shared_types:
                                # Network storage types are always shared
                                has_shared = True
                            elif storage_type in always_local_types:
                                # Local directory is always local
                                has_local = True
                            elif storage_type in can_be_shared_types:
                                # LVM, ZFS etc. - depends on 'shared' flag
                                # If not marked as shared, assume local
                                has_local = True
                            else:
                                # Unknown type - be conservative, assume local
                                has_local = True
                
                # Check LXC rootfs and mount points
                if vm_type == 'lxc' and (key == 'rootfs' or key.startswith('mp')):
                    if isinstance(value, str) and ':' in value:
                        storage_name = value.split(':')[0]
                        if storage_name in storage_configs:
                            storage_cfg = storage_configs[storage_name]
                            storage_type = storage_cfg.get('type', '')
                            is_shared_flag = storage_cfg.get('shared', 0)
                            
                            if is_shared_flag:
                                has_shared = True
                            elif storage_type in always_shared_types:
                                has_shared = True
                            elif storage_type in always_local_types:
                                has_local = True
                            elif storage_type in can_be_shared_types:
                                has_local = True
                            else:
                                has_local = True
            
            # If any disk is local, the VM can't be recovered
            if has_local:
                return 'local'
            elif has_shared:
                return 'shared'
            else:
                return 'unknown'
                
        except Exception as e:
            self.logger.debug(f"[HA] Error checking VM storage: {e}")
            return 'unknown'
    
    def get_balancing_excluded_vms(self) -> List[int]:
        """Get list of VMIDs excluded from load balancing for this cluster
        
        MK: VMs can be excluded from balancing in the VM config.
        This is useful for VMs with GPU passthrough, local-only storage,
        or other reasons why they shouldn't be migrated automatically.
        """
        try:
            db = get_db()
            cursor = db.conn.cursor()
            cursor.execute(
                'SELECT vmid FROM balancing_excluded_vms WHERE cluster_id = ?',
                (self.id,)
            )
            return [row['vmid'] for row in cursor.fetchall()]
        except Exception as e:
            self.logger.error(f"Error getting excluded VMs: {e}")
            return []
    
    def set_vm_balancing_excluded(self, vmid: int, excluded: bool, reason: str = None, user: str = 'system') -> bool:
        """Set whether a VM should be excluded from load balancing
        
        LW: Added Jan 2026 for pinned VMs - some people run HA or manually placed VMs
        MK: Uses INSERT OR REPLACE for older SQLite compat (upsert syntax is 3.24+)
        
        Returns True on success, False on error.
        """
        try:
            db = get_db()
            cursor = db.conn.cursor()
            
            # MK: Ensure table exists (migration for existing databases)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS balancing_excluded_vms (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cluster_id TEXT NOT NULL,
                    vmid INTEGER NOT NULL,
                    reason TEXT,
                    created_by TEXT,
                    created_at TEXT,
                    UNIQUE(cluster_id, vmid)
                )
            ''')
            
            if excluded:
                # MK: Use INSERT OR REPLACE for SQLite compatibility
                cursor.execute('''
                    INSERT OR REPLACE INTO balancing_excluded_vms (cluster_id, vmid, reason, created_by, created_at)
                    VALUES (?, ?, ?, ?, ?)
                ''', (self.id, vmid, reason, user, datetime.now().isoformat()))
                self.logger.info(f"VM {vmid} excluded from balancing (reason: {reason})")
            else:
                cursor.execute(
                    'DELETE FROM balancing_excluded_vms WHERE cluster_id = ? AND vmid = ?',
                    (self.id, vmid)
                )
                self.logger.info(f"VM {vmid} removed from balancing exclusion")
            
            db.conn.commit()
            return True
        except Exception as e:
            self.logger.error(f"Error setting VM balancing exclusion: {e}")
            return False
    
    def is_vm_balancing_excluded(self, vmid: int) -> dict:
        """Check if a VM is excluded from balancing and get the reason"""
        try:
            db = get_db()
            cursor = db.conn.cursor()
            cursor.execute(
                'SELECT vmid, reason, created_by, created_at FROM balancing_excluded_vms WHERE cluster_id = ? AND vmid = ?',
                (self.id, vmid)
            )
            row = cursor.fetchone()
            if row:
                return {
                    'excluded': True,
                    'reason': row['reason'],
                    'created_by': row['created_by'],
                    'created_at': row['created_at']
                }
            return {'excluded': False}
        except Exception as e:
            self.logger.error(f"Error checking VM balancing exclusion: {e}")
            return {'excluded': False}
    
    def check_vm_storage_type(self, node: str, vmid: int, vm_type: str) -> str:
        # public wrapper for _ha_check_vm_storage
        return self._ha_check_vm_storage(vmid, vm_type, node)
    
    def _ha_get_vms_on_node(self, node: str) -> List[Dict]:
        try:
            # Ensure we're connected to a working node
            if not self.is_connected:
                if not self.connect_to_proxmox():
                    self.logger.error(f"[HA] Cannot connect to get VMs on {node}")
                    return []
            
            h = self.current_host or self.config.host
            url = f"https://{h}:8006/api2/json/cluster/resources"
            r = self._create_session().get(url, params={'type': 'vm'}, timeout=10)
            
            if r.status_code == 200:
                res = r.json().get('data', [])
                # Get VMs that were on the failed node AND were running
                vms = [x for x in res if x.get('node') == node and x.get('status') == 'running']
                self.logger.info(f"[HA] Found {len(vms)} running VMs on node {node}")
                return vms
            else:
                self.logger.error(f"[HA] Failed to get cluster resources: {r.status_code}")
            return []
        except Exception as e:
            self.logger.error(f"[HA] Error getting VMs on node: {e}")
            return []
    
    def _ha_get_available_nodes(self, exclude_node: str = None) -> List[str]:
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes"
            response = self._create_session().get(url, timeout=10)
            
            if response.status_code == 200:
                nodes = response.json().get('data', [])
                available = []
                for n in nodes:
                    nm = n.get('node')
                    if nm == exclude_node: continue
                    if n.get('status') != 'online': continue
                    if nm in self.nodes_in_maintenance: continue
                    available.append(nm)
                return available
            return []
        except:
            return []
    
    def _ha_select_target_node(self, available_nodes: List[str], vm: Dict) -> Optional[str]:
        # pick least loaded node
        if not available_nodes:
            return None
        
        node_status = self.get_node_status()
        
        if not node_status:
            return available_nodes[0]  # fallback
        
        # Sort by score (lower is better)
        scored_nodes = []
        for node in available_nodes:
            if node in node_status:
                score = node_status[node].get('score', 100)
                scored_nodes.append((node, score))
            else:
                scored_nodes.append((node, 50))  # Default score
        
        scored_nodes.sort(key=lambda x: x[1])
        return scored_nodes[0][0] if scored_nodes else None
    
    def _ha_start_vm_on_node(self, vmid: int, vm_type: str, target_node: str, original_node: str) -> bool:
        """Attempt to start a VM on a target node after HA failover
        
        CRITICAL FOR 2-NODE CLUSTERS:
        1. FIRST force quorum (otherwise pmxcfs is read-only!)
        2. THEN move config, clear locks, start VM
        
        Without quorum, /etc/pve is read-only and nothing works!
        """
        host = self.current_host or self.config.host
        
        try:
            # ═══════════════════════════════════════════════════════════════════
            # STEP 1: FORCE QUORUM FIRST (for 2-node clusters)
            # Without quorum, pmxcfs is read-only - we can't do anything!
            # ═══════════════════════════════════════════════════════════════════
            two_node_mode = self.ha_config.get('two_node_mode', False)
            force_quorum = self.ha_config.get('force_quorum_on_failure', False)
            
            if two_node_mode or force_quorum:
                self.logger.info(f"[HA] ═══════════════════════════════════════════════════════")
                self.logger.info(f"[HA] STEP 1: Forcing quorum on {target_node} (2-Node Mode)")
                self.logger.info(f"[HA] ═══════════════════════════════════════════════════════")
                
                if self._ha_try_force_quorum(target_node):
                    self.logger.info(f"[HA] ✓ Quorum forced successfully")
                    time.sleep(3)  # Give corosync/pmxcfs time to update
                else:
                    self.logger.error(f"[HA] ✗ Could not force quorum - SSH access required!")
                    self.logger.error(f"[HA] Manual fix: SSH to {target_node} and run: pvecm expected 1")
                    # Continue anyway - maybe it will work
            
            # ═══════════════════════════════════════════════════════════════════
            # STEP 2: Try to fence the dead node (if fencing is configured)
            # ═══════════════════════════════════════════════════════════════════
            self.logger.info(f"[HA] STEP 2: Attempting to fence {original_node}")
            fenced = self._ha_fence_node(original_node)
            if not fenced:
                self.logger.warning(f"[HA] ⚠ Could not fence node {original_node}")
            
            # ═══════════════════════════════════════════════════════════════════
            # STEP 3: Check current VM status
            # ═══════════════════════════════════════════════════════════════════
            self.logger.info(f"[HA] STEP 3: Checking VM {vmid} status")
            try:
                status_url = f"https://{host}:8006/api2/json/cluster/resources"
                status_resp = self._create_session().get(status_url, params={'type': 'vm'}, timeout=10)
                if status_resp.status_code == 200:
                    resources = status_resp.json().get('data', [])
                    vm_resource = next((r for r in resources if r.get('vmid') == vmid), None)
                    if vm_resource:
                        vm_status = vm_resource.get('status')
                        vm_node = vm_resource.get('node')
                        self.logger.info(f"[HA] VM {vmid} current state: {vm_status} on {vm_node}")
            except Exception as e:
                self.logger.warning(f"[HA] Could not check VM state: {e}")
            
            # ═══════════════════════════════════════════════════════════════════
            # STEP 4: Clear any locks on the VM
            # ═══════════════════════════════════════════════════════════════════
            self.logger.info(f"[HA] STEP 4: Clearing locks on VM {vmid}")
            self._ha_clear_vm_lock(vmid, vm_type, target_node, original_node)
            
            # ═══════════════════════════════════════════════════════════════════
            # STEP 5: Move VM config to target node
            # The config must be in /etc/pve/nodes/<target>/qemu-server/<vmid>.conf
            # ═══════════════════════════════════════════════════════════════════
            self.logger.info(f"[HA] STEP 5: Moving VM {vmid} config from {original_node} to {target_node}")
            config_moved = self._ha_move_vm_config(vmid, vm_type, original_node, target_node)
            if config_moved:
                self.logger.info(f"[HA] ✓ VM {vmid} config moved to {target_node}")
                time.sleep(2)  # Give pmxcfs time to sync
            else:
                self.logger.warning(f"[HA] Could not move config - will try to start anyway")
            
            # ═══════════════════════════════════════════════════════════════════
            # STEP 6: Start the VM on target node
            # ═══════════════════════════════════════════════════════════════════
            self.logger.info(f"[HA] STEP 6: Starting VM {vmid} on {target_node}")
            
            if vm_type == 'qemu':
                start_url = f"https://{host}:8006/api2/json/nodes/{target_node}/qemu/{vmid}/status/start"
            else:
                start_url = f"https://{host}:8006/api2/json/nodes/{target_node}/lxc/{vmid}/status/start"
            
            start_response = self._create_session().post(start_url, timeout=15)
            
            if start_response.status_code == 200:
                self.logger.info(f"[HA] ✓ VM {vmid} started successfully on {target_node}")
                return True
            
            error_text = start_response.text.lower()
            self.logger.error(f"[HA] Failed to start VM {vmid}: {start_response.text}")
            
            # Handle specific errors
            if 'no quorum' in error_text or 'cluster not ready' in error_text:
                self.logger.error(f"[HA] ✗ Still no quorum - force quorum may have failed")
                if not (two_node_mode or force_quorum):
                    self.logger.error(f"[HA] Enable '2-Node Cluster Mode' in HA settings!")
                return False
            
            if 'does not exist' in error_text or 'not found' in error_text:
                self.logger.error(f"[HA] ✗ VM config not found on {target_node}")
                self.logger.error(f"[HA] Config move may have failed - check SSH access")
                return False
            
            # Try with skiplock if there's a lock (only works for root@pam)
            if 'lock' in error_text and self.config.user.lower().startswith('root@'):
                self.logger.info(f"[HA] VM {vmid} has lock, trying with skiplock=1")
                start_response = self._create_session().post(start_url, data={'skiplock': 1}, timeout=15)
                if start_response.status_code == 200:
                    self.logger.info(f"[HA] ✓ VM {vmid} started with skiplock")
                    return True
            
            return False
                    
        except Exception as e:
            self.logger.error(f"[HA] Error starting VM {vmid} on {target_node}: {e}")
            return False
    
    def _ha_check_quorum(self) -> bool:
        """Check if we have quorum (majority vote) to proceed with HA actions
        
        For 2-node clusters, quorum is determined by:
        1. Pinging external hosts (gateway, DNS servers, etc.)
        2. If we can reach the outside world, we have quorum
        3. The isolated node (can't reach outside) should NOT take action
        
        This prevents split-brain where both nodes think the other is dead.
        """
        quorum_hosts = self.ha_config.get('quorum_hosts', [])
        gateway = self.ha_config.get('quorum_gateway')
        required_votes = self.ha_config.get('quorum_required_votes', 2)
        
        # Build list of hosts to check
        hosts_to_check = list(quorum_hosts)
        if gateway:
            hosts_to_check.insert(0, gateway)
        
        # If no quorum hosts configured, try to auto-detect
        if not hosts_to_check:
            # Try common external hosts
            hosts_to_check = [
                '8.8.8.8',      # Google DNS
                '1.1.1.1',      # Cloudflare DNS
                '9.9.9.9',      # Quad9 DNS
            ]
            self.logger.debug(f"[HA] No quorum hosts configured, using defaults: {hosts_to_check}")
        
        # Count successful pings
        votes = 0
        for host in hosts_to_check:
            if self._ha_ping_host(host):
                votes += 1
                self.logger.debug(f"[HA] Quorum vote from {host}: ✓")
            else:
                self.logger.debug(f"[HA] Quorum vote from {host}: ✗")
        
        # We always count ourselves as 1 vote
        total_possible = len(hosts_to_check) + 1  # +1 for ourselves
        votes += 1  # Our own vote
        
        has_quorum = votes >= required_votes
        
        self.ha_have_quorum = has_quorum
        self.ha_last_quorum_check = datetime.now()
        
        self.logger.info(f"[HA] Quorum check: {votes}/{total_possible} votes (need {required_votes})")
        
        return has_quorum
    
    def _ha_verify_network(self) -> bool:
        """Verify network connectivity before HA recovery
        
        This is an additional safety check beyond quorum.
        We verify we can reach important infrastructure.
        """
        check_hosts = self.ha_config.get('network_check_hosts', [])
        required = self.ha_config.get('network_check_required', 1)
        
        # If no hosts configured, use gateway or skip
        if not check_hosts:
            gateway = self.ha_config.get('quorum_gateway')
            if gateway:
                check_hosts = [gateway]
            else:
                # Try to detect default gateway
                try:
                    import subprocess
                    result = subprocess.run(
                        ['ip', 'route', 'show', 'default'],
                        capture_output=True, text=True, timeout=5
                    )
                    if result.returncode == 0 and 'via' in result.stdout:
                        gateway = result.stdout.split('via')[1].split()[0]
                        check_hosts = [gateway]
                except:
                    pass
        
        if not check_hosts:
            self.logger.debug("[HA] No network check hosts, skipping verification")
            return True
        
        successful = 0
        for host in check_hosts:
            if self._ha_ping_host(host):
                successful += 1
        
        if successful >= required:
            return True
        else:
            self.logger.warning(f"[HA] Network check: only {successful}/{len(check_hosts)} hosts reachable (need {required})")
            return False
    
    def _ha_ping_host(self, host: str, timeout: int = 3) -> bool:
        # ping check
        try:
            import subprocess
            result = subprocess.run(
                ['ping', '-c', '1', '-W', str(timeout), host],
                capture_output=True, timeout=timeout + 2
            )
            return result.returncode == 0
        except subprocess.TimeoutExpired:
            return False
        except FileNotFoundError:
            # ping not available, try socket
            try:
                import socket
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(timeout)
                result = sock.connect_ex((host, 80))
                sock.close()
                return result == 0
            except:
                return False
        except Exception:
            return False
    
    def _ha_self_fence(self):
        """Self-fence: prevent this node from running VMs if we lose quorum
        
        This is a safety mechanism for 2-node clusters.
        If we detect we're isolated (no quorum), we:
        1. Stop all running VMs gracefully
        2. Prevent new VMs from starting
        3. Optionally trigger a watchdog reboot
        
        This ensures the OTHER node (which has quorum) can take over.
        """
        if not self.ha_config.get('self_fence_enabled', True):
            return
        
        self.logger.error("[HA] ========== SELF-FENCING TRIGGERED ==========")
        self.logger.error("[HA] This node has lost quorum and will fence itself")
        self.logger.error("[HA] This prevents split-brain data corruption")
        
        # Mark that we should not start any VMs
        self.ha_have_quorum = False
        
        # If watchdog is enabled, trigger it
        if self.ha_config.get('watchdog_enabled', False):
            self.logger.error("[HA] Triggering hardware watchdog reboot in 60 seconds...")
            try:
                # Write to watchdog device - this will reboot the system
                # if not reset within the timeout
                with open('/dev/watchdog', 'w') as wd:
                    wd.write('V')  # Magic close character
                self.logger.error("[HA] Watchdog armed - system will reboot if quorum not restored")
            except Exception as e:
                self.logger.error(f"[HA] Could not arm watchdog: {e}")
    
    def _ha_fence_node(self, node: str) -> bool:
        """Attempt to fence (power off) a node via IPMI/iLO/DRAC
        
        Fencing is CRITICAL for 2-node clusters to prevent split-brain!
        Without fencing, the dead node might come back and try to access
        the same VMs, causing data corruption.
        
        Returns True if fencing succeeded or was not needed, False otherwise.
        """
        # check fencing is configured for this node
        fencing_config = getattr(self.config, 'fencing', {})
        node_fencing = fencing_config.get(node, {})
        
        if not node_fencing:
            self.logger.warning(f"[HA] No fencing configured for node {node}")
            self.logger.warning(f"[HA] Configure fencing in config: fencing.{node}.type = 'ipmi'")
            return False
        
        fence_type = node_fencing.get('type', '').lower()
        
        try:
            if fence_type == 'ipmi':
                # IPMI fencing
                ipmi_host = node_fencing.get('host')
                ipmi_user = node_fencing.get('user', 'ADMIN')
                ipmi_pass = node_fencing.get('password')
                
                if not ipmi_host or not ipmi_pass:
                    self.logger.error(f"[HA] IPMI fencing requires host and password")
                    return False
                
                self.logger.info(f"[HA] Fencing node {node} via IPMI at {ipmi_host}")
                
                # Power off via ipmitool
                import subprocess
                result = subprocess.run(
                    ['ipmitool', '-I', 'lanplus', '-H', ipmi_host, 
                     '-U', ipmi_user, '-P', ipmi_pass, 'power', 'off'],
                    capture_output=True, timeout=30
                )
                
                if result.returncode == 0:
                    self.logger.info(f"[HA] ✓ Successfully fenced node {node}")
                    time.sleep(5)  # Wait for node to fully power off
                    return True
                else:
                    self.logger.error(f"[HA] IPMI fencing failed: {result.stderr.decode()}")
                    return False
                    
            elif fence_type == 'ssh':
                # SSH fencing (emergency shutdown)
                ssh_host = node_fencing.get('host', node)
                ssh_user = node_fencing.get('user', 'root')
                
                self.logger.info(f"[HA] Fencing node {node} via SSH shutdown")
                
                import subprocess
                result = subprocess.run(
                    ['ssh', '-o', 'ConnectTimeout=5', '-o', 'StrictHostKeyChecking=no',
                     f'{ssh_user}@{ssh_host}', 'poweroff'],
                    capture_output=True, timeout=15
                )
                
                # SSH might fail if node is really dead, that's OK
                time.sleep(10)  # Wait for shutdown
                return True
                
            elif fence_type == 'proxmox':
                # Use Proxmox's built-in HA fencing
                self.logger.info(f"[HA] Using Proxmox HA fencing for {node}")
                # Proxmox handles this automatically if HA is configured
                return True
                
            else:
                self.logger.warning(f"[HA] Unknown fencing type: {fence_type}")
                return False
                
        except subprocess.TimeoutExpired:
            self.logger.warning(f"[HA] Fencing timed out for {node}")
            return False
        except FileNotFoundError:
            self.logger.error(f"[HA] Fencing tool not found (ipmitool/ssh)")
            return False
        except Exception as e:
            self.logger.error(f"[HA] Fencing error for {node}: {e}")
            return False
    
    # ═══════════════════════════════════════════════════════════════════════════
    # AUTOMATIC SPLIT-BRAIN PROTECTION - NS Jan 2026
    # 
    # ZERO MANUAL SETUP REQUIRED! Uses existing SSH credentials.
    #
    # How it works:
    # 1. Node appears dead (no API response)
    # 2. PegaProx tries SSH to the "dead" node
    # 3. If SSH works → Node is ALIVE (network split!)
    #    → Stop VMs on that node via SSH
    #    → Then start VMs on surviving node
    # 4. If SSH fails → Node is truly DEAD
    #    → Safe to start VMs on surviving node
    #
    # This prevents split-brain WITHOUT requiring:
    # - Manual agent installation
    # - Hardware fencing (IPMI/iLO)
    # - QDevice setup
    # - Shared storage configuration
    # ═══════════════════════════════════════════════════════════════════════════
    
    def _ha_check_node_via_ssh(self, node: str) -> dict:
        """Check if a node is actually alive via SSH - ON ALL NETWORKS!
        
        This is the KEY to automatic split-brain prevention:
        - If SSH works on ANY network → Node is alive, just network-partitioned
        - If SSH fails on ALL networks → Node is truly dead
        
        CRITICAL for multi-network setups (Server + iSCSI):
        - Server network down → SSH to management IP fails
        - But iSCSI network up → Node still writing to storage!
        - We MUST check ALL IPs before declaring node dead!
        
        Returns:
            dict with:
            - 'reachable': bool - Can we SSH to the node on ANY network?
            - 'reachable_ips': list - Which IPs responded
            - 'has_running_vms': bool - Are there VMs running?
            - 'running_vms': list - List of running VMIDs
            - 'has_storage_locks': bool - Are VMs still locking storage?
        """
        result = {
            'reachable': False, 
            'reachable_ips': [],
            'has_running_vms': False, 
            'running_vms': [], 
            'running_cts': [],
            'has_storage_locks': False
        }
        
        try:
            # Get ALL IPs for this node (all networks!)
            all_ips = self._ha_get_all_node_ips(node)
            
            if not all_ips:
                self.logger.warning(f"[HA] Cannot get any IP for node {node}")
                # Still check storage locks!
                lock_check = self._ha_check_vm_locks_on_storage(node)
                result['has_storage_locks'] = lock_check.get('has_active_locks', False)
                if result['has_storage_locks']:
                    self.logger.critical(f"[HA] ⚠️ Node {node} has ACTIVE STORAGE LOCKS!")
                    self.logger.critical(f"[HA] ⚠️ Node is likely alive on storage network!")
                    result['reachable'] = True  # Treat as reachable!
                return result
            
            self.logger.info(f"[HA] 🔍 Checking node {node} on ALL networks: {all_ips}")
            
            # Get SSH credentials from cluster config
            api_user = self.config.user
            ssh_user = api_user.split('@')[0] if '@' in api_user else api_user
            ssh_password = self.config.pass_
            ssh_key = getattr(self.config, 'ssh_key', '')
            
            # Try SSH on ALL IPs
            check_cmd = "qm list 2>/dev/null | grep running | awk '{print $1}' | tr '\\n' ',' ; echo '|' ; pct list 2>/dev/null | grep running | awk '{print $1}' | tr '\\n' ','"
            
            for ip in all_ips:
                self.logger.info(f"[HA] Trying SSH to {node} via {ip}...")
                
                output = None
                
                # Try SSH with key first
                if ssh_key:
                    output = self._ssh_run_command_with_key_output(ip, ssh_user, check_cmd, ssh_key)
                
                # Try passwordless SSH
                if output is None:
                    output = self._ssh_run_command_output(ip, ssh_user, check_cmd)
                
                # Try SSH with password
                if output is None and ssh_password:
                    output = self._ssh_run_command_with_password_output(ip, ssh_user, check_cmd, ssh_password)
                
                if output is not None:
                    result['reachable'] = True
                    result['reachable_ips'].append(ip)
                    self.logger.warning(f"[HA] ⚠️ Node {node} IS REACHABLE via {ip}!")
                    
                    # Parse output: "vmid1,vmid2,|ctid1,ctid2,"
                    parts = output.strip().split('|')
                    if len(parts) >= 1 and parts[0].strip():
                        vms = [v.strip() for v in parts[0].split(',') if v.strip()]
                        result['running_vms'] = vms
                    if len(parts) >= 2 and parts[1].strip():
                        cts = [c.strip() for c in parts[1].split(',') if c.strip()]
                        result['running_cts'] = cts
                    
                    result['has_running_vms'] = bool(result['running_vms'] or result['running_cts'])
                    break  # Found a working connection, no need to try more IPs
            
            if result['reachable']:
                self.logger.critical(f"[HA] ════════════════════════════════════════════════════════")
                self.logger.critical(f"[HA] ⚠️ NETWORK SPLIT DETECTED - NOT A NODE FAILURE!")
                self.logger.critical(f"[HA] Node {node} reachable via: {result['reachable_ips']}")
                self.logger.critical(f"[HA] Running VMs: {result['running_vms']}")
                self.logger.critical(f"[HA] Running CTs: {result['running_cts']}")
                self.logger.critical(f"[HA] ════════════════════════════════════════════════════════")
            else:
                # SSH failed on all IPs - but check storage locks as final safety check!
                self.logger.info(f"[HA] SSH failed on all IPs ({all_ips})")
                self.logger.info(f"[HA] Checking storage locks as final safety check...")
                
                lock_check = self._ha_check_vm_locks_on_storage(node)
                result['has_storage_locks'] = lock_check.get('has_active_locks', False)
                
                if result['has_storage_locks']:
                    self.logger.critical(f"[HA] ════════════════════════════════════════════════════════")
                    self.logger.critical(f"[HA] ⚠️ DANGER: SSH UNREACHABLE BUT STORAGE LOCKS ACTIVE!")
                    self.logger.critical(f"[HA] ⚠️ Node {node} may still be writing to storage!")
                    self.logger.critical(f"[HA] ⚠️ Locked VMs: {lock_check.get('locked_vms', [])}")
                    self.logger.critical(f"[HA] ════════════════════════════════════════════════════════")
                    # Treat as reachable to prevent split-brain!
                    result['reachable'] = True
                else:
                    self.logger.info(f"[HA] ✓ Node {node} confirmed UNREACHABLE (SSH failed, no storage locks)")
                
        except Exception as e:
            self.logger.error(f"[HA] Error in multi-network SSH check: {e}")
        
        return result
    
    def _ha_ssh_stop_vms_on_node(self, node: str, vmids: list = None, ctids: list = None, reachable_ips: list = None) -> bool:
        """Stop VMs on a node via SSH - tries ALL available IPs
        
        This is used when we detect a network split:
        The node is alive but partitioned from the cluster.
        We MUST stop VMs there before starting them elsewhere!
        
        Args:
            node: Node name
            vmids: List of VM IDs to stop
            ctids: List of Container IDs to stop
            reachable_ips: IPs that we know work (from the check phase)
        """
        try:
            # Use IPs that we know work, or get all IPs
            all_ips = reachable_ips if reachable_ips else self._ha_get_all_node_ips(node)
            if not all_ips:
                self.logger.error(f"[HA] No IPs available for node {node}")
                return False
            
            api_user = self.config.user
            ssh_user = api_user.split('@')[0] if '@' in api_user else api_user
            ssh_password = self.config.pass_
            ssh_key = getattr(self.config, 'ssh_key', '')
            
            self.logger.warning(f"[HA] ═══════════════════════════════════════════════════════")
            self.logger.warning(f"[HA] STOPPING VMs ON {node} VIA SSH (Split-Brain Prevention)")
            self.logger.warning(f"[HA] Trying IPs: {all_ips}")
            self.logger.warning(f"[HA] ═══════════════════════════════════════════════════════")
            
            # Find a working IP
            working_ip = None
            for ip in all_ips:
                # Quick connectivity test
                test_output = self._ssh_run_command_output(ip, ssh_user, "echo OK")
                if test_output is None and ssh_key:
                    test_output = self._ssh_run_command_with_key_output(ip, ssh_user, "echo OK", ssh_key)
                if test_output is None and ssh_password:
                    test_output = self._ssh_run_command_with_password_output(ip, ssh_user, "echo OK", ssh_password)
                
                if test_output is not None:
                    working_ip = ip
                    self.logger.info(f"[HA] Using IP {ip} for VM stop commands")
                    break
            
            if not working_ip:
                self.logger.error(f"[HA] Cannot reach node {node} on any IP!")
                return False
            
            stopped = []
            failed = []
            
            # Stop VMs
            if vmids:
                for vmid in vmids:
                    self.logger.info(f"[HA] Stopping VM {vmid} on {node}...")
                    stop_cmd = f"qm stop {vmid} --timeout 30 2>&1 || qm stop {vmid} --skiplock --timeout 30 2>&1"
                    
                    success = False
                    if ssh_key:
                        success = self._ssh_run_command_with_key(working_ip, ssh_user, stop_cmd, ssh_key)
                    if not success:
                        success = self._ssh_run_command(working_ip, ssh_user, stop_cmd)
                    if not success and ssh_password:
                        success = self._ssh_run_command_with_password(working_ip, ssh_user, stop_cmd, ssh_password)
                    
                    if success:
                        stopped.append(f"VM {vmid}")
                    else:
                        failed.append(f"VM {vmid}")
            
            # Stop containers
            if ctids:
                for ctid in ctids:
                    self.logger.info(f"[HA] Stopping CT {ctid} on {node}...")
                    stop_cmd = f"pct stop {ctid} --timeout 30 2>&1"
                    
                    success = False
                    if ssh_key:
                        success = self._ssh_run_command_with_key(working_ip, ssh_user, stop_cmd, ssh_key)
                    if not success:
                        success = self._ssh_run_command(working_ip, ssh_user, stop_cmd)
                    if not success and ssh_password:
                        success = self._ssh_run_command_with_password(working_ip, ssh_user, stop_cmd, ssh_password)
                    
                    if success:
                        stopped.append(f"CT {ctid}")
                    else:
                        failed.append(f"CT {ctid}")
            
            self.logger.info(f"[HA] Stopped on {node}: {stopped}")
            if failed:
                self.logger.warning(f"[HA] Failed to stop on {node}: {failed}")
            
            return len(failed) == 0
            
        except Exception as e:
            self.logger.error(f"[HA] Error stopping VMs via SSH: {e}")
            return False
    
    # ═══════════════════════════════════════════════════════════════════════════
    # SIMPLE SELF-FENCE AGENT - NS Jan 2026
    # 
    # Ultra-simple split-brain protection:
    # - Each node pings the manager AND the other node
    # - If BOTH unreachable → I'm isolated → stop my VMs
    # - No shared storage needed! Works with LVM, iSCSI, anything.
    # ═══════════════════════════════════════════════════════════════════════════
    
    _SELF_FENCE_AGENT_SCRIPT = '''#!/bin/bash
# PegaProx Self-Fence Agent
# Simple logic: If I can't reach ANYONE, I'm isolated → stop my VMs

MANAGER_IP="__MANAGER_IP__"
OTHER_NODES="__OTHER_NODES__"  # comma-separated list
CHECK_INTERVAL=5
FAIL_THRESHOLD=3
FAIL_COUNT=0

log() { 
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a /var/log/pegaprox-agent.log
}

can_reach_anyone() {
    # Try manager first
    if ping -c1 -W2 $MANAGER_IP >/dev/null 2>&1; then
        return 0
    fi
    
    # Try other nodes
    IFS=',' read -ra NODES <<< "$OTHER_NODES"
    for node_ip in "${NODES[@]}"; do
        if [ -n "$node_ip" ] && ping -c1 -W2 $node_ip >/dev/null 2>&1; then
            return 0
        fi
    done
    
    return 1
}

stop_all_vms() {
    log "STOPPING ALL VMs AND CONTAINERS!"
    
    # Stop VMs
    for vmid in $(qm list 2>/dev/null | grep running | awk '{print $1}'); do
        log "Stopping VM $vmid"
        qm stop $vmid --timeout 30 2>/dev/null &
    done
    
    # Stop containers
    for ctid in $(pct list 2>/dev/null | grep running | awk '{print $1}'); do
        log "Stopping CT $ctid"
        pct stop $ctid --timeout 30 2>/dev/null &
    done
    
    wait
    log "All VMs/CTs stopped"
}

log "PegaProx Self-Fence Agent starting"
log "Manager: $MANAGER_IP"
log "Other nodes: $OTHER_NODES"
log "Threshold: $FAIL_THRESHOLD failures"

while true; do
    if can_reach_anyone; then
        if [ $FAIL_COUNT -gt 0 ]; then
            log "Network recovered (was at $FAIL_COUNT failures)"
        fi
        FAIL_COUNT=0
    else
        ((FAIL_COUNT++))
        log "WARNING: Cannot reach anyone! Failure count: $FAIL_COUNT/$FAIL_THRESHOLD"
        
        if [ $FAIL_COUNT -ge $FAIL_THRESHOLD ]; then
            log "════════════════════════════════════════════════════════"
            log "ISOLATED! Cannot reach manager OR any other node!"
            log "Self-fencing to prevent split-brain..."
            log "════════════════════════════════════════════════════════"
            stop_all_vms
            
            # Wait for network to recover before resuming checks
            log "Waiting for network recovery..."
            while ! can_reach_anyone; do
                sleep 10
            done
            log "Network recovered! Resuming normal operation."
            FAIL_COUNT=0
        fi
    fi
    
    sleep $CHECK_INTERVAL
done
'''

    def _ha_install_self_fence_agent(self, node_name: str, node_ip: str) -> bool:
        """Install the simple self-fence agent on a node
        
        Args:
            node_name: Name of the node
            node_ip: IP address of the node
            
        Returns:
            True if installation successful
        """
        try:
            # Get manager IP (this PegaProx server)
            manager_ip = self._get_pegaprox_server_ip()
            if not manager_ip:
                self.logger.error(f"[HA] Cannot determine PegaProx server IP!")
                return False
            
            # Get other node IPs
            other_nodes = self._ha_get_other_node_ips(node_name)
            other_nodes_str = ','.join(other_nodes)
            
            self.logger.info(f"[HA] Installing self-fence agent on {node_name}")
            self.logger.info(f"[HA]   Manager IP: {manager_ip}")
            self.logger.info(f"[HA]   Other nodes: {other_nodes_str}")
            
            # Prepare agent script
            agent_script = self._SELF_FENCE_AGENT_SCRIPT
            agent_script = agent_script.replace('__MANAGER_IP__', manager_ip)
            agent_script = agent_script.replace('__OTHER_NODES__', other_nodes_str)
            
            # SSH credentials - try multiple sources
            ssh_user = getattr(self.config, 'ssh_user', None) or 'root'
            ssh_key = getattr(self.config, 'ssh_key_path', None) or getattr(self.config, 'ssh_key', None)
            ssh_password = getattr(self.config, 'ssh_password', None) or self.config.pass_  # Fallback to Proxmox password
            
            self.logger.debug(f"[HA] SSH credentials: user={ssh_user}, has_key={bool(ssh_key)}, has_password={bool(ssh_password)}")
            
            # Create agent script on node
            import base64
            script_b64 = base64.b64encode(agent_script.encode()).decode()
            
            install_cmd = f'''
echo "{script_b64}" | base64 -d > /usr/local/bin/pegaprox-agent.sh
chmod +x /usr/local/bin/pegaprox-agent.sh

cat > /etc/systemd/system/pegaprox-agent.service << 'SERVICEEOF'
[Unit]
Description=PegaProx Self-Fence Agent
After=network.target pve-cluster.service

[Service]
Type=simple
ExecStart=/usr/local/bin/pegaprox-agent.sh
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
SERVICEEOF

systemctl daemon-reload
systemctl enable pegaprox-agent.service
systemctl restart pegaprox-agent.service
echo "AGENT_INSTALLED"
'''
            
            # Execute installation - try different methods
            result = None
            
            # 1. Try with SSH key if available
            if result is None and ssh_key:
                self.logger.info(f"[HA] Trying SSH key authentication to {node_ip}...")
                result = self._ssh_run_command_with_key_output(node_ip, ssh_user, install_cmd, ssh_key)
            
            # 2. Try with sshpass (password) if available
            if result is None and ssh_password:
                # Check if sshpass is installed
                import shutil
                if shutil.which('sshpass'):
                    self.logger.info(f"[HA] Trying SSH password authentication to {node_ip}...")
                    result = self._ssh_run_command_with_password_output(node_ip, ssh_user, install_cmd, ssh_password)
                else:
                    self.logger.warning(f"[HA] sshpass not installed - cannot use password auth. Install with: apt install sshpass")
            
            # 3. Try with default SSH (requires pre-configured keys)
            if result is None:
                self.logger.info(f"[HA] Trying default SSH authentication to {node_ip}...")
                result = self._ssh_run_command_output(node_ip, ssh_user, install_cmd)
            
            if result and 'AGENT_INSTALLED' in result:
                self.logger.info(f"[HA] ✓ Self-fence agent installed on {node_name}")
                return True
            else:
                self.logger.error(f"[HA] ✗ Failed to install agent on {node_name}")
                self.logger.error(f"[HA]   SSH connection failed - please ensure:")
                self.logger.error(f"[HA]   1. SSH key is configured, OR")
                self.logger.error(f"[HA]   2. sshpass is installed and password is correct")
                self.logger.error(f"[HA]   Node IP: {node_ip}, User: {ssh_user}")
                self.logger.error(f"[HA]   Has key: {bool(ssh_key)}, Has password: {bool(ssh_password)}")
                return False
                
        except Exception as e:
            self.logger.error(f"[HA] Error installing self-fence agent on {node_name}: {e}")
            return False
    
    def _get_pegaprox_server_ip(self) -> str:
        """Get the IP address of this PegaProx server that nodes can reach"""
        import socket
        
        # Try to get the IP we use to connect to the cluster
        try:
            host = self.current_host or self.config.host
            # Create a socket to the cluster to find our outgoing IP
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.connect((host, 8006))
            local_ip = s.getsockname()[0]
            s.close()
            return local_ip
        except:
            pass
        
        # Fallback: try to get default interface IP
        try:
            hostname = socket.gethostname()
            return socket.gethostbyname(hostname)
        except:
            return ''
    
    def _ha_get_other_node_ips(self, exclude_node: str) -> list:
        """Get IP addresses of all nodes except the specified one"""
        other_ips = []
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes"
            resp = self._create_session().get(url, timeout=10)
            
            if resp.status_code == 200:
                nodes_data = resp.json().get('data', [])
                self.logger.debug(f"[HA] Found {len(nodes_data)} nodes in cluster")
                
                for node in nodes_data:
                    node_name = node.get('node', '')
                    self.logger.debug(f"[HA] Checking node: {node_name} (exclude: {exclude_node})")
                    
                    # Case-insensitive comparison - NS Jan 2026
                    if node_name and node_name.lower() != exclude_node.lower():
                        # Use existing _ha_get_node_ip function
                        node_ip = self._ha_get_node_ip(node_name)
                        
                        if node_ip:
                            other_ips.append(node_ip)
                            self.logger.info(f"[HA] Found other node: {node_name} -> {node_ip}")
                        else:
                            self.logger.warning(f"[HA] Could not find IP for node: {node_name}")
                            
        except Exception as e:
            self.logger.error(f"[HA] Error getting other node IPs: {e}")
        
        return other_ips
    
    def _ha_install_self_fence_on_all_nodes(self) -> dict:
        """Install self-fence agent on all cluster nodes
        
        Returns:
            Dict mapping node names to installation success
        """
        results = {}
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes"
            resp = self._create_session().get(url, timeout=10)
            
            if resp.status_code != 200:
                self.logger.error("[HA] Cannot get node list from cluster")
                return results
            
            nodes = resp.json().get('data', [])
            self.logger.info(f"[HA] Installing self-fence agent on {len(nodes)} nodes...")
            
            for node in nodes:
                node_name = node.get('node', '')
                if not node_name:
                    continue
                
                # Get node IP
                node_ip = self._ha_get_node_ip(node_name)
                if not node_ip:
                    self.logger.warning(f"[HA] Cannot determine IP for node {node_name}")
                    results[node_name] = False
                    continue
                
                # Install agent
                success = self._ha_install_self_fence_agent(node_name, node_ip)
                results[node_name] = success
            
            success_count = sum(1 for v in results.values() if v)
            self.logger.info(f"[HA] Self-fence agent installation complete: {success_count}/{len(results)}")
            
        except Exception as e:
            self.logger.error(f"[HA] Error installing self-fence agents: {e}")
        
        return results
    
    def _ha_uninstall_self_fence_on_all_nodes(self) -> dict:
        """Uninstall self-fence agent from all cluster nodes
        
        Returns:
            Dict mapping node names to uninstallation success
        """
        results = {}
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes"
            resp = self._create_session().get(url, timeout=10)
            
            if resp.status_code != 200:
                self.logger.error("[HA] Cannot get node list from cluster")
                return results
            
            nodes = resp.json().get('data', [])
            self.logger.info(f"[HA] Uninstalling self-fence agent from {len(nodes)} nodes...")
            
            for node in nodes:
                node_name = node.get('node', '')
                if not node_name:
                    continue
                
                # Get node IP
                node_ip = self._ha_get_node_ip(node_name)
                if not node_ip:
                    self.logger.warning(f"[HA] Cannot determine IP for node {node_name}")
                    results[node_name] = False
                    continue
                
                # Uninstall agent
                success = self._ha_uninstall_self_fence_agent(node_name, node_ip)
                results[node_name] = success
            
            success_count = sum(1 for v in results.values() if v)
            self.logger.info(f"[HA] Self-fence agent uninstallation complete: {success_count}/{len(results)}")
            
        except Exception as e:
            self.logger.error(f"[HA] Error uninstalling self-fence agents: {e}")
        
        return results
    
    def _ha_uninstall_self_fence_agent(self, node_name: str, node_ip: str) -> bool:
        """Uninstall the self-fence agent from a node
        
        Args:
            node_name: Name of the node
            node_ip: IP address of the node
            
        Returns:
            True if uninstallation successful
        """
        try:
            self.logger.info(f"[HA] Uninstalling self-fence agent from {node_name}")
            
            # SSH credentials - try multiple sources (same as install)
            ssh_user = getattr(self.config, 'ssh_user', None) or 'root'
            ssh_key = getattr(self.config, 'ssh_key_path', None) or getattr(self.config, 'ssh_key', None)
            ssh_password = getattr(self.config, 'ssh_password', None) or self.config.pass_
            
            uninstall_cmd = '''
systemctl stop pegaprox-agent.service 2>/dev/null || true
systemctl disable pegaprox-agent.service 2>/dev/null || true
rm -f /etc/systemd/system/pegaprox-agent.service
rm -f /usr/local/bin/pegaprox-agent.sh
systemctl daemon-reload
echo "AGENT_UNINSTALLED"
'''
            
            # Execute uninstallation - try different methods
            result = None
            
            if result is None and ssh_key:
                result = self._ssh_run_command_with_key_output(node_ip, ssh_user, uninstall_cmd, ssh_key)
            if result is None and ssh_password:
                result = self._ssh_run_command_with_password_output(node_ip, ssh_user, uninstall_cmd, ssh_password)
            if result is None:
                result = self._ssh_run_command_output(node_ip, ssh_user, uninstall_cmd)
            
            if result and 'AGENT_UNINSTALLED' in result:
                self.logger.info(f"[HA] ✓ Self-fence agent uninstalled from {node_name}")
                return True
            else:
                self.logger.error(f"[HA] ✗ Failed to uninstall agent from {node_name}: {result}")
                return False
                
        except Exception as e:
            self.logger.error(f"[HA] Error uninstalling self-fence agent from {node_name}: {e}")
            return False
    
    def _ha_stop_self_fence_agents(self):
        """Stop (but don't uninstall) self-fence agents on all nodes
        
        Used when HA is disabled to prevent agents from running without manager
        """
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes"
            resp = self._create_session().get(url, timeout=10)
            
            if resp.status_code != 200:
                return
            
            for node in resp.json().get('data', []):
                node_name = node.get('node', '')
                node_ip = self._ha_get_node_ip(node_name) if node_name else None
                
                if node_ip:
                    ssh_user = getattr(self.config, 'ssh_user', None) or 'root'
                    ssh_key = getattr(self.config, 'ssh_key_path', None) or getattr(self.config, 'ssh_key', None)
                    ssh_password = getattr(self.config, 'ssh_password', None) or self.config.pass_
                    
                    stop_cmd = 'systemctl stop pegaprox-agent.service 2>/dev/null || true'
                    
                    result = None
                    if ssh_key:
                        result = self._ssh_run_command_with_key_output(node_ip, ssh_user, stop_cmd, ssh_key)
                    if result is None and ssh_password:
                        result = self._ssh_run_command_with_password_output(node_ip, ssh_user, stop_cmd, ssh_password)
                    if result is None:
                        self._ssh_run_command_output(node_ip, ssh_user, stop_cmd)
                        
                    self.logger.info(f"[HA] Stopped self-fence agent on {node_name}")
                    
        except Exception as e:
            self.logger.error(f"[HA] Error stopping self-fence agents: {e}")
    
    def _ha_start_self_fence_agents(self):
        """Start self-fence agents on all nodes
        
        Used when HA is enabled and agents were previously installed
        """
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes"
            resp = self._create_session().get(url, timeout=10)
            
            if resp.status_code != 200:
                return
            
            for node in resp.json().get('data', []):
                node_name = node.get('node', '')
                node_ip = self._ha_get_node_ip(node_name) if node_name else None
                
                if node_ip:
                    ssh_user = getattr(self.config, 'ssh_user', None) or 'root'
                    ssh_key = getattr(self.config, 'ssh_key_path', None) or getattr(self.config, 'ssh_key', None)
                    ssh_password = getattr(self.config, 'ssh_password', None) or self.config.pass_
                    
                    start_cmd = 'systemctl start pegaprox-agent.service 2>/dev/null || true'
                    
                    result = None
                    if ssh_key:
                        result = self._ssh_run_command_with_key_output(node_ip, ssh_user, start_cmd, ssh_key)
                    if result is None and ssh_password:
                        result = self._ssh_run_command_with_password_output(node_ip, ssh_user, start_cmd, ssh_password)
                    if result is None:
                        self._ssh_run_command_output(node_ip, ssh_user, start_cmd)
                        
                    self.logger.info(f"[HA] Started self-fence agent on {node_name}")
                    
        except Exception as e:
            self.logger.error(f"[HA] Error starting self-fence agents: {e}")
    
    def _ha_discover_shared_storages(self, force_refresh: bool = False) -> list:
        """Automatically discover all shared storages in the cluster
        
        Queries Proxmox API for storages and filters for:
        - shared: 1 (accessible from all nodes)
        - type: nfs, cephfs, glusterfs (filesystem-based)
        
        Also detects block-based storages (LVM, iSCSI, RBD) to warn user.
        
        Results are cached to avoid repeated API calls.
        
        Returns list of dicts with storage info:
        [{'name': 'cephfs', 'type': 'cephfs', 'path': '/mnt/pve/cephfs', 'shared': True, 'is_filesystem': True}, ...]
        """
        # Return cached results if available and not forcing refresh
        if not force_refresh and hasattr(self, '_cached_shared_storages') and self._cached_shared_storages:
            return self._cached_shared_storages
        
        storages = []
        block_storages = []  # Track block-based storages for warning
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/storage"
            resp = self._create_session().get(url, timeout=10)
            
            if resp.status_code != 200:
                self.logger.warning("[HA] Cannot query storages from Proxmox API")
                return storages
            
            for storage in resp.json().get('data', []):
                # Only interested in shared storages
                if not storage.get('shared'):
                    continue
                
                storage_type = storage.get('type', '')
                storage_name = storage.get('storage', '')
                
                # Determine the mount path based on storage type
                mount_path = None
                
                if storage_type == 'nfs':
                    # NFS: path is directly provided or mounted at /mnt/pve/<name>
                    mount_path = storage.get('path') or f"/mnt/pve/{storage_name}"
                    
                elif storage_type == 'cephfs':
                    # CephFS: always mounted at /mnt/pve/<name>
                    mount_path = f"/mnt/pve/{storage_name}"
                    
                elif storage_type == 'glusterfs':
                    # GlusterFS: mounted at /mnt/pve/<name>
                    mount_path = f"/mnt/pve/{storage_name}"
                    
                elif storage_type == 'dir' and storage.get('shared'):
                    # Shared directory (might be NFS mounted elsewhere)
                    mount_path = storage.get('path')
                    
                elif storage_type == 'pbs':
                    # Proxmox Backup Server - not a filesystem, skip
                    continue
                    
                elif storage_type in ['rbd', 'iscsi', 'zfspool', 'lvm', 'lvmthin']:
                    # Block-based storage - track for warning but can't use for heartbeats
                    block_storages.append({
                        'name': storage_name,
                        'type': storage_type,
                        'shared': True,
                        'is_filesystem': False,
                    })
                    self.logger.debug(f"[HA] Found shared BLOCK storage (not usable for heartbeats): {storage_name} ({storage_type})")
                    continue
                
                if mount_path:
                    storages.append({
                        'name': storage_name,
                        'type': storage_type,
                        'path': mount_path,
                        'shared': True,
                        'is_filesystem': True,
                        'content': storage.get('content', ''),
                        'enabled': storage.get('enabled', True) if storage.get('disable') != 1 else False
                    })
                    self.logger.debug(f"[HA] Found shared filesystem storage: {storage_name} ({storage_type}) at {mount_path}")
            
            self.logger.info(f"[HA] Discovered {len(storages)} filesystem storages, {len(block_storages)} block storages")
            
            # Warn if only block storage available
            if not storages and block_storages:
                self.logger.warning(f"[HA] ════════════════════════════════════════════════════════")
                self.logger.warning(f"[HA] ⚠️ Only BLOCK storage found (LVM/iSCSI/RBD)!")
                self.logger.warning(f"[HA] ⚠️ Block storage cannot store heartbeat files.")
                self.logger.warning(f"[HA] ⚠️ Add a small NFS share for full protection.")
                self.logger.warning(f"[HA] ════════════════════════════════════════════════════════")
            
        except Exception as e:
            self.logger.error(f"[HA] Error discovering shared storages: {e}")
        
        # Cache results
        self._cached_shared_storages = storages
        self._cached_block_storages = block_storages
        return storages
    
    def _ha_get_best_shared_storage_path(self) -> str:
        """Automatically select the best shared storage for heartbeats
        
        Priority:
        1. CephFS (built-in, fast, reliable)
        2. GlusterFS (distributed)
        3. NFS (common, well-supported)
        4. Any other shared filesystem
        
        Returns the mount path or empty string if none found
        """
        storages = self._ha_discover_shared_storages()
        
        if not storages:
            self.logger.warning("[HA] No shared filesystem storages found!")
            return ''
        
        # Sort by preference
        type_priority = {'cephfs': 1, 'glusterfs': 2, 'nfs': 3, 'dir': 4}
        storages.sort(key=lambda s: type_priority.get(s['type'], 99))
        
        best = storages[0]
        self.logger.info(f"[HA] Selected best shared storage: {best['name']} ({best['type']}) at {best['path']}")
        
        return best['path']
    
    def _ha_auto_setup_split_brain_protection(self) -> bool:
        """Fully automatic split-brain protection setup
        
        1. Discovers shared storages
        2. Selects the best one
        3. Installs node agents
        4. Enables storage heartbeat
        
        Returns True if setup successful
        """
        self.logger.info("[HA] ═══════════════════════════════════════════════════════")
        self.logger.info("[HA] AUTOMATIC SPLIT-BRAIN PROTECTION SETUP")
        self.logger.info("[HA] ═══════════════════════════════════════════════════════")
        
        # Step 1: Find best shared storage
        storage_path = self._ha_get_best_shared_storage_path()
        
        if not storage_path:
            self.logger.warning("[HA] No shared storage found - falling back to SSH-only mode")
            self.logger.warning("[HA] ⚠️ This is NOT safe for dual-network setups!")
            return False
        
        # Step 2: Configure storage heartbeat
        self.ha_config['storage_heartbeat_enabled'] = True
        self.ha_config['storage_heartbeat_path'] = storage_path
        self.ha_config['dual_network_mode'] = True
        self.ha_config['poison_pill_enabled'] = True
        
        # Step 3: Install agents on all nodes
        self.logger.info(f"[HA] Installing node agents with storage path: {storage_path}")
        results = self._ha_install_agents_on_all_nodes()
        
        success_count = sum(1 for v in results.values() if v)
        total_count = len(results)
        
        if success_count == total_count:
            self.logger.info(f"[HA] ✓ All {total_count} node agents installed successfully!")
            return True
        elif success_count > 0:
            self.logger.warning(f"[HA] ⚠️ {success_count}/{total_count} node agents installed")
            return True
        else:
            self.logger.error("[HA] ✗ Failed to install any node agents!")
            return False
    
    # ═══════════════════════════════════════════════════════════════════════════
    # AUTOMATIC NODE AGENT INSTALLATION - NS Jan 2026
    # 
    # For dual-network setups where server network and storage network are separate.
    # The agent runs on each node and communicates via the STORAGE network.
    # This survives server network failures!
    #
    # The agent is automatically installed via SSH when dual_network_mode is enabled.
    # ═══════════════════════════════════════════════════════════════════════════
    
    # Minimal node agent script - embedded as string for auto-deployment
    _NODE_AGENT_SCRIPT = '''#!/bin/bash
# PegaProx Node Agent - Auto-installed for Dual-Network Split-Brain Protection
# This agent communicates via STORAGE network, not server network!

STORAGE_PATH="__STORAGE_PATH__"
HEARTBEAT_INTERVAL=5
NODE_NAME=$(hostname)
PEGAPROX_DIR="${STORAGE_PATH}/.pegaprox"
HEARTBEAT_FILE="${PEGAPROX_DIR}/heartbeat_node_${NODE_NAME}"
POISON_FILE="${PEGAPROX_DIR}/poison_${NODE_NAME}"
POISON_ACK_FILE="${PEGAPROX_DIR}/poison_ack_${NODE_NAME}"

log() { echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" >> /var/log/pegaprox-agent.log; }

write_heartbeat() {
    mkdir -p "$PEGAPROX_DIR" 2>/dev/null
    local vms=$(qm list 2>/dev/null | grep running | awk '{print $1}' | tr '\\n' ',')
    local cts=$(pct list 2>/dev/null | grep running | awk '{print $1}' | tr '\\n' ',')
    echo "{\\"timestamp\\":\\"$(date -Iseconds)\\",\\"node\\":\\"$NODE_NAME\\",\\"vms\\":\\"$vms\\",\\"cts\\":\\"$cts\\"}" > "$HEARTBEAT_FILE"
}

check_poison() {
    if [ -f "$POISON_FILE" ]; then
        log "POISON PILL DETECTED! Stopping all VMs..."
        for vmid in $(qm list 2>/dev/null | grep running | awk '{print $1}'); do
            log "Stopping VM $vmid"
            qm stop $vmid --timeout 30 2>/dev/null &
        done
        for ctid in $(pct list 2>/dev/null | grep running | awk '{print $1}'); do
            log "Stopping CT $ctid"
            pct stop $ctid --timeout 30 2>/dev/null &
        done
        wait
        echo "{\\"timestamp\\":\\"$(date -Iseconds)\\",\\"node\\":\\"$NODE_NAME\\",\\"vms_stopped\\":true}" > "$POISON_ACK_FILE"
        rm -f "$POISON_FILE"
        log "All VMs stopped, poison acknowledged"
    fi
}

log "PegaProx Node Agent starting (storage: $STORAGE_PATH)"
while true; do
    write_heartbeat
    check_poison
    sleep $HEARTBEAT_INTERVAL
done
'''

    _NODE_AGENT_SERVICE = '''[Unit]
Description=PegaProx Node Agent (Dual-Network Split-Brain Protection)
After=network.target pve-cluster.service
Wants=pve-cluster.service

[Service]
Type=simple
ExecStart=/usr/local/bin/pegaprox-agent.sh
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
'''

    def _ha_install_node_agent(self, node: str) -> bool:
        """Auto-install the node agent on a Proxmox node via SSH
        
        This is called automatically when dual_network_mode is enabled.
        The agent writes heartbeats to shared storage and responds to poison pills.
        """
        storage_path = self.ha_config.get('storage_heartbeat_path')
        if not storage_path:
            self.logger.error(f"[HA] Cannot install agent: storage_heartbeat_path not configured!")
            return False
        
        try:
            node_ip = self._ha_get_node_ip(node)
            if not node_ip:
                self.logger.error(f"[HA] Cannot get IP for node {node}")
                return False
            
            api_user = self.config.user
            ssh_user = api_user.split('@')[0] if '@' in api_user else api_user
            ssh_password = self.config.pass_
            ssh_key = getattr(self.config, 'ssh_key', '')
            
            self.logger.info(f"[HA] 🔧 Installing node agent on {node} ({node_ip})...")
            
            # Prepare script with actual storage path
            agent_script = self._NODE_AGENT_SCRIPT.replace('__STORAGE_PATH__', storage_path)
            
            # Create the script file on the node
            # Use base64 to avoid escaping issues
            import base64
            script_b64 = base64.b64encode(agent_script.encode()).decode()
            service_b64 = base64.b64encode(self._NODE_AGENT_SERVICE.encode()).decode()
            
            install_cmd = f'''
echo "{script_b64}" | base64 -d > /usr/local/bin/pegaprox-agent.sh && 
chmod +x /usr/local/bin/pegaprox-agent.sh && 
echo "{service_b64}" | base64 -d > /etc/systemd/system/pegaprox-agent.service && 
systemctl daemon-reload && 
systemctl enable pegaprox-agent && 
systemctl restart pegaprox-agent && 
echo "AGENT_INSTALLED_OK"
'''
            
            success = False
            if ssh_key:
                output = self._ssh_run_command_with_key_output(node_ip, ssh_user, install_cmd, ssh_key)
                success = output and 'AGENT_INSTALLED_OK' in output
            
            if not success:
                output = self._ssh_run_command_output(node_ip, ssh_user, install_cmd)
                success = output and 'AGENT_INSTALLED_OK' in output
            
            if not success and ssh_password:
                output = self._ssh_run_command_with_password_output(node_ip, ssh_user, install_cmd, ssh_password)
                success = output and 'AGENT_INSTALLED_OK' in output
            
            if success:
                self.logger.info(f"[HA] ✓ Node agent installed successfully on {node}")
                self.ha_config['node_agent_installed'][node] = True
                return True
            else:
                self.logger.error(f"[HA] ✗ Failed to install node agent on {node}")
                return False
                
        except Exception as e:
            self.logger.error(f"[HA] Error installing node agent on {node}: {e}")
            return False
    
    def _ha_install_agents_on_all_nodes(self) -> dict:
        """Install node agents on all nodes in the cluster
        
        Returns dict: {node_name: success_bool}
        """
        results = {}
        
        # Get all nodes
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes"
            resp = self._create_session().get(url, timeout=10)
            
            if resp.status_code != 200:
                self.logger.error("[HA] Cannot get node list for agent installation")
                return results
            
            nodes = resp.json().get('data', [])
            
            self.logger.info(f"[HA] ═══════════════════════════════════════════════════════")
            self.logger.info(f"[HA] INSTALLING NODE AGENTS ON {len(nodes)} NODES")
            self.logger.info(f"[HA] ═══════════════════════════════════════════════════════")
            
            for node in nodes:
                node_name = node.get('node')
                success = self._ha_install_node_agent(node_name)
                results[node_name] = success
            
            success_count = sum(1 for v in results.values() if v)
            self.logger.info(f"[HA] Agent installation complete: {success_count}/{len(nodes)} successful")
            
        except Exception as e:
            self.logger.error(f"[HA] Error during agent installation: {e}")
        
        return results
    
    def _ha_check_node_agent_heartbeat(self, node: str) -> dict:
        """Check if a node's agent is writing heartbeats to storage
        
        This works even if the server network is down, because
        the heartbeats are written via the storage network!
        
        Returns:
            dict with:
            - 'alive': bool - Is the heartbeat recent?
            - 'age_seconds': float - Age of last heartbeat
            - 'running_vms': list - VMs reported running
        """
        result = {'alive': False, 'age_seconds': None, 'running_vms': [], 'running_cts': []}
        
        storage_path = self.ha_config.get('storage_heartbeat_path')
        if not storage_path:
            return result
        
        heartbeat_file = os.path.join(storage_path, '.pegaprox', f'heartbeat_node_{node}')
        timeout = self.ha_config.get('storage_heartbeat_timeout', 30)
        
        try:
            if os.path.exists(heartbeat_file):
                mtime = datetime.fromtimestamp(os.path.getmtime(heartbeat_file))
                age = (datetime.now() - mtime).total_seconds()
                result['age_seconds'] = age
                result['alive'] = age < timeout
                
                # Read heartbeat content
                with open(heartbeat_file, 'r') as f:
                    import json
                    data = json.load(f)
                    if data.get('vms'):
                        result['running_vms'] = [v.strip() for v in data['vms'].split(',') if v.strip()]
                    if data.get('cts'):
                        result['running_cts'] = [c.strip() for c in data['cts'].split(',') if c.strip()]
                
                self.logger.debug(f"[HA] Node {node} storage heartbeat: age={age:.1f}s, alive={result['alive']}")
            else:
                self.logger.debug(f"[HA] No storage heartbeat file for {node}")
                
        except Exception as e:
            self.logger.error(f"[HA] Error reading storage heartbeat for {node}: {e}")
        
        return result
    
    def _ssh_run_command_output(self, host: str, user: str, command: str, timeout: int = 30) -> str:
        """Run SSH command and return output - HA PRIORITY (no rate limiting)
        
        NS: Jan 2026 - HA status checks bypass semaphore for immediate execution
        """
        _ssh_track_connection('ha', +1)
        
        try:
            import subprocess
            result = subprocess.run(
                ['ssh', '-o', 'ConnectTimeout=10', '-o', 'StrictHostKeyChecking=no',
                 '-o', 'BatchMode=yes', f'{user}@{host}', command],
                capture_output=True, text=True, timeout=timeout
            )
            if result.returncode == 0:
                return result.stdout
            self.logger.debug(f"[SSH] Command failed on {host}: {result.stderr[:200] if result.stderr else 'no error output'}")
            return None
        except subprocess.TimeoutExpired:
            self.logger.debug(f"[SSH] Command timed out on {host}")
            return None
        except Exception as e:
            self.logger.debug(f"[SSH] Exception on {host}: {e}")
            return None
        finally:
            _ssh_track_connection('ha', -1)
    
    def _ssh_run_command_with_key_output(self, host: str, user: str, command: str, key: str, timeout: int = 30) -> str:
        """Run SSH command with key and return output - HA PRIORITY (no rate limiting)
        
        NS: Jan 2026 - HA operations bypass semaphore
        """
        _ssh_track_connection('ha', +1)
        
        try:
            import subprocess
            import tempfile
            
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.key') as f:
                f.write(key)
                key_file = f.name
            os.chmod(key_file, 0o600)
            
            try:
                result = subprocess.run(
                    ['ssh', '-o', 'ConnectTimeout=10', '-o', 'StrictHostKeyChecking=no',
                     '-i', key_file, f'{user}@{host}', command],
                    capture_output=True, text=True, timeout=timeout
                )
                if result.returncode == 0:
                    return result.stdout
                self.logger.debug(f"[SSH] Key auth failed on {host}: {result.stderr[:200] if result.stderr else 'no error output'}")
                return None
            finally:
                os.unlink(key_file)
        except subprocess.TimeoutExpired:
            self.logger.debug(f"[SSH] Key command timed out on {host}")
            return None
        except Exception as e:
            self.logger.debug(f"[SSH] Key exception on {host}: {e}")
            return None
        finally:
            _ssh_track_connection('ha', -1)
    
    def _ssh_run_command_with_password_output(self, host: str, user: str, command: str, password: str, timeout: int = 30) -> str:
        """Run SSH command with password and return output - HA PRIORITY (no rate limiting)
        
        NS: Jan 2026 - HA operations bypass semaphore
        """
        _ssh_track_connection('ha', +1)
        
        try:
            import subprocess
            env = os.environ.copy()
            env['SSHPASS'] = password
            
            result = subprocess.run(
                ['sshpass', '-e', 'ssh', '-o', 'ConnectTimeout=10', '-o', 'StrictHostKeyChecking=no',
                 f'{user}@{host}', command],
                capture_output=True, text=True, timeout=timeout, env=env
            )
            if result.returncode == 0:
                return result.stdout
            self.logger.debug(f"[SSH] Password auth failed on {host}: {result.stderr[:200] if result.stderr else 'no error output'}")
            return None
        except FileNotFoundError:
            self.logger.debug(f"[SSH] sshpass not installed - cannot use password auth")
            return None
        except subprocess.TimeoutExpired:
            self.logger.debug(f"[SSH] Password command timed out on {host}")
            return None
        except Exception as e:
            self.logger.debug(f"[SSH] Password exception on {host}: {e}")
            return None
        finally:
            _ssh_track_connection('ha', -1)
    
    # Legacy storage-based functions kept for advanced users who want extra safety
    def _ha_storage_heartbeat_init(self):
        """Initialize storage-based heartbeat system"""
        if not self.ha_config.get('storage_heartbeat_enabled'):
            return
        
        storage_path = self.ha_config.get('storage_heartbeat_path')
        if not storage_path:
            self.logger.warning("[HA] Storage heartbeat enabled but no path configured!")
            return
        
        # Create .pegaprox directory on shared storage
        heartbeat_dir = os.path.join(storage_path, '.pegaprox')
        try:
            os.makedirs(heartbeat_dir, exist_ok=True)
            self.logger.info(f"[HA] Storage heartbeat directory: {heartbeat_dir}")
        except Exception as e:
            self.logger.error(f"[HA] Cannot create heartbeat directory: {e}")
            return
        
        # Start heartbeat writer thread
        self.ha_heartbeat_stop.clear()
        self.ha_heartbeat_thread = threading.Thread(
            target=self._ha_storage_heartbeat_writer,
            daemon=True,
            name=f"HA-Heartbeat-{self.config.name}"
        )
        self.ha_heartbeat_thread.start()
        self.logger.info("[HA] Storage heartbeat writer started")
    
    def _ha_storage_heartbeat_writer(self):
        """Background thread that writes heartbeats to shared storage"""
        storage_path = self.ha_config.get('storage_heartbeat_path')
        interval = self.ha_config.get('storage_heartbeat_interval', 5)
        
        heartbeat_dir = os.path.join(storage_path, '.pegaprox')
        
        while not self.ha_heartbeat_stop.is_set():
            try:
                # Write heartbeat for this PegaProx instance
                heartbeat_file = os.path.join(heartbeat_dir, f'heartbeat_pegaprox_{self.id}')
                heartbeat_data = {
                    'timestamp': datetime.now().isoformat(),
                    'cluster_id': self.id,
                    'cluster_name': self.config.name,
                    'connected_to': self.current_host,
                    'ha_enabled': self.config.ha_enabled,
                    'nodes_status': {k: v.get('status') for k, v in self.ha_node_status.items()}
                }
                
                with open(heartbeat_file, 'w') as f:
                    import json
                    json.dump(heartbeat_data, f)
                
                self.ha_last_heartbeat_write = datetime.now()
                
                # Also check for poison pills targeting our nodes
                self._ha_check_poison_pills()
                
            except Exception as e:
                self.logger.error(f"[HA] Heartbeat write error: {e}")
            
            self.ha_heartbeat_stop.wait(interval)
    
    def _ha_check_storage_heartbeat(self, node: str) -> dict:
        """Check the storage heartbeat of another node
        
        Returns:
            dict with keys:
            - 'alive': bool - True if heartbeat is recent
            - 'last_seen': datetime or None
            - 'age_seconds': int - seconds since last heartbeat
        """
        storage_path = self.ha_config.get('storage_heartbeat_path')
        timeout = self.ha_config.get('storage_heartbeat_timeout', 30)
        
        if not storage_path:
            return {'alive': None, 'last_seen': None, 'age_seconds': None, 'error': 'No storage path configured'}
        
        heartbeat_dir = os.path.join(storage_path, '.pegaprox')
        
        # Look for heartbeat files from Proxmox nodes (written by pvestatd or our agent)
        # Also look for VM status files
        result = {'alive': None, 'last_seen': None, 'age_seconds': None}
        
        try:
            # Check node-specific heartbeat (if we install an agent on nodes)
            node_heartbeat = os.path.join(heartbeat_dir, f'heartbeat_node_{node}')
            if os.path.exists(node_heartbeat):
                mtime = datetime.fromtimestamp(os.path.getmtime(node_heartbeat))
                age = (datetime.now() - mtime).total_seconds()
                result['last_seen'] = mtime
                result['age_seconds'] = age
                result['alive'] = age < timeout
                
                self.logger.debug(f"[HA] Node {node} heartbeat age: {age:.1f}s (timeout: {timeout}s)")
                return result
            
            # Fallback: Check if node has written to shared storage recently
            # This works with NFS/Ceph where we can see file mtimes
            node_status_file = os.path.join(heartbeat_dir, f'status_{node}')
            if os.path.exists(node_status_file):
                mtime = datetime.fromtimestamp(os.path.getmtime(node_status_file))
                age = (datetime.now() - mtime).total_seconds()
                result['last_seen'] = mtime
                result['age_seconds'] = age
                result['alive'] = age < timeout
                return result
            
            # No heartbeat found - node hasn't registered yet or is dead
            result['error'] = 'No heartbeat file found'
            result['alive'] = False
            
        except Exception as e:
            result['error'] = str(e)
            self.logger.error(f"[HA] Error checking storage heartbeat for {node}: {e}")
        
        return result
    
    def _ha_write_poison_pill(self, target_node: str, reason: str) -> bool:
        """Write a poison pill to storage, telling the target node to stop its VMs
        
        The poison pill mechanism:
        1. We write a file saying "node X must stop VMs"
        2. If node X is alive and can see storage, it reads this and stops VMs
        3. Node X then writes an acknowledgment
        4. We proceed with recovery only after acknowledgment OR timeout
        
        This is the SAFEST way to handle 2-node clusters without hardware fencing!
        """
        if not self.ha_config.get('poison_pill_enabled', True):
            return False
        
        storage_path = self.ha_config.get('storage_heartbeat_path')
        if not storage_path:
            return False
        
        heartbeat_dir = os.path.join(storage_path, '.pegaprox')
        poison_file = os.path.join(heartbeat_dir, f'poison_{target_node}')
        
        try:
            poison_data = {
                'timestamp': datetime.now().isoformat(),
                'target_node': target_node,
                'reason': reason,
                'issued_by': f'pegaprox_{self.id}',
                'action_required': 'STOP_ALL_VMS',
                'recovery_will_start_after': (datetime.now() + timedelta(seconds=60)).isoformat()
            }
            
            with open(poison_file, 'w') as f:
                import json
                json.dump(poison_data, f)
            
            self.logger.warning(f"[HA] ☠️ POISON PILL written for {target_node}: {reason}")
            self.logger.info(f"[HA] If {target_node} is alive and can see storage, it MUST stop its VMs")
            
            return True
            
        except Exception as e:
            self.logger.error(f"[HA] Failed to write poison pill: {e}")
            return False
    
    def _ha_check_poison_pills(self):
        """Check if there are poison pills targeting nodes in our cluster
        
        This is called regularly by the heartbeat thread.
        If we find a poison pill for a node we're connected to, we need to act!
        """
        storage_path = self.ha_config.get('storage_heartbeat_path')
        if not storage_path:
            return
        
        heartbeat_dir = os.path.join(storage_path, '.pegaprox')
        
        try:
            # Check for poison pills for any node in our cluster
            for node_name in list(self.ha_node_status.keys()):
                poison_file = os.path.join(heartbeat_dir, f'poison_{node_name}')
                
                if os.path.exists(poison_file):
                    with open(poison_file, 'r') as f:
                        import json
                        poison_data = json.load(f)
                    
                    # Check if poison pill is recent (< 5 minutes old)
                    poison_time = datetime.fromisoformat(poison_data['timestamp'])
                    age = (datetime.now() - poison_time).total_seconds()
                    
                    if age < 300:  # 5 minutes
                        self.logger.critical(f"[HA] ☠️ POISON PILL DETECTED for {node_name}!")
                        self.logger.critical(f"[HA] Reason: {poison_data.get('reason')}")
                        self.logger.critical(f"[HA] Issued by: {poison_data.get('issued_by')}")
                        
                        # If we're connected to this node, we should NOT start VMs on it
                        if self.current_host and node_name in self.current_host:
                            self.logger.critical(f"[HA] We are connected to poisoned node! Switching connection...")
                            # Don't start new VMs, let the other PegaProx instance handle recovery
                    
        except Exception as e:
            self.logger.error(f"[HA] Error checking poison pills: {e}")
    
    def _ha_wait_for_poison_ack(self, target_node: str, timeout: int = 60) -> bool:
        """Wait for acknowledgment that the poisoned node has stopped VMs
        
        Returns True if:
        - Acknowledgment received (node stopped VMs)
        - Timeout expired (we proceed anyway, with risk)
        
        Returns False if:
        - Node is confirmed still running VMs (ABORT RECOVERY!)
        """
        storage_path = self.ha_config.get('storage_heartbeat_path')
        if not storage_path:
            return True  # Can't check, proceed
        
        heartbeat_dir = os.path.join(storage_path, '.pegaprox')
        ack_file = os.path.join(heartbeat_dir, f'poison_ack_{target_node}')
        
        self.logger.info(f"[HA] Waiting up to {timeout}s for poison acknowledgment from {target_node}...")
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                if os.path.exists(ack_file):
                    with open(ack_file, 'r') as f:
                        import json
                        ack_data = json.load(f)
                    
                    if ack_data.get('vms_stopped'):
                        self.logger.info(f"[HA] ✓ Received poison acknowledgment from {target_node}")
                        self.logger.info(f"[HA]   VMs stopped: {ack_data.get('stopped_vms', [])}")
                        
                        # Clean up poison files
                        poison_file = os.path.join(heartbeat_dir, f'poison_{target_node}')
                        try:
                            os.remove(poison_file)
                            os.remove(ack_file)
                        except:
                            pass
                        
                        return True
                
                # Check if node heartbeat is still active (node is alive but ignoring poison)
                heartbeat = self._ha_check_storage_heartbeat(target_node)
                if heartbeat.get('alive') and heartbeat.get('age_seconds', 999) < 10:
                    self.logger.warning(f"[HA] ⚠ Node {target_node} is STILL ALIVE and writing heartbeats!")
                    self.logger.warning(f"[HA] This could indicate split-brain risk!")
                    
            except Exception as e:
                self.logger.error(f"[HA] Error checking poison ack: {e}")
            
            time.sleep(2)
        
        self.logger.warning(f"[HA] ⚠ Poison acknowledgment timeout for {target_node}")
        
        # Final check: is the node still writing heartbeats?
        heartbeat = self._ha_check_storage_heartbeat(target_node)
        if heartbeat.get('alive'):
            self.logger.error(f"[HA] ✗ DANGER: Node {target_node} heartbeat still active after poison!")
            self.logger.error(f"[HA] ✗ ABORTING RECOVERY to prevent split-brain!")
            return False
        
        self.logger.info(f"[HA] Node {target_node} heartbeat is stale - safe to proceed")
        return True
    
    def _ha_acquire_recovery_lock(self, failed_node: str) -> bool:
        """Try to acquire a distributed lock for recovery
        
        Only one PegaProx instance should perform recovery at a time.
        This prevents multiple recovery attempts from different sources.
        """
        storage_path = self.ha_config.get('storage_heartbeat_path')
        if not storage_path:
            return True  # No storage path, can't lock
        
        heartbeat_dir = os.path.join(storage_path, '.pegaprox')
        lock_file = os.path.join(heartbeat_dir, f'recovery_lock_{failed_node}')
        
        try:
            # Check if lock exists and is recent
            if os.path.exists(lock_file):
                mtime = datetime.fromtimestamp(os.path.getmtime(lock_file))
                age = (datetime.now() - mtime).total_seconds()
                
                if age < 300:  # Lock valid for 5 minutes
                    with open(lock_file, 'r') as f:
                        import json
                        lock_data = json.load(f)
                    
                    if lock_data.get('holder') != f'pegaprox_{self.id}':
                        self.logger.warning(f"[HA] Recovery lock held by {lock_data.get('holder')}")
                        return False
            
            # Acquire lock
            lock_data = {
                'timestamp': datetime.now().isoformat(),
                'holder': f'pegaprox_{self.id}',
                'target_node': failed_node,
                'cluster': self.config.name
            }
            
            with open(lock_file, 'w') as f:
                import json
                json.dump(lock_data, f)
            
            self.logger.info(f"[HA] ✓ Acquired recovery lock for {failed_node}")
            return True
            
        except Exception as e:
            self.logger.error(f"[HA] Error acquiring recovery lock: {e}")
            return False
    
    def _ha_release_recovery_lock(self, failed_node: str):
        """Release the recovery lock"""
        storage_path = self.ha_config.get('storage_heartbeat_path')
        if not storage_path:
            return
        
        heartbeat_dir = os.path.join(storage_path, '.pegaprox')
        lock_file = os.path.join(heartbeat_dir, f'recovery_lock_{failed_node}')
        
        try:
            if os.path.exists(lock_file):
                os.remove(lock_file)
                self.logger.info(f"[HA] Released recovery lock for {failed_node}")
        except Exception as e:
            self.logger.error(f"[HA] Error releasing recovery lock: {e}")
    
    def _ha_try_force_quorum(self, target_node: str) -> bool:
        """Force quorum on the surviving node in a 2-node cluster
        
        This runs: pvecm expected 1
        Which tells corosync to accept 1 node as quorum.
        
        Uses the cluster's existing credentials (same as Proxmox API login).
        """
        self.logger.warning(f"[HA] ════════════════════════════════════════════════════════")
        self.logger.warning(f"[HA] FORCING QUORUM ON {target_node}")
        self.logger.warning(f"[HA] ════════════════════════════════════════════════════════")
        
        try:
            # Get target node IP
            node_ip = self._ha_get_node_ip(target_node)
            
            if not node_ip:
                self.logger.error(f"[HA] Cannot determine IP for {target_node}")
                return False
            
            self.logger.info(f"[HA] Target node IP: {node_ip}")
            
            # Use cluster credentials - same as Proxmox API login
            # User format is usually "root@pam" - extract just the username
            api_user = self.config.user  # e.g. "root@pam"
            ssh_user = api_user.split('@')[0] if '@' in api_user else api_user  # -> "root"
            ssh_password = self.config.pass_
            ssh_key = getattr(self.config, 'ssh_key', '')  # SSH private key from cluster config
            
            self.logger.info(f"[HA] Using cluster credentials (user: {ssh_user})")
            
            # Method 1: Try SSH with configured key first (most secure)
            if ssh_key:
                self.logger.info(f"[HA] Trying SSH with configured key...")
                if self._ssh_run_command_with_key(node_ip, ssh_user, 'pvecm expected 1', ssh_key):
                    return True
            
            # Method 2: Try passwordless SSH (if system keys are set up)
            if self._ssh_run_command(node_ip, ssh_user, 'pvecm expected 1'):
                return True
            
            # Method 3: Try SSH with password (using sshpass - secure env var method)
            if ssh_password:
                if self._ssh_run_command_with_password(node_ip, ssh_user, 'pvecm expected 1', ssh_password):
                    return True
            
            self.logger.error(f"[HA] Could not force quorum via SSH")
            self.logger.error(f"[HA] Manual fix: SSH to {target_node} and run: pvecm expected 1")
            self.logger.error(f"[HA] Tip: Configure SSH key in cluster settings for secure automatic SSH")
            return False
                
        except Exception as e:
            self.logger.error(f"[HA] Error forcing quorum: {e}")
            return False
    
    def _ha_check_restore_quorum(self):
        """Check if all nodes are online and restore quorum to normal
        
        NS: Jan 2026 - When a failed node comes back online in a 2-node cluster,
        we need to restore the expected votes from 1 back to 2.
        
        This runs: pvecm expected N (where N = number of nodes)
        """
        try:
            # Check if two_node_mode is enabled
            two_node_mode = self.ha_config.get('two_node_mode', False)
            force_quorum = self.ha_config.get('force_quorum_on_failure', False)
            
            if not (two_node_mode or force_quorum):
                # Not in special quorum mode, nothing to restore
                return
            
            # Count total and online nodes
            total_nodes = len(self.ha_node_status)
            online_nodes = sum(1 for n, s in self.ha_node_status.items() if s.get('status') == 'online')
            
            if total_nodes < 2:
                return
            
            # Only restore if ALL nodes are online
            if online_nodes < total_nodes:
                self.logger.info(f"[HA] {online_nodes}/{total_nodes} nodes online - waiting for all nodes before restoring quorum")
                return
            
            self.logger.info(f"[HA] ════════════════════════════════════════════════════════")
            self.logger.info(f"[HA] ALL NODES ONLINE - RESTORING QUORUM TO {total_nodes}")
            self.logger.info(f"[HA] ════════════════════════════════════════════════════════")
            
            # Pick any online node to run the command
            target_node = None
            for node_name, status in self.ha_node_status.items():
                if status.get('status') == 'online':
                    target_node = node_name
                    break
            
            if not target_node:
                self.logger.error("[HA] No online node found to restore quorum")
                return
            
            # Get target node IP
            node_ip = self._ha_get_node_ip(target_node)
            
            if not node_ip:
                self.logger.error(f"[HA] Cannot determine IP for {target_node}")
                return
            
            # Use cluster credentials
            api_user = self.config.user
            ssh_user = api_user.split('@')[0] if '@' in api_user else api_user
            ssh_password = self.config.pass_
            ssh_key = getattr(self.config, 'ssh_key', '')  # SSH key from cluster config
            
            restore_cmd = f'pvecm expected {total_nodes}'
            
            self.logger.info(f"[HA] Running '{restore_cmd}' on {target_node} ({node_ip})")
            
            # Method 1: Try SSH with configured key first (most secure)
            if ssh_key:
                if self._ssh_run_command_with_key(node_ip, ssh_user, restore_cmd, ssh_key):
                    self.logger.info(f"[HA] ✓ Quorum restored to {total_nodes} nodes")
                    broadcast_sse('ha_status', {
                        'event': 'quorum_restored',
                        'message': f'Quorum restored to {total_nodes} nodes',
                        'expected_votes': total_nodes,
                        'cluster_id': self.id
                    }, self.id)
                    return
            
            # Method 2: Try passwordless SSH
            if self._ssh_run_command(node_ip, ssh_user, restore_cmd):
                self.logger.info(f"[HA] ✓ Quorum restored to {total_nodes} nodes")
                broadcast_sse('ha_status', {
                    'event': 'quorum_restored',
                    'message': f'Quorum restored to {total_nodes} nodes',
                    'expected_votes': total_nodes,
                    'cluster_id': self.id
                }, self.id)
                return
            
            # Method 3: Try SSH with password
            if ssh_password:
                if self._ssh_run_command_with_password(node_ip, ssh_user, restore_cmd, ssh_password):
                    self.logger.info(f"[HA] ✓ Quorum restored to {total_nodes} nodes")
                    broadcast_sse('ha_status', {
                        'event': 'quorum_restored',
                        'message': f'Quorum restored to {total_nodes} nodes',
                        'expected_votes': total_nodes,
                        'cluster_id': self.id
                    }, self.id)
                    return
            
            self.logger.warning(f"[HA] Could not auto-restore quorum")
            self.logger.warning(f"[HA] Manual fix: SSH to any node and run: {restore_cmd}")
                
        except Exception as e:
            self.logger.error(f"[HA] Error restoring quorum: {e}")
    
    def _ha_get_node_ip(self, node_name: str) -> Optional[str]:
        """Get primary IP address for a node"""
        ips = self._ha_get_all_node_ips(node_name)
        return ips[0] if ips else None
    
    def _ha_get_all_node_ips(self, node_name: str) -> List[str]:
        """Get ALL IP addresses for a node (all networks!)
        
        This is CRITICAL for split-brain prevention in multi-network setups:
        - Server network (management)
        - iSCSI network (storage)
        - Migration network
        - etc.
        
        If ANY network can reach the node, it's still alive!
        """
        ips = []
        
        # Check manual override first
        node_ips = self.ha_config.get('node_ips', {})
        if node_name in node_ips:
            manual_ip = node_ips[node_name]
            if isinstance(manual_ip, list):
                ips.extend(manual_ip)
            else:
                ips.append(manual_ip)
        
        # Try to get from cluster status (Corosync ring addresses)
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/cluster/status"
            resp = self._create_session().get(url, timeout=10)
            if resp.status_code == 200:
                for item in resp.json().get('data', []):
                    # Case-insensitive comparison - NS Jan 2026
                    if item.get('type') == 'node' and item.get('name', '').lower() == node_name.lower():
                        if item.get('ip') and item.get('ip') not in ips:
                            ips.append(item.get('ip'))
                            self.logger.debug(f"[HA] Found IP {item.get('ip')} for {node_name} from cluster status")
        except Exception as e:
            self.logger.debug(f"[HA] Could not get cluster status for {node_name}: {e}")
        
        # Try to get all network interfaces from node config
        # Use lowercase node name as Proxmox API is case-sensitive
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes/{node_name.lower()}/network"
            resp = self._create_session().get(url, timeout=10)
            if resp.status_code == 200:
                for iface in resp.json().get('data', []):
                    # Get IPv4 addresses from all interfaces
                    if iface.get('address') and iface.get('address') not in ips:
                        ips.append(iface.get('address'))
                    # Also check CIDR notation
                    if iface.get('cidr'):
                        ip = iface.get('cidr').split('/')[0]
                        if ip and ip not in ips:
                            ips.append(ip)
        except Exception as e:
            self.logger.debug(f"[HA] Could not get network info for {node_name}: {e}")
        
        # Try DNS resolution as fallback
        if not ips:
            try:
                import socket
                # Try node name directly
                ip = socket.gethostbyname(node_name)
                if ip:
                    ips.append(ip)
            except:
                pass
            
            try:
                import socket
                # Try FQDN patterns
                for suffix in ['', '.local', '.lan', '.cluster']:
                    try:
                        ip = socket.gethostbyname(f"{node_name}{suffix}")
                        if ip and ip not in ips:
                            ips.append(ip)
                    except:
                        pass
            except:
                pass
        
        self.logger.debug(f"[HA] Node {node_name} IPs: {ips}")
        return ips
    
    def _ha_check_vm_locks_on_storage(self, node_name: str, vmids: List[int] = None) -> dict:
        """Check if VMs have active locks on storage
        
        Proxmox creates lock files when VMs are running:
        - /etc/pve/nodes/<node>/qemu-server/<vmid>.conf.lock
        - Or lock entries in the config file itself
        
        If locks exist and are recent, the node is still actively using storage!
        
        This catches the case where:
        - Server network is down (SSH fails)
        - But iSCSI network is up (node still writing to disks!)
        
        Returns:
            dict with:
            - 'has_active_locks': bool
            - 'locked_vms': list of VMIDs with active locks
            - 'lock_age': age of most recent lock in seconds
        """
        result = {'has_active_locks': False, 'locked_vms': [], 'lock_age': None}
        
        try:
            host = self.current_host or self.config.host
            
            # Get VMs on the node
            if vmids is None:
                vms_on_node = self._ha_get_vms_on_node(node_name)
                vmids = [vm.get('vmid') for vm in vms_on_node]
            
            for vmid in vmids:
                # Check if VM config has a lock
                url = f"https://{host}:8006/api2/json/nodes/{node_name}/qemu/{vmid}/config"
                try:
                    resp = self._create_session().get(url, timeout=5)
                    if resp.status_code == 200:
                        config = resp.json().get('data', {})
                        if config.get('lock'):
                            result['locked_vms'].append(vmid)
                            self.logger.warning(f"[HA] VM {vmid} has active lock: {config.get('lock')}")
                except:
                    pass
            
            result['has_active_locks'] = len(result['locked_vms']) > 0
            
            # Also check pmxcfs for recent activity from the node
            # The .members file in /etc/pve shows active nodes
            try:
                # This works because pmxcfs is a cluster filesystem
                # If we can read it, we can see all nodes' activity
                url = f"https://{host}:8006/api2/json/cluster/status"
                resp = self._create_session().get(url, timeout=5)
                if resp.status_code == 200:
                    for item in resp.json().get('data', []):
                        if item.get('type') == 'node' and item.get('name') == node_name:
                            # Check if node is still in quorum
                            if item.get('online') == 1:
                                self.logger.warning(f"[HA] ⚠️ Node {node_name} is still showing as ONLINE in cluster status!")
                                result['has_active_locks'] = True
            except:
                pass
                
        except Exception as e:
            self.logger.error(f"[HA] Error checking VM locks: {e}")
        
        return result
    
    # ═══════════════════════════════════════════════════════════════════════════
    # SCSI-3 PERSISTENT RESERVATIONS - TRUE STORAGE-LEVEL FENCING
    # NS Jan 2026
    #
    # This is the MOST RELIABLE method for dual-network setups!
    # The storage itself prevents split-brain by:
    # 1. Each node registers a "key" with the storage
    # 2. When fencing, we send a PREEMPT command to remove the dead node's key
    # 3. The storage then REFUSES all I/O from the dead node
    # 4. Even if the node is alive, it cannot corrupt data!
    #
    # Requires: sg3_utils package on PegaProx server and nodes
    # Works with: iSCSI, FC, SAS (any SCSI device)
    # ═══════════════════════════════════════════════════════════════════════════
    
    def _ha_scsi_fence_node(self, failed_node: str, vm_disks: List[str] = None) -> bool:
        """Use SCSI-3 Persistent Reservations to fence a node from storage
        
        This is TRUE storage-level fencing - even if the node is alive,
        it cannot write to the disks anymore!
        
        Args:
            failed_node: Name of the node to fence
            vm_disks: List of disk paths (e.g., ['/dev/sdb', '/dev/mapper/...'])
        
        Returns:
            True if fencing succeeded
        """
        if not self.ha_config.get('scsi_reservation_enabled', False):
            return False
        
        self.logger.info(f"[HA] ═══════════════════════════════════════════════════════")
        self.logger.info(f"[HA] SCSI-3 PERSISTENT RESERVATION FENCING for {failed_node}")
        self.logger.info(f"[HA] ═══════════════════════════════════════════════════════")
        
        try:
            # Get the surviving node
            surviving_node = None
            for node in self.ha_node_status:
                if node != failed_node and self.ha_node_status[node].get('status') == 'online':
                    surviving_node = node
                    break
            
            if not surviving_node:
                self.logger.error("[HA] No surviving node found for SCSI fencing")
                return False
            
            # Get SSH credentials
            api_user = self.config.user
            ssh_user = api_user.split('@')[0] if '@' in api_user else api_user
            ssh_password = self.config.pass_
            
            surviving_ip = self._ha_get_node_ip(surviving_node)
            if not surviving_ip:
                return False
            
            # Get the failed node's SCSI registration key
            # Convention: key is based on node name hash or configured
            failed_key = self.ha_config.get('scsi_keys', {}).get(failed_node)
            if not failed_key:
                # Generate key from node name (first 8 chars hex)
                failed_key = format(hash(failed_node) & 0xFFFFFFFFFFFFFFFF, '016x')[:16]
            
            self.logger.info(f"[HA] Failed node SCSI key: {failed_key}")
            
            # Get disks to fence
            if not vm_disks:
                vm_disks = self._ha_get_shared_disks_for_node(failed_node)
            
            if not vm_disks:
                self.logger.warning("[HA] No shared disks found to fence")
                return False
            
            fenced_disks = []
            
            for disk in vm_disks:
                self.logger.info(f"[HA] Fencing disk: {disk}")
                
                # Command to preempt the failed node's registration
                # sg_persist --out --preempt --param-sark=<failed_key> --prout-type=5 <device>
                fence_cmd = f"sg_persist --out --preempt --param-sark={failed_key} --prout-type=5 {disk} 2>&1"
                
                # Run on surviving node
                success = self._ssh_run_command_with_password(surviving_ip, ssh_user, fence_cmd, ssh_password)
                
                if success:
                    fenced_disks.append(disk)
                    self.logger.info(f"[HA] ✓ Disk {disk} fenced successfully")
                else:
                    self.logger.error(f"[HA] ✗ Failed to fence disk {disk}")
            
            if fenced_disks:
                self.logger.info(f"[HA] ✓ SCSI fencing complete: {len(fenced_disks)} disks fenced")
                return True
            else:
                self.logger.error("[HA] ✗ SCSI fencing failed - no disks could be fenced")
                return False
                
        except Exception as e:
            self.logger.error(f"[HA] SCSI fencing error: {e}")
            return False
    
    def _ha_get_shared_disks_for_node(self, node_name: str) -> List[str]:
        """Get list of shared storage disks used by VMs on a node"""
        disks = []
        
        try:
            # Get VMs on the node
            vms = self._ha_get_vms_on_node(node_name)
            
            for vm in vms:
                vmid = vm.get('vmid')
                
                # Get VM config to find disk paths
                host = self.current_host or self.config.host
                url = f"https://{host}:8006/api2/json/nodes/{node_name}/qemu/{vmid}/config"
                
                resp = self._create_session().get(url, timeout=5)
                if resp.status_code == 200:
                    config = resp.json().get('data', {})
                    
                    # Look for disk entries (scsi0, virtio0, etc.)
                    for key, value in config.items():
                        if any(key.startswith(prefix) for prefix in ['scsi', 'virtio', 'ide', 'sata']):
                            if isinstance(value, str):
                                # Parse disk path from "storage:vm-xxx-disk-0" format
                                # For iSCSI, this would be the actual device path
                                if '/' in value:
                                    disk_path = value.split(',')[0]
                                    if disk_path not in disks:
                                        disks.append(disk_path)
        except Exception as e:
            self.logger.error(f"[HA] Error getting shared disks: {e}")
        
        return disks
    
    def _ssh_run_command(self, host: str, user: str, command: str, key_file: str = None) -> bool:
        """Run SSH command on remote host - HA PRIORITY (no rate limiting)
        
        NS: Jan 2026 - HA operations bypass the semaphore because:
        1. They are critical (fencing must happen immediately)
        2. They are short (< 5 seconds typically)
        3. They are rare (only during actual failures)
        """
        _ssh_track_connection('ha', +1)
        
        try:
            ssh_cmd = ['ssh', '-o', 'StrictHostKeyChecking=no', '-o', 'ConnectTimeout=10', '-o', 'BatchMode=yes']
            if key_file:
                ssh_cmd.extend(['-i', key_file])
            ssh_cmd.append(f'{user}@{host}')
            ssh_cmd.append(command)
            
            self.logger.info(f"[HA] Running: ssh {user}@{host} '{command}'")
            
            result = subprocess.run(ssh_cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                self.logger.info(f"[HA] ✓ Command successful: {result.stdout.strip()}")
                return True
            else:
                self.logger.error(f"[HA] ✗ Command failed: {result.stderr.strip()}")
                return False
        except subprocess.TimeoutExpired:
            self.logger.error(f"[HA] SSH command timed out")
            return False
        except FileNotFoundError:
            self.logger.error(f"[HA] SSH not found")
            return False
        finally:
            _ssh_track_connection('ha', -1)
    
    def _ssh_run_command_with_key(self, host: str, user: str, command: str, key_content: str) -> bool:
        """Run SSH command using a private key from cluster config
        
        MK: Security fix - writes key to temp file with strict permissions,
        uses it for SSH, then immediately deletes it.
        """
        import tempfile
        
        if not key_content or not key_content.strip():
            return False
        
        key_fd = None
        key_path = None
        
        try:
            # Write key to temp file with secure permissions (0600)
            key_fd, key_path = tempfile.mkstemp(prefix='pegaprox_ssh_', suffix='.key')
            os.chmod(key_path, 0o600)
            
            with os.fdopen(key_fd, 'w') as f:
                # Ensure key has proper newlines
                key_data = key_content.strip()
                if not key_data.endswith('\n'):
                    key_data += '\n'
                f.write(key_data)
            key_fd = None  # fd is now closed
            
            self.logger.info(f"[HA] Trying SSH with configured key...")
            
            # Run SSH with the key file
            result = self._ssh_run_command(host, user, command, key_file=key_path)
            
            return result
            
        except Exception as e:
            self.logger.error(f"[HA] SSH with key failed: {e}")
            return False
        finally:
            # Always clean up the temp key file
            if key_fd is not None:
                try:
                    os.close(key_fd)
                except:
                    pass
            if key_path and os.path.exists(key_path):
                try:
                    os.remove(key_path)
                except:
                    pass
    
    def _ssh_run_command_with_password(self, host: str, user: str, command: str, password: str) -> bool:
        """Run SSH command with password using sshpass - HA PRIORITY (no rate limiting)
        
        MK: Security fix - use SSHPASS environment variable instead of
        command line argument. Command line args are visible in 'ps aux'!
        
        NS: Jan 2026 - HA operations bypass semaphore for immediate execution
        """
        _ssh_track_connection('ha', +1)
        
        try:
            # Check if sshpass is available
            which_result = subprocess.run(['which', 'sshpass'], capture_output=True)
            if which_result.returncode != 0:
                self.logger.warning(f"[HA] sshpass not installed, trying without password...")
                _ssh_track_connection('ha', -1)  # Will be tracked by _ssh_run_command
                return self._ssh_run_command(host, user, command)
            
            ssh_cmd = [
                'sshpass', '-e',
                'ssh', '-o', 'StrictHostKeyChecking=no', '-o', 'ConnectTimeout=10',
                f'{user}@{host}',
                command
            ]
            
            env = os.environ.copy()
            env['SSHPASS'] = password
            
            self.logger.info(f"[HA] Running: sshpass ssh {user}@{host} '{command}'")
            
            result = subprocess.run(ssh_cmd, capture_output=True, text=True, timeout=30, env=env)
            
            if result.returncode == 0:
                self.logger.info(f"[HA] ✓ Command successful: {result.stdout.strip()}")
                return True
            else:
                self.logger.error(f"[HA] ✗ Command failed: {result.stderr.strip()}")
                return False
        except Exception as e:
            self.logger.error(f"[HA] SSH with password failed: {e}")
            return False
        finally:
            _ssh_track_connection('ha', -1)
    
    def _ha_clear_vm_lock(self, vmid: int, vm_type: str, target_node: str, original_node: str):
        
        host = self.current_host or self.config.host
        
        try:
            if vm_type == 'qemu':
                # Try to clear the lock via API
                config_url = f"https://{host}:8006/api2/json/nodes/{target_node}/qemu/{vmid}/config"
                
                # First check current config for lock
                config_response = self._create_session().get(config_url, timeout=10)
                if config_response.status_code == 200:
                    config = config_response.json().get('data', {})
                    if 'lock' in config:
                        self.logger.info(f"[HA] VM {vmid} has lock '{config['lock']}', clearing...")
                        unlock_response = self._create_session().put(
                            config_url, 
                            data={'delete': 'lock'},
                            timeout=15
                        )
                        if unlock_response.status_code == 200:
                            self.logger.info(f"[HA] ✓ Cleared lock on VM {vmid}")
                        else:
                            self.logger.warning(f"[HA] Could not clear lock via API: {unlock_response.text}")
            else:
                # LXC - similar approach
                config_url = f"https://{host}:8006/api2/json/nodes/{target_node}/lxc/{vmid}/config"
                config_response = self._create_session().get(config_url, timeout=10)
                if config_response.status_code == 200:
                    config = config_response.json().get('data', {})
                    if 'lock' in config:
                        self._create_session().put(config_url, data={'delete': 'lock'}, timeout=15)
                        
        except Exception as e:
            self.logger.warning(f"[HA] Error clearing VM lock: {e}")
            return False
    
    def _ha_move_vm_config(self, vmid: int, vm_type: str, source_node: str, target_node: str) -> bool:
        """Move VM config file from source node to target node
        
        In Proxmox, VM configs are stored per-node at:
        - QEMU: /etc/pve/nodes/<node>/qemu-server/<vmid>.conf
        - LXC:  /etc/pve/nodes/<node>/lxc/<vmid>.conf
        
        For HA failover, we need to move the config to the new node.
        This is done via SSH since there's no API for this.
        """
        try:
            # Determine config paths
            if vm_type == 'qemu':
                source_path = f"/etc/pve/nodes/{source_node}/qemu-server/{vmid}.conf"
                target_path = f"/etc/pve/nodes/{target_node}/qemu-server/{vmid}.conf"
            else:
                source_path = f"/etc/pve/nodes/{source_node}/lxc/{vmid}.conf"
                target_path = f"/etc/pve/nodes/{target_node}/lxc/{vmid}.conf"
            
            self.logger.info(f"[HA] Moving VM {vmid} config: {source_path} -> {target_path}")
            
            # Get target node IP
            target_ip = self._ha_get_node_ip(target_node)
            if not target_ip:
                target_ip = self.current_host or self.config.host
            
            # Use cluster credentials for SSH
            api_user = self.config.user
            ssh_user = api_user.split('@')[0] if '@' in api_user else api_user
            ssh_password = self.config.pass_
            
            # Build the move command - use mv to atomically move the config
            # The /etc/pve filesystem (pmxcfs) is cluster-aware
            move_cmd = f"mv {source_path} {target_path}"
            
            # Try passwordless SSH first
            if self._ssh_run_command(target_ip, ssh_user, move_cmd):
                return True
            
            # Try with password
            if ssh_password and self._ssh_run_command_with_password(target_ip, ssh_user, move_cmd, ssh_password):
                return True
            
            # Alternative: Try to copy instead of move (in case mv fails due to permissions)
            copy_cmd = f"cp {source_path} {target_path} && rm {source_path}"
            
            if self._ssh_run_command(target_ip, ssh_user, copy_cmd):
                return True
            
            if ssh_password and self._ssh_run_command_with_password(target_ip, ssh_user, copy_cmd, ssh_password):
                return True
            
            self.logger.error(f"[HA] ✗ Could not move VM config - SSH access required")
            self.logger.error(f"[HA] Manual fix: mv {source_path} {target_path}")
            return False
            
        except Exception as e:
            self.logger.error(f"[HA] Error moving VM config: {e}")
            return False
    
    def get_ha_status(self) -> Dict:
        
        
        # First, try to get node count from cluster if ha_node_status is empty
        # Do this OUTSIDE the lock to avoid deadlocks
        cluster_online = 0
        cluster_total = 0
        
        try:
            if self.is_connected:
                host = self.current_host or self.config.host
                url = f"https://{host}:8006/api2/json/nodes"
                session = self._create_session()
                response = session.get(url, timeout=5)
                if response.status_code == 200:
                    cluster_nodes = response.json().get('data', [])
                    cluster_total = len(cluster_nodes)
                    cluster_online = sum(1 for n in cluster_nodes if n.get('status') == 'online')
        except Exception as e:
            self.logger.debug(f"[HA] Could not fetch nodes for status: {e}")
        
        with self.ha_lock:
            # Count online/offline nodes from ha_node_status if available
            online_nodes = sum(1 for d in self.ha_node_status.values() if d.get('status') == 'online')
            total_nodes = len(self.ha_node_status)
            
            # If no nodes tracked yet, use cluster data
            if total_nodes == 0 and cluster_total > 0:
                total_nodes = cluster_total
                online_nodes = cluster_online
            
            # Determine status
            if total_nodes == 0:
                health_status = 'unknown'
            elif online_nodes == total_nodes:
                health_status = 'healthy'
            elif online_nodes > 0:
                health_status = 'degraded'
            else:
                health_status = 'critical'
            
            return {
                'enabled': self.ha_enabled,
                'check_interval': self.ha_check_interval,
                'failure_threshold': self.ha_failure_threshold,
                'nodes': {
                    name: {
                        'status': data['status'],
                        'last_seen': data['last_seen'].isoformat() if data.get('last_seen') else None,
                        'consecutive_failures': data.get('consecutive_failures', 0)
                    }
                    for name, data in self.ha_node_status.items()
                },
                'recovery_in_progress': list(self.ha_recovery_in_progress.keys()),
                'fallback_hosts': self.config.fallback_hosts,
                
                # Split-brain prevention status
                'split_brain_prevention': {
                    'quorum_enabled': self.ha_config.get('quorum_enabled', True),
                    'have_quorum': self.ha_have_quorum,
                    'last_quorum_check': self.ha_last_quorum_check.isoformat() if self.ha_last_quorum_check else None,
                    'self_fence_enabled': self.ha_config.get('self_fence_enabled', True),
                    'watchdog_enabled': self.ha_config.get('watchdog_enabled', False),
                    'recovery_delay': self.ha_config.get('recovery_delay', 30),
                    'quorum_hosts': self.ha_config.get('quorum_hosts', []),
                    'quorum_gateway': self.ha_config.get('quorum_gateway', ''),
                    'quorum_required_votes': self.ha_config.get('quorum_required_votes', 2),
                    'verify_network': self.ha_config.get('verify_network_before_recovery', True),
                    # 2-Node Mode
                    'two_node_mode': self.ha_config.get('two_node_mode', False),
                    # Storage-based Split-Brain Protection - NS Jan 2026
                    'storage_heartbeat_enabled': self.ha_config.get('storage_heartbeat_enabled', False),
                    'storage_heartbeat_path': self.ha_config.get('storage_heartbeat_path', ''),
                    'storage_heartbeat_timeout': self.ha_config.get('storage_heartbeat_timeout', 30),
                    'poison_pill_enabled': self.ha_config.get('poison_pill_enabled', True),
                    'strict_fencing': self.ha_config.get('strict_fencing', False),
                    'last_heartbeat_write': self.ha_last_heartbeat_write.isoformat() if self.ha_last_heartbeat_write else None,
                },
                
                # Cluster health summary
                'cluster_health': {
                    'online_nodes': online_nodes,
                    'total_nodes': total_nodes,
                    'is_2_node_cluster': total_nodes == 2,
                    'status': health_status
                },
                
                # Auto-discovered shared storages - NS Jan 2026
                'discovered_storages': getattr(self, '_cached_shared_storages', []),
                'block_storages': getattr(self, '_cached_block_storages', []),
                'auto_protection_active': bool(self.ha_config.get('storage_heartbeat_path')),
                
                # Self-Fence Protection - NS Jan 2026
                'self_fence_installed': self.ha_config.get('self_fence_installed', False),
                'self_fence_nodes': self.ha_config.get('self_fence_nodes', []),
            }
    
    def get_tasks(self, limit: int = 50) -> List[Dict]:
        """
        Retrieve recent tasks from the Proxmox cluster.
        
        MK: This fetches the cluster-wide task list and normalizes the data
        structure for the frontend. Tasks are sorted newest-first.
        
        Args:
            limit: Maximum number of tasks to return (default: 50)
            
        Returns:
            List of task dicts with normalized fields:
            - upid: Unique task identifier
            - node: Node where task ran
            - type: Task type (qmigrate, vzdump, etc.)
            - status: running|completed|failed
            - starttime/endtime: Unix timestamps
            - vmid: VM/CT ID if applicable
            - error: Error message if failed
        """
        # Fail early if we're not connected to avoid hanging
        if not self.is_connected or not self.session:
            return []
        
        tasks = []
        try:
            host = self.current_host or self.config.host
            
            # Get cluster-wide tasks
            url = f"https://{host}:8006/api2/json/cluster/tasks"
            response = self._create_session().get(url, timeout=10)
            
            if response.status_code == 200:
                cluster_tasks = response.json().get('data', [])
                for task in cluster_tasks[:limit]:
                    # Parse the task and extract useful info
                    task_info = {
                        'upid': task.get('upid', ''),
                        'node': task.get('node', ''),
                        'type': task.get('type', ''),
                        'status': task.get('status', 'running'),
                        'starttime': task.get('starttime'),
                        'endtime': task.get('endtime'),
                        'user': task.get('user', ''),
                        'id': task.get('id', ''),
                    }
                    
                    # Parse VMID from ID if present
                    if task_info['id']:
                        try:
                            task_info['vmid'] = int(task_info['id'])
                        except (ValueError, TypeError):
                            task_info['vmid'] = task_info['id']
                    
                    # Get exit status for completed tasks
                    if task.get('status') and task['status'] != 'running':
                        task_info['exitstatus'] = task.get('exitstatus', '')
                        # Mark as failed if exit status indicates error
                        if task_info['exitstatus'] and 'error' in task_info['exitstatus'].lower():
                            task_info['status'] = 'failed'
                            task_info['error'] = task_info['exitstatus']
                        elif task_info['exitstatus'] and task_info['exitstatus'] != 'OK':
                            task_info['status'] = 'failed'
                            task_info['error'] = task_info['exitstatus']
                    
                    tasks.append(task_info)
            
            # Sort by starttime descending (newest first)
            tasks.sort(key=lambda x: x.get('starttime') or 0, reverse=True)
            
            return tasks
            
        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError):
            # MK: don't immediately mark disconnected - transient errors happen
            return []
        except Exception as e:
            self.logger.error(f"Error getting tasks: {e}")
            return []
    
    def get_datacenter_options(self) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/cluster/options"
            response = self._create_session().get(url, timeout=15)
            
            if response.status_code == 200:
                options = response.json().get('data', {})
                return {
                    'success': True,
                    'options': {
                        'task_log_max_days': options.get('max-workers', 4),  # Default 4
                        'console': options.get('console', 'default'),
                        'keyboard': options.get('keyboard', ''),
                        'language': options.get('language', ''),
                        'email_from': options.get('email_from', ''),
                        'migration_type': options.get('migration', {}).get('type', 'secure'),
                        'migration_network': options.get('migration', {}).get('network', ''),
                        'ha_shutdown_policy': options.get('ha', {}).get('shutdown_policy', 'conditional'),
                        # Raw options for reference
                        'raw': options
                    }
                }
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            self.logger.error(f"Error getting datacenter options: {e}")
            return {'success': False, 'error': str(e)}
    
    def stop_task(self, node: str, upid: str) -> bool:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return False
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes/{node}/tasks/{upid}"
            response = self._api_delete(url)
            
            if response.status_code == 200:
                self.logger.info(f"Task {upid} cancelled on {node}")
                return True
            else:
                self.logger.error(f"Failed to cancel task {upid}: {response.status_code}")
                return False
        except Exception as e:
            self.logger.error(f"Error cancelling task: {e}")
            return False
    
    def get_task_log(self, node: str, upid: str, limit: int = 1000) -> str:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return "Error: Not connected to Proxmox"
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes/{node}/tasks/{upid}/log"
            params = {'limit': limit}
            response = self._create_session().get(url, params=params)
            
            if response.status_code == 200:
                data = response.json().get('data', [])
                # Combine log lines
                log_lines = []
                for entry in sorted(data, key=lambda x: x.get('n', 0)):
                    line = entry.get('t', '')
                    log_lines.append(line)
                return '\n'.join(log_lines)
            else:
                return f"Error: Could not fetch log (Status {response.status_code})"
        except Exception as e:
            self.logger.error(f"Error getting task log: {e}")
            return f"Error: {str(e)}"
    
    # =====================================================
    # PROXMOX NATIVE HA INTEGRATION
    # =====================================================
    
    def get_proxmox_ha_resources(self) -> List[Dict]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/cluster/ha/resources"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting Proxmox HA resources: {e}")
            return []
    
    def get_proxmox_ha_groups(self) -> List[Dict]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/cluster/ha/groups"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting Proxmox HA groups: {e}")
            return []
    
    def add_vm_to_proxmox_ha(self, vmid: int, vm_type: str = 'vm', group: str = None, 
                             max_restart: int = 1, max_relocate: int = 1, state: str = 'started',
                             comment: str = None) -> Dict:
        """Add VM/CT to Proxmox native HA
        
        Args:
            vmid: VM or Container ID
            vm_type: 'vm' or 'ct'
            group: HA group name (optional)
            max_restart: Max restart attempts (0-10)
            max_relocate: Max relocate attempts (0-10)
            state: Request state - 'started', 'stopped', 'ignored', 'disabled'
            comment: Optional comment/description
        """
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Not connected'}
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/cluster/ha/resources"
            
            # sid format: vm:100 or ct:100
            sid = f"{vm_type}:{vmid}"
            
            data = {
                'sid': sid,
                'max_restart': max_restart,
                'max_relocate': max_relocate,
                'state': state or 'started'
            }
            
            if group:
                data['group'] = group
            if comment:
                data['comment'] = comment
            
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                self.logger.info(f"[HA] Added {sid} to Proxmox HA")
                return {'success': True, 'message': f'{sid} added to HA'}
            else:
                error = response.text
                self.logger.error(f"[HA] Failed to add {sid} to HA: {error}")
                return {'success': False, 'error': error}
                
        except Exception as e:
            self.logger.error(f"Error adding to Proxmox HA: {e}")
            return {'success': False, 'error': str(e)}
    
    def remove_vm_from_proxmox_ha(self, vmid: int, vm_type: str = 'vm') -> Dict:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Not connected'}
        
        try:
            host = self.current_host or self.config.host
            sid = f"{vm_type}:{vmid}"
            url = f"https://{host}:8006/api2/json/cluster/ha/resources/{sid}"
            
            response = self._api_delete(url)
            
            if response.status_code == 200:
                self.logger.info(f"[HA] Removed {sid} from Proxmox HA")
                return {'success': True, 'message': f'{sid} removed from HA'}
            else:
                error = response.text
                self.logger.error(f"[HA] Failed to remove {sid} from HA: {error}")
                return {'success': False, 'error': error}
                
        except Exception as e:
            self.logger.error(f"Error removing from Proxmox HA: {e}")
            return {'success': False, 'error': str(e)}
    
    def _get_node_ip(self, node_name: str) -> Optional[str]:
        # get node IP
        try:
            # First try to get from Proxmox API
            if not self.is_connected:
                if not self.connect_to_proxmox():
                    return None
            
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes/{node_name}/network"
            response = self._api_get(url)
            
            if response.status_code == 200:
                networks = response.json()['data']
                
                # Get primary host IP parts for subnet matching
                primary_parts = self.config.host.split('.')
                
                candidate_ips = []
                
                for net in networks:
                    if net.get('type') in ['bridge', 'eth', 'bond', 'vmbr']:
                        addr = net.get('address')
                        if not addr:
                            continue
                        
                        # Skip localhost
                        if addr.startswith('127.'):
                            continue
                        
                        # check it's a private IP
                        is_private = (
                            addr.startswith('10.') or 
                            addr.startswith('192.168.') or 
                            addr.startswith('172.16.') or
                            addr.startswith('172.17.') or
                            addr.startswith('172.18.') or
                            addr.startswith('172.19.') or
                            addr.startswith('172.20.') or
                            addr.startswith('172.21.') or
                            addr.startswith('172.22.') or
                            addr.startswith('172.23.') or
                            addr.startswith('172.24.') or
                            addr.startswith('172.25.') or
                            addr.startswith('172.26.') or
                            addr.startswith('172.27.') or
                            addr.startswith('172.28.') or
                            addr.startswith('172.29.') or
                            addr.startswith('172.30.') or
                            addr.startswith('172.31.')
                        )
                        
                        # check same subnet as primary (first 3 octets)
                        addr_parts = addr.split('.')
                        same_subnet = (len(addr_parts) >= 3 and len(primary_parts) >= 3 and
                                      addr_parts[0] == primary_parts[0] and
                                      addr_parts[1] == primary_parts[1] and
                                      addr_parts[2] == primary_parts[2])
                        
                        # Priority: 1) Same subnet as primary, 2) Public IP, 3) Private IP
                        if same_subnet:
                            candidate_ips.insert(0, (addr, 'same_subnet'))
                        elif not is_private:
                            candidate_ips.append((addr, 'public'))
                        else:
                            candidate_ips.append((addr, 'private'))
                
                # Sort by priority: same_subnet > public > private
                priority = {'same_subnet': 0, 'public': 1, 'private': 2}
                candidate_ips.sort(key=lambda x: priority.get(x[1], 3))
                
                if candidate_ips:
                    best_ip = candidate_ips[0][0]
                    ip_type = candidate_ips[0][1]
                    self.logger.info(f"Found IP for {node_name}: {best_ip} ({ip_type})")
                    return best_ip
            
            # Fallback: if node_name is the cluster host, use that
            if node_name in self.config.host:
                return self.config.host
            
            # Try to resolve hostname
            try:
                ip = socket.gethostbyname(node_name)
                self.logger.info(f"Resolved {node_name} to {ip}")
                return ip
            except socket.gaierror:
                pass
            
            # Last resort: assume it's the same as cluster host if single node
            self.logger.warning(f"Could not determine IP for {node_name}, using cluster host")
            return self.config.host
            
        except Exception as e:
            self.logger.error(f"Error getting node IP: {e}")
            return None
    
    def _ssh_connect(self, host: str, retries: int = 3, retry_delay: float = 2.0):
        """SSH connect with retry logic and connection rate limiting
        
        NS: Jan 2026 - Limits concurrent CONNECTION ATTEMPTS (not active sessions).
        The semaphore is released after successful connection, allowing new
        connections while this one is active. This prevents connection storms
        while not blocking long-running operations.
        
        HA operations use separate methods without any rate limiting.
        """
        paramiko = get_paramiko()
        if not paramiko:
            self.logger.error("paramiko not installed, cannot use SSH features")
            return None
        
        # Get connection parameters
        username = self.config.ssh_user if self.config.ssh_user else self.config.user.split('@')[0]
        ssh_port = getattr(self.config, 'ssh_port', 22) or 22
        ssh_key = getattr(self.config, 'ssh_key', '')
        
        for attempt in range(1, retries + 1):
            # Rate limit connection ATTEMPTS (not active connections)
            stats = get_ssh_connection_stats()
            self.logger.debug(f"SSH queue: {stats['active_normal']} connecting, {stats['active_ha']} HA active")
            
            acquired = _ssh_semaphore.acquire(timeout=120)
            if not acquired:
                self.logger.error(f"SSH to {host} queued too long (120s) - increase PEGAPROX_SSH_MAX_CONCURRENT")
                return None
            
            _ssh_track_connection('normal', +1)
            
            try:
                if attempt > 1:
                    self.logger.info(f"SSH retry {attempt}/{retries} to {host}...")
                else:
                    self.logger.info(f"Connecting to {host}:{ssh_port} as {username}...")
                
                ssh = paramiko.SSHClient()
                ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
                
                connect_kwargs = {
                    'hostname': host,
                    'port': ssh_port,
                    'username': username,
                    'timeout': 30,
                    'banner_timeout': 30,
                    'allow_agent': False,
                    'look_for_keys': False
                }
                
                if ssh_key:
                    import io
                    key_file = io.StringIO(ssh_key)
                    pkey = None
                    for key_name, key_class in [
                        ('RSA', paramiko.RSAKey),
                        ('Ed25519', paramiko.Ed25519Key),
                        ('ECDSA', paramiko.ECDSAKey),
                        ('DSA', paramiko.DSSKey)
                    ]:
                        try:
                            key_file.seek(0)
                            pkey = key_class.from_private_key(key_file)
                            break
                        except:
                            continue
                    
                    if not pkey:
                        self.logger.error("Could not load SSH key - unsupported format")
                        return None
                    
                    connect_kwargs['pkey'] = pkey
                else:
                    connect_kwargs['password'] = self.config.pass_
                
                ssh.connect(**connect_kwargs)
                self.logger.info(f"SSH connected to {host}" + (f" (attempt {attempt})" if attempt > 1 else ""))
                
                # SUCCESS - release semaphore immediately, connection is established
                # This allows new connections while this session runs
                return ssh
                
            except paramiko.ssh_exception.AuthenticationException as e:
                self.logger.error(f"SSH auth failed for {username}@{host}: {e}")
                return None
                
            except (paramiko.ssh_exception.NoValidConnectionsError, socket.timeout, TimeoutError) as e:
                if attempt < retries:
                    delay = retry_delay * (2 ** (attempt - 1))
                    self.logger.warning(f"SSH to {host} failed (attempt {attempt}), retry in {delay}s...")
                    time.sleep(delay)
                    continue
                self.logger.error(f"SSH to {host} failed after {retries} attempts: {e}")
                return None
                
            except Exception as e:
                if attempt < retries:
                    delay = retry_delay * (2 ** (attempt - 1))
                    self.logger.warning(f"SSH error (attempt {attempt}), retry in {delay}s: {e}")
                    time.sleep(delay)
                    continue
                self.logger.error(f"SSH to {host} failed: {e}")
                return None
                
            finally:
                # Always release semaphore after attempt (success or failure)
                _ssh_track_connection('normal', -1)
                _ssh_semaphore.release()
        
        return None
    
    def _ssh_execute(self, ssh, command: str, task: UpdateTask = None) -> tuple:
        
        try:
            self.logger.info(f"Executing: {command}")
            
            stdin, stdout, stderr = ssh.exec_command(command, get_pty=True)
            
            output_lines = []
            
            # Read output line by line
            for line in iter(stdout.readline, ''):
                line = line.strip()
                if line:
                    output_lines.append(line)
                    if task:
                        task.add_output(line)
                    self.logger.debug(f"SSH: {line}")
            
            exit_code = stdout.channel.recv_exit_status()
            
            # Also capture any stderr
            stderr_output = stderr.read().decode('utf-8').strip()
            if stderr_output and task:
                for line in stderr_output.split('\n'):
                    if line.strip():
                        task.add_output(f"[stderr] {line.strip()}")
            
            return exit_code, '\n'.join(output_lines), stderr_output
            
        except Exception as e:
            self.logger.error(f"SSH execute error: {e}")
            return -1, '', str(e)
    
    def _wait_for_node_online(self, node_name: str, timeout: int = 600) -> bool:
        
        self.logger.info(f"Waiting for {node_name} to come back online (timeout: {timeout}s)...")
        start_time = time.time()
        
        # First wait a bit for the node to actually go down
        # 30s seems to work, less and you get false positives
        time.sleep(30)
        
        while time.time() - start_time < timeout:
            try:
                # Try to get node status from Proxmox API
                # Force reconnect in case session expired
                self.session = None  # force new session
                if self.connect_to_proxmox():
                    node_status = self.get_node_status()
                    if node_name in node_status:
                        if node_status[node_name]['status'] == 'online':
                            self.logger.info(f"[OK] {node_name} is back online!")
                            return True
                
            except Exception as e:
                self.logger.debug(f"Waiting for {node_name}: {e}")
            
            time.sleep(10)
        
        self.logger.error(f"Timeout waiting for {node_name} to come online")
        return False
    
    def start_node_update(self, node_name: str, reboot: bool = True, force: bool = False) -> Optional[UpdateTask]:
        
        # check node is in maintenance mode (unless force)
        if not force:
            if node_name not in self.nodes_in_maintenance:
                self.logger.error(f"Cannot update {node_name}: not in maintenance mode")
                return None
            
            maintenance_task = self.nodes_in_maintenance[node_name]
            if maintenance_task.status not in ['completed', 'completed_with_errors']:
                self.logger.error(f"Cannot update {node_name}: evacuation not complete")
                return None
        else:
            self.logger.warning(f"Force-updating {node_name} without maintenance mode!")
        
        # check already updating
        with self.update_lock:
            if node_name in self.nodes_updating:
                return self.nodes_updating[node_name]
            
            task = UpdateTask(node_name, reboot)
            self.nodes_updating[node_name] = task
        
        self.logger.info(f"[SYNC] Starting update for node: {node_name} (reboot: {reboot}, force: {force})")
        
        # Start update in background - use gevent if available for paramiko compatibility
        if GEVENT_PATCHED:
            try:
                import gevent
                gevent.spawn(self._perform_node_update, node_name, task)
                self.logger.info(f"[SYNC] Update spawned with gevent greenlet")
            except Exception as e:
                self.logger.warning(f"Gevent spawn failed, falling back to thread: {e}")
                update_thread = threading.Thread(
                    target=self._perform_node_update,
                    args=(node_name, task)
                )
                update_thread.daemon = True
                update_thread.start()
        else:
            update_thread = threading.Thread(
                target=self._perform_node_update,
                args=(node_name, task)
            )
            update_thread.daemon = True
            update_thread.start()
        
        return task
    
    def _perform_node_update(self, node_name: str, task: UpdateTask):
        
        ssh = None
        
        try:
            task.status = 'updating'
            task.phase = 'connecting'
            task.add_output(f"Connecting to / Verbinde zu {node_name}...")
            
            # Get node IP
            node_ip = self._get_node_ip(node_name)
            if not node_ip:
                raise Exception(f"Could not determine IP for {node_name}")
            
            task.add_output(f"Node IP: {node_ip}")
            
            # Connect via SSH
            ssh = self._ssh_connect(node_ip)
            if not ssh:
                # Check why SSH failed - get username for hint
                if self.config.ssh_user:
                    username = self.config.ssh_user
                else:
                    username = self.config.user.split('@')[0]
                ssh_port = getattr(self.config, 'ssh_port', 22) or 22
                
                if self.config.ssh_key:
                    raise Exception(f"SSH connection failed to {username}@{node_ip}:{ssh_port} - Check SSH key configuration / SSH Verbindung fehlgeschlagen - Prüfe SSH Key Konfiguration")
                else:
                    raise Exception(f"SSH connection failed to {username}@{node_ip}:{ssh_port} - Check if password auth is enabled on the node or configure an SSH key / SSH Verbindung fehlgeschlagen - Prüfe ob Passwort-Auth auf dem Node aktiviert ist oder konfiguriere einen SSH Key")
            
            task.add_output("SSH connection established / SSH Verbindung hergestellt")
            
            # Check if we're root (common on Proxmox) - if so, no sudo needed
            stdin, stdout, stderr = ssh.exec_command('id -u')
            uid = stdout.read().decode().strip()
            sudo_prefix = '' if uid == '0' else 'sudo '
            
            if uid == '0':
                task.add_output("Running as root - no sudo needed")
            else:
                task.add_output(f"Running as uid {uid} - using sudo")
            
            # Phase 1: apt update
            task.phase = 'apt_update'
            task.add_output("Running apt update...")
            
            exit_code, output, stderr = self._ssh_execute(
                ssh, 
                f'{sudo_prefix}DEBIAN_FRONTEND=noninteractive apt-get update',
                task
            )
            
            if exit_code != 0:
                raise Exception(f"apt update failed / fehlgeschlagen: {stderr}")
            
            task.add_output("[OK] apt update successful / erfolgreich")
            
            # Phase 2: apt dist-upgrade
            task.phase = 'apt_upgrade'
            task.add_output("Running apt dist-upgrade...")
            
            exit_code, output, stderr = self._ssh_execute(
                ssh,
                f'{sudo_prefix}DEBIAN_FRONTEND=noninteractive apt-get dist-upgrade -y -o Dpkg::Options::="--force-confdef" -o Dpkg::Options::="--force-confold"',
                task
            )
            
            if exit_code != 0:
                raise Exception(f"apt dist-upgrade failed / fehlgeschlagen: {stderr}")
            
            # Count upgraded packages (rough estimate from output)
            if 'upgraded' in output.lower():
                for line in output.split('\n'):
                    if 'upgraded' in line.lower() and 'newly installed' in line.lower():
                        parts = line.split()
                        try:
                            task.packages_upgraded = int(parts[0])
                        except:
                            pass
            
            task.add_output(f"[OK] apt dist-upgrade successful / erfolgreich ({task.packages_upgraded} packages / Pakete)")
            
            # Phase 3: Cleanup
            task.add_output("Cleaning up / Aufräumen...")
            self._ssh_execute(ssh, f'{sudo_prefix}apt-get autoremove -y', task)
            self._ssh_execute(ssh, f'{sudo_prefix}apt-get autoclean', task)
            
            # Close SSH before reboot
            ssh.close()
            ssh = None
            
            # Phase 4: Reboot if requested
            if task.reboot:
                task.phase = 'reboot'
                task.status = 'rebooting'
                task.add_output("[SYNC] Initiating reboot / Starte Neustart...")
                
                # Reconnect for reboot command
                ssh = self._ssh_connect(node_ip)
                if ssh:
                    try:
                        # Check if root
                        stdin, stdout, stderr = ssh.exec_command('id -u')
                        uid = stdout.read().decode().strip()
                        is_root = (uid == '0')
                        
                        self.logger.info(f"Sending reboot command to {node_name} (root={is_root})")
                        task.add_output(f"Running as {'root' if is_root else 'non-root user'}")
                        
                        # Get transport and open channel with PTY for sudo support
                        transport = ssh.get_transport()
                        channel = transport.open_session()
                        channel.get_pty()
                        channel.settimeout(10)
                        
                        # Execute reboot command
                        if is_root:
                            channel.exec_command('shutdown -r now')
                        else:
                            channel.exec_command('sudo shutdown -r now')
                        
                        # Wait briefly for command to be sent
                        time.sleep(3)
                        
                        # Try to read any output (will fail when connection drops, that's ok)
                        try:
                            output = channel.recv(1024).decode()
                            if output:
                                task.add_output(f"Reboot output: {output.strip()}")
                        except:
                            pass
                        
                        channel.close()
                        task.add_output("Reboot command sent / Reboot-Befehl gesendet")
                        
                    except Exception as e:
                        self.logger.info(f"Reboot command sent (connection closed as expected): {e}")
                        task.add_output("Reboot command sent / Reboot-Befehl gesendet")
                    finally:
                        try:
                            ssh.close()
                        except:
                            pass
                        ssh = None
                else:
                    task.add_output("[WARN] Could not reconnect for reboot / Konnte nicht für Reboot verbinden")
                    task.add_output("Trying alternative reboot method / Versuche alternative Methode...")
                    
                    # Try via Proxmox API as fallback
                    try:
                        # Proxmox has a reboot command via API
                        url = f"https://{self.config.host}:8006/api2/json/nodes/{node_name}/status"
                        response = self.session.post(url, data={'command': 'reboot'}, verify=False)
                        if response.status_code == 200:
                            task.add_output("Reboot initiated via Proxmox API")
                        else:
                            task.add_output(f"API reboot failed: {response.status_code}")
                    except Exception as api_e:
                        task.add_output(f"API reboot also failed: {api_e}")
                
                task.add_output("Waiting for node to reboot / Warte auf Neustart...")
                
                # Wait for node to come back online
                task.phase = 'wait_online'
                task.status = 'waiting_online'
                
                if self._wait_for_node_online(node_name):
                    task.add_output(f"[OK] {node_name} is back online / ist wieder online!")
                else:
                    task.add_output(f"[WARN] Timeout waiting for / beim Warten auf {node_name}")
                    task.error = "Node did not come back online in time"
            
            # Done!
            task.status = 'completed'
            task.phase = 'done'
            task.completed_at = datetime.now()
            task.add_output(f"[OK] Update completed / abgeschlossen!")
            
            # Auto-exit maintenance mode after successful update
            if node_name in self.nodes_in_maintenance:
                task.add_output(f"Exiting maintenance mode / Beende Wartungsmodus...")
                try:
                    self.exit_maintenance_mode(node_name)
                    task.add_output(f"[OK] Maintenance mode exited / Wartungsmodus beendet")
                except Exception as e:
                    task.add_output(f"[WARN] Could not exit maintenance mode / Konnte Wartungsmodus nicht beenden: {e}")
            
            self.logger.info(f"[OK] Node update completed for {node_name}")
            
        except Exception as e:
            self.logger.error(f"[ERROR] Node update failed for {node_name}: {e}")
            task.status = 'failed'
            task.error = str(e)
            task.add_output(f"[ERROR] Error / Fehler: {e}")
        
        finally:
            if ssh:
                try:
                    ssh.close()
                except:
                    pass
    
    def get_update_status(self, node_name: str) -> Optional[Dict]:
        
        with self.update_lock:
            if node_name in self.nodes_updating:
                return self.nodes_updating[node_name].to_dict()
            return None
    
    def clear_update_status(self, node_name: str) -> bool:
        
        with self.update_lock:
            if node_name in self.nodes_updating:
                task = self.nodes_updating[node_name]
                if task.status in ['completed', 'failed']:
                    del self.nodes_updating[node_name]
                    return True
            return False
    
    # VM Control Methods
    def vm_action(self, node: str, vmid: int, vm_type: str, action: str, force: bool = False) -> Dict[str, Any]:
        # NS: start/stop/shutdown/reboot/reset - basic VM lifecycle stuff
        if not self.is_connected and not self.connect_to_proxmox():
            return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        # LXC containers don't support 'reset' - only QEMU VMs do
        if vm_type == 'lxc' and action == 'reset':
            return {'success': False, 'error': 'Reset is not supported for LXC containers. Use reboot instead.'}
        
        try:
            host = self.current_host or self.config.host
            endpoint = 'qemu' if vm_type == 'qemu' else 'lxc'
            url = f"https://{host}:8006/api2/json/nodes/{node}/{endpoint}/{vmid}/status/{action}"
            
            data = {}
            
            # MK: Force stop handling - different for QEMU vs LXC
            # Fixed 27.01.2026 - skiplock requires root@pam, removed for non-root users
            if force and action == 'stop':
                self.logger.info(f"Force stopping {vm_type}/{vmid}")
                if vm_type == 'qemu':
                    # QEMU: Use timeout=0 for immediate stop (more compatible than forceStop)
                    data['timeout'] = 0
                # LW: skiplock only works for root@pam - causes error for other users
                # "Only root may use this option" - so we skip it for non-root
                if self.config.user.lower().startswith('root@'):
                    data['skiplock'] = 1
            
            self.logger.info(f"VM Action: {action} on {vm_type}/{vmid}@{node}" + (" FORCE" if force else ""))
            resp = self._api_post(url, data=data if data else None)
            
            if resp.status_code == 200:
                self.logger.info(f"[OK] {action} on {vmid}")
                return {'success': True, 'data': resp.json().get('data')}
            else:
                self.logger.error(f"[ERROR] {resp.text}")
                return {'success': False, 'error': resp.text}
                
        except Exception as e:
            self.logger.error(f"[ERROR] vm_action: {e}")
            return {'success': False, 'error': str(e)}
    
    def clone_vm(self, node: str, vmid: int, vm_type: str, newid: int, name: str = None, 
                 full: bool = True, target_node: str = None, target_storage: str = None,
                 description: str = None) -> Dict[str, Any]:
        """clone a vm"""
        # MK: full clone = independent copy, linked clone = shares base with original
        # linked clones only work for qemu
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/clone"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/clone"
            
            data = {'newid': newid}
            
            if name:
                data['name'] = name
            
            if full:
                data['full'] = 1
            else:
                # linked clone
                if vm_type == 'qemu':
                    data['full'] = 0
                    
            # Optional target location
            if target_node:
                data['target'] = target_node
            if target_storage:
                data['storage'] = target_storage
            if description:
                data['description'] = description
            
            self.logger.info(f"Cloning {vm_type}/{vmid} to {newid} (full={full})")
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                result = response.json()
                self.logger.info(f"[OK] Clone started: {vmid} -> {newid}")
                return {'success': True, 'data': result.get('data')}
            else:
                error_msg = response.text
                self.logger.error(f"[ERROR] Clone failed: {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Clone error: {e}")
            return {'success': False, 'error': str(e)}
    
    def get_next_vmid(self) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/cluster/nextid"
            response = self._api_get(url)
            
            if response.status_code == 200:
                data = response.json()
                return {'success': True, 'vmid': int(data.get('data', 100))}
            else:
                return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def migrate_vm_manual(self, node: str, vmid: int, vm_type: str, target_node: str, online: bool = True, options: Dict = None) -> Dict[str, Any]:
        # manual migrate
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        if options is None:
            options = {}
        
        try:
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/migrate"
                data = {
                    'target': target_node,
                    'online': 1 if online else 0,
                }
                # Add target storage if specified
                if options.get('targetstorage'):
                    data['targetstorage'] = options['targetstorage']
                # Add with-local-disks option
                if options.get('with_local_disks'):
                    data['with-local-disks'] = 1
            else:  # lxc
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/migrate"
                data = {
                    'target': target_node,
                    'restart': 1 if online else 0,  # LXC uses restart instead of online
                }
                # Add target storage if specified
                if options.get('targetstorage'):
                    data['target-storage'] = options['targetstorage']
                # Force (for conntrack state)
                if options.get('force'):
                    data['force'] = 1
            
            self.logger.info(f"Migrating {vm_type}/{vmid} from {node} to {target_node}" + 
                           (f" (storage: {options.get('targetstorage')})" if options.get('targetstorage') else ""))
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                task_data = response.json()
                self.logger.info(f"[OK] Migration started for {vmid}, task: {task_data.get('data')}")
                return {'success': True, 'task': task_data.get('data')}
            else:
                error_msg = response.text
                self.logger.error(f"[ERROR] Migration failed: {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Migration error: {e}")
            return {'success': False, 'error': str(e)}
    
    def remote_migrate_vm(self, node: str, vmid: int, vm_type: str, 
                          target_endpoint: str, target_storage: str, target_bridge: str,
                          target_vmid: int = None, online: bool = True, 
                          delete_source: bool = True, bwlimit: int = None) -> Dict[str, Any]:
        """
        Cross-cluster migration using remote-migrate
        
        Args:
            node: Source node
            vmid: VM ID
            vm_type: 'qemu' or 'lxc'
            target_endpoint: Target cluster API endpoint string including apitoken
                Format: apitoken=PVEAPIToken=<user>!<token>=<secret>,host=<host>,fingerprint=<fp>
            target_storage: Target storage (e.g., "local-lvm" or "local-lvm:target-lvm" for mapping)
            target_bridge: Target bridge (e.g., "vmbr0" or "vmbr0:vmbr1" for mapping)
            target_vmid: Optional different VMID on target
            online: Live migration (VM stays running)
            delete_source: Delete source VM after successful migration
            bwlimit: Bandwidth limit in KiB/s
        """
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            if vm_type == 'qemu':
                url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/remote_migrate"
            else:  # lxc
                url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/remote_migrate"
            
            # target-vmid is required - use same as source if not specified
            actual_target_vmid = target_vmid if target_vmid else vmid
            
            data = {
                'target-endpoint': target_endpoint,
                'target-vmid': actual_target_vmid,
                'target-storage': target_storage,
                'target-bridge': target_bridge,
            }
            
            if vm_type == 'qemu':
                data['online'] = 1 if online else 0
            
            if delete_source:
                data['delete'] = 1
            
            if bwlimit:
                data['bwlimit'] = bwlimit
            
            self.logger.info(f"Remote migrating {vm_type}/{vmid} from {node} to target cluster (target vmid: {actual_target_vmid})")
            self.logger.debug(f"Migration data: {data}")
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                task_data = response.json()
                self.logger.info(f"[OK] Remote migration started for {vmid}, task: {task_data.get('data')}")
                return {'success': True, 'task': task_data.get('data')}
            else:
                error_msg = response.text
                self.logger.error(f"[ERROR] Remote migration failed: {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Remote migration error: {e}")
            return {'success': False, 'error': str(e)}
    
    def get_cluster_fingerprint(self) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            import ssl
            import socket
            import hashlib
            
            # Get SSL certificate
            context = ssl.create_default_context()
            context.check_hostname = False
            context.verify_mode = ssl.CERT_NONE
            
            with socket.create_connection((self.config.host, 8006)) as sock:
                with context.wrap_socket(sock, server_hostname=self.config.host) as ssock:
                    cert_der = ssock.getpeercert(binary_form=True)
                    fingerprint = hashlib.sha256(cert_der).hexdigest()
                    # Format as colon-separated
                    fingerprint_formatted = ':'.join(fingerprint[i:i+2] for i in range(0, len(fingerprint), 2))
            
            return {
                'success': True, 
                'fingerprint': fingerprint_formatted,
                'host': self.config.host,
                'port': 8006
            }
        except Exception as e:
            self.logger.error(f"Error getting fingerprint: {e}")
            return {'success': False, 'error': str(e)}

    def create_api_token(self, token_name: str = 'pegaprox-migrate') -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            user = self.config.user
            
            # Create token without privilege separation (privsep=0)
            url = f"https://{host}:8006/api2/json/access/users/{user}/token/{token_name}"
            data = {
                'privsep': 0,  # No privilege separation - token has same permissions as user
                'expire': 0,   # No expiration (we'll delete it manually)
                'comment': 'PegaProx temporary migration token'
            }
            
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                result = response.json()
                token_data = result.get('data', {})
                token_value = token_data.get('value', '')
                full_token_id = token_data.get('full-tokenid', f"{user}!{token_name}")
                
                self.logger.info(f"[OK] Created API token: {full_token_id}")
                return {
                    'success': True,
                    'token_id': full_token_id,
                    'token_value': token_value,
                    'token_name': token_name
                }
            else:
                error_msg = response.text
                self.logger.error(f"[ERROR] Failed to create API token: {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Error creating API token: {e}")
            return {'success': False, 'error': str(e)}

    def delete_api_token(self, token_name: str = 'pegaprox-migrate') -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            user = self.config.user
            
            url = f"https://{host}:8006/api2/json/access/users/{user}/token/{token_name}"
            response = self._api_delete(url)
            
            if response.status_code == 200:
                self.logger.info(f"[OK] Deleted API token: {user}!{token_name}")
                return {'success': True}
            else:
                error_msg = response.text
                self.logger.warning(f"[WARN] Failed to delete API token: {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Error deleting API token: {e}")
            return {'success': False, 'error': str(e)}

    def delete_vm(self, node: str, vmid: int, vm_type: str, purge: bool = False, destroy_unreferenced: bool = False) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            # First check if VM is running and stop it
            if vm_type == 'qemu':
                status_url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/status/current"
            else:
                status_url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/status/current"
            
            status_response = self._create_session().get(status_url, timeout=15)
            if status_response.status_code == 200:
                status = status_response.json()['data'].get('status')
                if status == 'running':
                    # Stop the VM first
                    self.logger.info(f"Stopping {vm_type}/{vmid} before deletion")
                    if vm_type == 'qemu':
                        stop_url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/status/stop"
                    else:
                        stop_url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/status/stop"
                    self._create_session().post(stop_url, timeout=15)
                    # Wait a bit for stop
                    import time
                    time.sleep(3)
            
            # Now delete
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}"
            
            params = {}
            if purge:
                params['purge'] = 1
            if destroy_unreferenced and vm_type == 'qemu':
                params['destroy-unreferenced-disks'] = 1
            
            self.logger.info(f"Deleting {vm_type}/{vmid} on {node} with params: {params}")
            response = self._create_session().delete(url, params=params)
            
            if response.status_code == 200:
                task_data = response.json()
                self.logger.info(f"[OK] {vm_type}/{vmid} deleted, task: {task_data.get('data')}")
                
                # MK: Cleanup - remove VM from balancing exclusion list
                try:
                    db = get_db()
                    cursor = db.conn.cursor()
                    cursor.execute(
                        'DELETE FROM balancing_excluded_vms WHERE cluster_id = ? AND vmid = ?',
                        (self.id, vmid)
                    )
                    if cursor.rowcount > 0:
                        self.logger.info(f"Removed VM {vmid} from balancing exclusion list")
                    db.conn.commit()
                except Exception as cleanup_err:
                    self.logger.warning(f"Failed to cleanup balancing exclusion for VM {vmid}: {cleanup_err}")
                
                return {'success': True, 'task': task_data.get('data')}
            else:
                error_msg = response.text
                self.logger.error(f"[ERROR] Deletion failed: {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Deletion error: {e}")
            return {'success': False, 'error': str(e)}

    # NOTE: get_next_vmid is defined earlier in this class (line ~3989)
    # Removed duplicate definition here - NS Jan 2026
    
    def get_templates(self, node: str) -> List[Dict]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        templates = []
        try:
            # Get VM templates
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu"
            response = self._api_get(url)
            if response.status_code == 200:
                for vm in response.json()['data']:
                    if vm.get('template'):
                        templates.append({
                            'type': 'qemu',
                            'vmid': vm['vmid'],
                            'name': vm.get('name', f"VM {vm['vmid']}"),
                        })
            
            # Get CT templates from storage
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/storage"
            storage_response = self._api_get(url)
            if storage_response.status_code == 200:
                for storage in storage_response.json()['data']:
                    if 'vztmpl' in storage.get('content', ''):
                        tmpl_url = f"https://{self.host}:8006/api2/json/nodes/{node}/storage/{storage['storage']}/content"
                        tmpl_response = self._create_session().get(tmpl_url, params={'content': 'vztmpl'})
                        if tmpl_response.status_code == 200:
                            for tmpl in tmpl_response.json()['data']:
                                templates.append({
                                    'type': 'lxc',
                                    'volid': tmpl['volid'],
                                    'name': tmpl.get('volid', '').split('/')[-1],
                                })
        except Exception as e:
            self.logger.error(f"Error getting templates: {e}")
        
        return templates
    
    def create_vm(self, node: str, vm_config: Dict) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu"
            
            # Required fields
            vmid = vm_config.get('vmid')
            if not vmid:
                next_id = self.get_next_vmid()
                if not next_id['success']:
                    return next_id
                vmid = next_id['vmid']
            
            data = {
                'vmid': vmid,
                'name': vm_config.get('name', f'vm-{vmid}'),
                'memory': vm_config.get('memory', 2048),
                'cores': vm_config.get('cores', 2),
                'sockets': vm_config.get('sockets', 1),
            }
            
            # CPU
            if vm_config.get('cpu'):
                data['cpu'] = vm_config['cpu']
            
            # MK: CPU Affinity
            if vm_config.get('cpu_affinity'):
                data['affinity'] = vm_config['cpu_affinity']
            
            # MK: NUMA
            if vm_config.get('numa'):
                data['numa'] = '1'
            
            # MK: Advanced Memory - Ballooning
            if vm_config.get('min_memory'):
                data['balloon'] = vm_config['min_memory']
            elif vm_config.get('ballooning') is False:
                data['balloon'] = '0'  # Disable ballooning
            
            # MK: Memory Shares
            if vm_config.get('shares'):
                data['shares'] = vm_config['shares']
            
            # BIOS
            if vm_config.get('bios'):
                data['bios'] = vm_config['bios']
            
            # OS Type
            if vm_config.get('ostype'):
                data['ostype'] = vm_config['ostype']
            
            # Machine type - Proxmox expects format: type=<machine> or just pc/q35
            if vm_config.get('machine'):
                machine = vm_config['machine']
                # Proxmox 8+ expects the machine type directly without 'type=' prefix
                # Valid values: pc, q35, pc-i440fx-*, pc-q35-*, etc.
                if machine in ['i440fx', 'pc']:
                    data['machine'] = 'pc'
                elif machine == 'q35':
                    data['machine'] = 'q35'
                else:
                    data['machine'] = machine
            
            # Storage/Disk with enhanced options
            storage = vm_config.get('storage', 'local-lvm')
            disk_size = vm_config.get('disk_size', '32')
            # Remove G suffix if present - Proxmox API expects just the number
            disk_size = str(disk_size).replace('G', '').replace('g', '')
            
            disk_type = vm_config.get('disk_type', 'scsi')
            disk_format = vm_config.get('disk_format', '')  # raw, qcow2, vmdk
            scsi_hw = vm_config.get('scsihw', 'virtio-scsi-pci')
            
            # Build disk string - format depends on storage type
            # For LVM/ZFS: storage:size (e.g., local-lvm:32)
            # For directory: storage:size,format=X (e.g., local:32,format=qcow2)
            disk_str = f"{storage}:{disk_size}"
            
            # Add disk options
            disk_options = []
            
            # Add format if specified (needed for directory-based storage)
            if disk_format:
                disk_options.append(f"format={disk_format}")
            
            if vm_config.get('disk_cache'):
                disk_options.append(f"cache={vm_config['disk_cache']}")
            if vm_config.get('disk_discard'):
                disk_options.append("discard=on")
            if vm_config.get('disk_iothread') and disk_type == 'scsi':
                disk_options.append("iothread=1")
            if vm_config.get('disk_ssd'):
                disk_options.append("ssd=1")
            
            if disk_options:
                disk_str += "," + ",".join(disk_options)
            
            # Set disk based on type
            if disk_type == 'scsi':
                data['scsihw'] = scsi_hw
                data['scsi0'] = disk_str
            elif disk_type == 'virtio':
                data['virtio0'] = disk_str
            elif disk_type == 'ide':
                data['ide0'] = disk_str
            elif disk_type == 'sata':
                data['sata0'] = disk_str
            
            # MK: Additional Disks
            additional_disks = vm_config.get('additional_disks', [])
            disk_counters = {'scsi': 1, 'virtio': 1, 'sata': 1}  # Start at 1, 0 is primary
            for add_disk in additional_disks:
                add_type = add_disk.get('type', 'scsi')
                add_storage = add_disk.get('storage', storage)
                add_size = str(add_disk.get('size', '32')).replace('G', '').replace('g', '')
                add_disk_str = f"{add_storage}:{add_size}"
                
                # MK: Use per-disk options - each disk has its own format, cache, discard, iothread, ssd
                add_opts = []
                
                # Format: per-disk only (no fallback to global)
                add_format = add_disk.get('format', '')
                if add_format:
                    add_opts.append(f"format={add_format}")
                
                # Cache: per-disk only
                add_cache = add_disk.get('cache', '')
                if add_cache:
                    add_opts.append(f"cache={add_cache}")
                
                # Discard: per-disk (defaults to true in UI)
                if add_disk.get('discard', True):
                    add_opts.append("discard=on")
                
                # IO Thread for SCSI: per-disk (defaults to true in UI)
                if add_disk.get('iothread', True) and add_type == 'scsi':
                    add_opts.append("iothread=1")
                
                # SSD emulation: per-disk
                if add_disk.get('ssd', False):
                    add_opts.append("ssd=1")
                
                if add_opts:
                    add_disk_str += "," + ",".join(add_opts)
                
                disk_idx = disk_counters.get(add_type, 1)
                data[f"{add_type}{disk_idx}"] = add_disk_str
                disk_counters[add_type] = disk_idx + 1
                
                # MK: If this disk uses a different SCSI controller, note it (Proxmox only allows one scsihw though)
                # The per-disk scsihw is mostly for UI consistency; Proxmox uses one controller for all SCSI disks
            
            # EFI disk for UEFI
            if vm_config.get('bios') == 'ovmf':
                efi_storage = vm_config.get('efi_storage', storage)
                efi_type = "4m"
                if vm_config.get('efi_pre_enroll'):
                    data['efidisk0'] = f"{efi_storage}:1,efitype={efi_type},pre-enrolled-keys=1"
                else:
                    data['efidisk0'] = f"{efi_storage}:1,efitype={efi_type}"
                # UEFI requires q35 machine type
                if not vm_config.get('machine') or vm_config.get('machine') in ['i440fx', 'pc']:
                    data['machine'] = 'q35'
            
            # TPM
            if vm_config.get('tpm_storage'):
                tpm_version = vm_config.get('tpm_version', 'v2.0')
                data['tpmstate0'] = f"{vm_config['tpm_storage']}:1,version={tpm_version}"
            
            # Network
            net_model = vm_config.get('net_model', 'virtio')
            net_bridge = vm_config.get('net_bridge', 'vmbr0')
            net_str = f"{net_model},bridge={net_bridge}"
            if vm_config.get('net_firewall'):
                net_str += ",firewall=1"
            if vm_config.get('net_tag'):
                net_str += f",tag={vm_config['net_tag']}"
            # MK: MAC Address
            if vm_config.get('net_macaddr'):
                net_str += f",macaddr={vm_config['net_macaddr']}"
            # MK: MTU
            if vm_config.get('net_mtu'):
                net_str += f",mtu={vm_config['net_mtu']}"
            # MK: Rate Limit (MB/s -> Proxmox uses MB/s directly)
            if vm_config.get('net_rate'):
                net_str += f",rate={vm_config['net_rate']}"
            # MK: Disconnect (link_down)
            if vm_config.get('net_disconnect'):
                net_str += ",link_down=1"
            data['net0'] = net_str
            
            # CD-ROM / ISO
            boot_order = []
            if disk_type == 'scsi':
                boot_order.append('scsi0')
            elif disk_type == 'virtio':
                boot_order.append('virtio0')
            elif disk_type == 'ide':
                boot_order.append('ide0')
            elif disk_type == 'sata':
                boot_order.append('sata0')
            
            if vm_config.get('iso'):
                data['ide2'] = f"{vm_config['iso']},media=cdrom"
                boot_order.append('ide2')
            
            # VirtIO drivers ISO for Windows (secondary CD-ROM)
            if vm_config.get('virtio_iso'):
                data['ide3'] = f"{vm_config['virtio_iso']},media=cdrom"
            
            boot_order.append('net0')
            data['boot'] = 'order=' + ';'.join(boot_order)
            
            # VGA
            if vm_config.get('vga'):
                data['vga'] = vm_config['vga']
            
            # QEMU Agent
            if vm_config.get('agent'):
                data['agent'] = '1'
            
            # Start on boot
            if vm_config.get('onboot'):
                data['onboot'] = '1'
            
            # Start after creation
            start_after = vm_config.get('start', False)
            
            # MK: HA config (will be applied after VM creation)
            ha_enabled = vm_config.get('ha_enabled', False)
            ha_group = vm_config.get('ha_group', '')
            
            self.logger.info(f"Creating VM {vmid} on {node} with config: {data}")
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                task_data = response.json()
                self.logger.info(f"[OK] VM {vmid} created, task: {task_data.get('data')}")
                
                # MK: Add to HA if enabled
                if ha_enabled:
                    try:
                        ha_url = f"https://{self.host}:8006/api2/json/cluster/ha/resources"
                        ha_data = {'sid': f"vm:{vmid}"}
                        if ha_group:
                            ha_data['group'] = ha_group
                        ha_response = self._api_post(ha_url, data=ha_data)
                        if ha_response.status_code == 200:
                            self.logger.info(f"[OK] VM {vmid} added to HA")
                        else:
                            self.logger.warning(f"[WARN] Failed to add VM {vmid} to HA: {ha_response.text}")
                    except Exception as ha_err:
                        self.logger.warning(f"[WARN] HA config failed: {ha_err}")
                
                return {'success': True, 'vmid': vmid, 'task': task_data.get('data')}
            else:
                error_msg = response.text
                self.logger.error(f"[ERROR] VM creation failed: {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            self.logger.error(f"[ERROR] VM creation error: {e}")
            return {'success': False, 'error': str(e)}
    
    
    def create_container(self, node: str, ct_config: Dict) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc"
            
            # Required fields
            vmid = ct_config.get('vmid')
            if not vmid:
                next_id = self.get_next_vmid()
                if not next_id['success']:
                    return next_id
                vmid = next_id['vmid']
            
            # Template is required for CT creation
            template = ct_config.get('template')
            if not template:
                return {'success': False, 'error': 'Template is required for container creation'}
            
            # Build data payload
            # MK: defaults are conservative, users can change later
            data = {
                'vmid': vmid,
                'hostname': ct_config.get('hostname', ct_config.get('name', f'ct-{vmid}')),
                'ostemplate': template,
                'memory': ct_config.get('memory', 512),  # MB
                'swap': ct_config.get('swap', 512),
                'cores': ct_config.get('cores', 1),
                'password': ct_config.get('password', 'changeme'),  # should probably require this
            }
            
            # Root disk
            storage = ct_config.get('storage', 'local-lvm')
            disk_size = ct_config.get('disk_size', '8')  # GB
            data['rootfs'] = f"{storage}:{disk_size}"
            
            # MK: Additional Mount Points
            additional_disks = ct_config.get('additional_disks', [])
            for idx, mp in enumerate(additional_disks):
                mp_storage = mp.get('storage', storage)
                mp_size = str(mp.get('size', '8')).replace('G', '').replace('g', '')
                mp_path = mp.get('path', f'/mnt/data{idx}')
                # Format: storage:size,mp=/path
                data[f'mp{idx}'] = f"{mp_storage}:{mp_size},mp={mp_path}"
            
            # Network configuration
            # NS: this networking stuff is confusing, proxmox docs are not great
            net_bridge = ct_config.get('net_bridge', 'vmbr0')
            net_str = f"name=eth0,bridge={net_bridge}"
            
            # IPv4 configuration
            net_ip_type = ct_config.get('net_ip_type', 'dhcp')
            if net_ip_type == 'dhcp':
                net_str += ",ip=dhcp"
            elif net_ip_type == 'static':
                net_ip = ct_config.get('net_ip', '')
                if net_ip:
                    net_str += f",ip={net_ip}"
                    if ct_config.get('net_gw'):
                        net_str += f",gw={ct_config['net_gw']}"
            # 'manual' = no ip config
            
            # IPv6 configuration
            net_ip6_type = ct_config.get('net_ip6_type', 'dhcp')
            if net_ip6_type == 'dhcp':
                net_str += ",ip6=dhcp"
            elif net_ip6_type == 'slaac':
                net_str += ",ip6=auto"
            elif net_ip6_type == 'static':
                net_ip6 = ct_config.get('net_ip6', '')
                if net_ip6:
                    net_str += f",ip6={net_ip6}"
                    if ct_config.get('net_gw6'):
                        net_str += f",gw6={ct_config['net_gw6']}"
            # 'manual' = no ip6 config
            
            # VLAN tag
            if ct_config.get('net_tag'):
                net_str += f",tag={ct_config['net_tag']}"
            
            # Firewall
            if ct_config.get('net_firewall'):
                net_str += ",firewall=1"
            
            # Disconnected
            if ct_config.get('net_disconnected'):
                net_str += ",link_down=1"
            
            # MK: MAC Address
            if ct_config.get('net_macaddr'):
                net_str += f",hwaddr={ct_config['net_macaddr']}"
            
            # MK: MTU
            if ct_config.get('net_mtu'):
                net_str += f",mtu={ct_config['net_mtu']}"
            
            # MK: Rate Limit (MB/s)
            if ct_config.get('net_rate'):
                net_str += f",rate={ct_config['net_rate']}"
            
            data['net0'] = net_str
            
            # DNS settings
            if ct_config.get('dns_domain'):
                data['searchdomain'] = ct_config['dns_domain']
            if ct_config.get('dns_servers'):
                data['nameserver'] = ct_config['dns_servers']
            
            # Unprivileged
            if ct_config.get('unprivileged', True):
                data['unprivileged'] = '1'
            
            # Start on boot
            if ct_config.get('onboot'):
                data['onboot'] = '1'
            
            # SSH public keys
            ssh_keys = ct_config.get('ssh_public_keys') or ct_config.get('ssh_key')
            if ssh_keys:
                data['ssh-public-keys'] = ssh_keys
            
            # Features (nesting, etc.)
            features = []
            if ct_config.get('nesting'):
                features.append('nesting=1')
            if features:
                data['features'] = ','.join(features)
            
            # MK: HA config (will be applied after CT creation)
            ha_enabled = ct_config.get('ha_enabled', False)
            ha_group = ct_config.get('ha_group', '')
            
            self.logger.info(f"Creating container {vmid} on {node}")
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                task_data = response.json()
                self.logger.info(f"[OK] Container {vmid} created, task: {task_data.get('data')}")
                
                # MK: Add to HA if enabled
                if ha_enabled:
                    try:
                        ha_url = f"https://{self.host}:8006/api2/json/cluster/ha/resources"
                        ha_data = {'sid': f"ct:{vmid}"}
                        if ha_group:
                            ha_data['group'] = ha_group
                        ha_response = self._api_post(ha_url, data=ha_data)
                        if ha_response.status_code == 200:
                            self.logger.info(f"[OK] Container {vmid} added to HA")
                        else:
                            self.logger.warning(f"[WARN] Failed to add CT {vmid} to HA: {ha_response.text}")
                    except Exception as ha_err:
                        self.logger.warning(f"[WARN] HA config failed: {ha_err}")
                
                return {'success': True, 'vmid': vmid, 'task': task_data.get('data')}
            else:
                error_msg = response.text
                self.logger.error(f"[ERROR] Container creation failed: {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Container creation error: {e}")
            return {'success': False, 'error': str(e)}
    
    # ==================== SNAPSHOT METHODS ====================
    
    def get_snapshots(self, node: str, vmid: int, vm_type: str) -> List[Dict]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            host = self.current_host or self.config.host
            if vm_type == 'qemu':
                url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/snapshot"
            else:
                url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/snapshot"
            
            response = self._api_get(url)
            
            if response.status_code == 200:
                snapshots = response.json()['data']
                # Sort by snaptime, filter out 'current'
                return sorted(
                    [s for s in snapshots if s.get('name') != 'current'],
                    key=lambda x: x.get('snaptime', 0),
                    reverse=True
                )
            return []
        except Exception as e:
            self.logger.error(f"Error getting snapshots: {e}")
            return []
    
    def check_snapshot_capability(self, node: str, vmid: int, vm_type: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'can_snapshot': False, 'reason': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            
            # Get VM/CT config
            if vm_type == 'qemu':
                config_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            else:
                config_url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
            
            response = self._create_session().get(config_url, timeout=15)
            if response.status_code != 200:
                return {'can_snapshot': False, 'reason': 'Could not get VM configuration'}
            
            config = response.json().get('data', {})
            issues = []
            
            if vm_type == 'qemu':
                # Check for issues that prevent QEMU snapshots
                for key, value in config.items():
                    if key.startswith('scsi') or key.startswith('virtio') or key.startswith('ide') or key.startswith('sata'):
                        if isinstance(value, str):
                            # Check for raw format (no snapshots)
                            if ',format=raw' in value or value.endswith(':raw'):
                                issues.append(f"Disk {key} uses raw format (no snapshots)")
                            # Check for passthrough devices
                            if '/dev/' in value:
                                issues.append(f"Disk {key} is a passthrough device")
                            # Check for iscsi without snapshots
                            if 'iscsi:' in value.lower():
                                issues.append(f"Disk {key} is iSCSI (may not support snapshots)")
                    
                    # Check for PCI passthrough
                    if key.startswith('hostpci'):
                        issues.append(f"PCI passthrough device {key} prevents live snapshots with RAM")
                    
                    # Check for USB passthrough  
                    if key.startswith('usb') and '/dev/' in str(value):
                        issues.append(f"USB passthrough {key} may affect snapshots")
                
                # check EFI disk exists without proper storage
                if 'efidisk0' in config:
                    efi_disk = config['efidisk0']
                    if isinstance(efi_disk, str) and 'raw' in efi_disk.lower():
                        issues.append("EFI disk uses raw format")
            
            else:  # LXC container
                # Check for bind mounts
                for key, value in config.items():
                    if key.startswith('mp') and isinstance(value, str):
                        if ',bind' in value or 'bind=' in value:
                            issues.append(f"Mount point {key} is a bind mount (excluded from snapshots)")
                
                # Check rootfs
                rootfs = config.get('rootfs', '')
                if isinstance(rootfs, str):
                    if 'dir:' in rootfs.lower() or 'nfs:' in rootfs.lower():
                        issues.append("Container uses directory or NFS storage (limited snapshot support)")
            
            if issues:
                return {
                    'can_snapshot': True,  # Usually still possible, just with limitations
                    'warnings': issues,
                    'reason': '; '.join(issues)
                }
            
            return {'can_snapshot': True, 'warnings': [], 'reason': None}
            
        except Exception as e:
            self.logger.error(f"Error checking snapshot capability: {e}")
            return {'can_snapshot': False, 'reason': str(e)}
    
    def create_snapshot(self, node: str, vmid: int, vm_type: str, snapname: str, description: str = '', vmstate: bool = False) -> Dict[str, Any]:
        """create a snapshot"""
        # LW: this was surprisingly annoying to get right
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            if vm_type == 'qemu':
                url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/snapshot"
            else:
                url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/snapshot"
            
            data = {'snapname': snapname}
            if description:
                data['description'] = description
            if vmstate and vm_type == 'qemu':
                data['vmstate'] = 1  # include RAM
            
            # self.logger.info(f"creating snapshot {snapname}")  # DEBUG
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                task_data = response.json()
                return {'success': True, 'task': task_data.get('data')}
            else:
                # try to parse error msg
                error_text = response.text
                try:
                    error_json = response.json()
                    if 'errors' in error_json:
                        error_text = str(error_json['errors'])
                    elif 'data' in error_json:
                        error_text = str(error_json['data'])
                except:
                    pass
                
                # make error messages nicer
                if 'not supported' in error_text.lower():
                    error_text = 'Snapshots are not supported for this storage type'
                elif 'running' in error_text.lower() and 'vmstate' in error_text.lower():
                    error_text = 'Cannot create RAM snapshot: VM has PCI passthrough or other incompatible devices'
                elif 'lock' in error_text.lower():
                    error_text = 'VM is locked (another operation in progress)'
                
                return {'success': False, 'error': error_text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def delete_snapshot(self, node: str, vmid: int, vm_type: str, snapname: str) -> Dict[str, Any]:
        """delete snapshot"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/snapshot/{snapname}"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/snapshot/{snapname}"
            
            # self.logger.info(f"deleting snapshot {snapname}")  # spammy
            response = self._api_delete(url)
            
            if response.status_code == 200:
                task_data = response.json()
                return {'success': True, 'task': task_data.get('data')}
            else:
                return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def rollback_snapshot(self, node: str, vmid: int, vm_type: str, snapname: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/snapshot/{snapname}/rollback"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/snapshot/{snapname}/rollback"
            
            self.logger.info(f"Rolling back {vm_type}/{vmid} to snapshot '{snapname}'")
            response = self._create_session().post(url, timeout=15)
            
            if response.status_code == 200:
                task_data = response.json()
                return {'success': True, 'task': task_data.get('data')}
            else:
                return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    # ==================== REPLICATION METHODS ====================
    
    def get_replication_jobs(self, vmid: int = None) -> List[Dict]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/cluster/replication"
            response = self._api_get(url)
            
            if response.status_code == 200:
                jobs = response.json()['data']
                if vmid:
                    jobs = [j for j in jobs if j.get('guest') == vmid]
                return jobs
            return []
        except Exception as e:
            self.logger.error(f"Error getting replication jobs: {e}")
            return []
    
    def get_replication_status(self) -> List[Dict]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/cluster/replication"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json()['data']
            return []
        except Exception as e:
            self.logger.error(f"Error getting replication status: {e}")
            return []
    
    def create_replication_job(self, vmid: int, target_node: str, schedule: str = '*/15', rate: int = None, comment: str = '') -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            # Find the source node for this VM
            resources = self.get_cluster_resources()
            vm = next((r for r in resources if r.get('vmid') == vmid), None)
            if not vm:
                return {'success': False, 'error': f'VM {vmid} not found'}
            
            source_node = vm.get('node')
            vm_type = vm.get('type')
            
            # Create job ID
            job_id = f"{vmid}-0"  # Default to first replication job
            
            url = f"https://{self.host}:8006/api2/json/cluster/replication"
            
            data = {
                'id': job_id,
                'target': target_node,
                'type': 'local',
                'schedule': schedule,
            }
            
            if rate:
                data['rate'] = rate  # Rate limit in MB/s
            if comment:
                data['comment'] = comment
            
            self.logger.info(f"Creating replication job for {vmid} to {target_node}")
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                return {'success': True, 'job_id': job_id}
            else:
                return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def delete_replication_job(self, job_id: str, keep: bool = False, force: bool = False) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/cluster/replication/{job_id}"
            
            params = {}
            if keep:
                params['keep'] = 1  # Keep replicated data
            if force:
                params['force'] = 1
            
            self.logger.info(f"Deleting replication job {job_id}")
            response = self._create_session().delete(url, params=params)
            
            if response.status_code == 200:
                return {'success': True}
            else:
                return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def run_replication_now(self, job_id: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            # Extract vmid and job number from job_id (format: vmid-jobnumber)
            parts = job_id.split('-')
            vmid = parts[0]
            
            url = f"https://{self.host}:8006/api2/json/nodes/localhost/replication/{job_id}/schedule_now"
            
            self.logger.info(f"Triggering immediate replication for job {job_id}")
            response = self._create_session().post(url, timeout=15)
            
            if response.status_code == 200:
                return {'success': True}
            else:
                return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def get_vnc_ticket(self, node: str, vmid: int, vm_type: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            # Use the cluster host - Proxmox API will route to correct node
            host = self.current_host or self.config.host
            
            # Build URL based on VM type
            if vm_type == 'qemu':
                url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/vncproxy"
            else:  # lxc
                url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/vncproxy"
            
            self.logger.info(f"[VNC] Requesting ticket from {host} for {vm_type}/{vmid} on {node}")
            
            # Request websocket VNC with longer timeout
            session = self._create_session()
            response = session.post(url, data={'websocket': 1}, timeout=30)
            
            if response.status_code == 200:
                data = response.json()['data']
                ticket = data.get('ticket')
                port = data.get('port')
                self.logger.info(f"[VNC] Got ticket for {vm_type}/{vmid}, port={port}")
                
                return {
                    'success': True,
                    'ticket': ticket,
                    'port': port,
                    'host': host,
                    'node': node,
                    'vmid': vmid,
                    'vm_type': vm_type,
                    'pve_auth_cookie': self._ticket
                }
            else:
                error_msg = response.text
                self.logger.error(f"[VNC] Ticket failed ({response.status_code}): {error_msg}")
                return {'success': False, 'error': f"Proxmox error: {error_msg}"}
        
        except requests.exceptions.Timeout as e:
            self.logger.error(f"[VNC] Timeout connecting to Proxmox: {e}")
            return {'success': False, 'error': 'Proxmox connection timeout. Is the VM running?'}
        except requests.exceptions.ConnectionError as e:
            self.logger.error(f"[VNC] Connection error: {e}")
            return {'success': False, 'error': 'Cannot connect to Proxmox'}
        except Exception as e:
            self.logger.error(f"[VNC] Error: {e}")
            return {'success': False, 'error': str(e)}
    
    def get_node_shell_ticket(self, node: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/termproxy"
            
            # Request websocket terminal
            response = self._create_session().post(url, timeout=15)
            
            self.logger.info(f"Termproxy response status: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()['data']
                user = data.get('user') or self.config.user
                self.logger.info(f"[OK] Got shell ticket for node {node}, port={data.get('port')}, user={user}")
                return {
                    'success': True,
                    'ticket': data.get('ticket'),
                    'port': data.get('port'),
                    'user': user,
                    'host': self.config.host,
                    'node': node,
                    'pve_auth_cookie': self._ticket
                }
            else:
                error_msg = response.text
                self.logger.error(f"[ERROR] Shell ticket failed: {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Shell ticket error: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}
    
    def get_spice_ticket(self, node: str, vmid: int, vm_type: str) -> Dict[str, Any]:
        
        if vm_type != 'qemu':
            return {'success': False, 'error': 'SPICE only available for QEMU VMs'}
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/spiceproxy"
            response = self._create_session().post(url, timeout=15)
            
            if response.status_code == 200:
                data = response.json()['data']
                self.logger.info(f"[OK] Got SPICE ticket for {vmid}")
                return {'success': True, 'data': data}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_vm_config(self, node: str, vmid: int, vm_type: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
            
            response = self._api_get(url)
            
            if response.status_code == 200:
                config = response.json()['data']
                
                # Also get current status for some dynamic info
                if vm_type == 'qemu':
                    status_url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/status/current"
                else:
                    status_url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/status/current"
                
                status_response = self._create_session().get(status_url, timeout=15)
                status = {}
                if status_response.status_code == 200:
                    status = status_response.json()['data']
                
                # Parse config into structured format
                parsed = self._parse_vm_config(config, vm_type)
                parsed['raw'] = config
                parsed['status'] = status
                parsed['vmid'] = vmid
                parsed['node'] = node
                parsed['type'] = vm_type
                
                # MK: Add lock info - important for UI to show locked state
                lock_reason = config.get('lock')
                if lock_reason:
                    lock_descriptions = {
                        'migrate': 'Migration in progress',
                        'backup': 'Backup in progress', 
                        'snapshot': 'Snapshot operation in progress',
                        'rollback': 'Snapshot rollback in progress',
                        'clone': 'Clone operation in progress',
                        'create': 'VM creation in progress',
                        'disk': 'Disk operation in progress',
                        'suspended': 'VM suspended to disk',
                        'suspending': 'VM is being suspended',
                        'copy': 'Copy operation in progress',
                    }
                    parsed['lock'] = {
                        'locked': True,
                        'reason': lock_reason,
                        'description': lock_descriptions.get(lock_reason, f'Locked: {lock_reason}'),
                        'unlock_command': f"qm unlock {vmid}" if vm_type == 'qemu' else f"pct unlock {vmid}"
                    }
                else:
                    parsed['lock'] = {'locked': False}
                
                return {'success': True, 'config': parsed}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Get VM config error: {e}")
            return {'success': False, 'error': str(e)}
    
    def unlock_vm(self, node: str, vmid: int, vm_type: str) -> Dict[str, Any]:
        """Unlock a VM/CT by removing the lock from its config
        
        Lock reasons in Proxmox:
        - migrate: VM is being migrated
        - backup: VM is being backed up
        - snapshot: Snapshot operation in progress
        - rollback: Snapshot rollback in progress
        - clone: VM is being cloned
        - create: VM is being created
        - disk: Disk operation in progress
        - suspended: VM is suspended to disk
        
        Use with caution - unlocking during an active operation can cause issues!
        """
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            
            # First get current config to see lock reason
            if vm_type == 'qemu':
                config_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            else:
                config_url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
            
            config_response = self._api_get(config_url)
            if config_response.status_code != 200:
                return {'success': False, 'error': 'Could not get VM config'}
            
            config = config_response.json().get('data', {})
            lock_reason = config.get('lock')
            
            if not lock_reason:
                return {'success': True, 'message': 'VM is not locked', 'was_locked': False}
            
            self.logger.info(f"Unlocking {vm_type}/{vmid} on {node} (lock reason: {lock_reason})")
            
            # Remove the lock by setting delete=lock
            response = self._api_put(config_url, data={'delete': 'lock'})
            
            if response.status_code == 200:
                self.logger.info(f"[OK] Unlocked {vm_type}/{vmid} (was: {lock_reason})")
                return {
                    'success': True, 
                    'message': f'VM unlocked successfully',
                    'was_locked': True,
                    'lock_reason': lock_reason
                }
            else:
                error = response.text
                self.logger.error(f"[ERROR] Failed to unlock {vm_type}/{vmid}: {error}")
                return {'success': False, 'error': error}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Unlock VM error: {e}")
            return {'success': False, 'error': str(e)}
    
    def get_vm_lock_status(self, node: str, vmid: int, vm_type: str) -> Dict[str, Any]:
        """Get lock status of a VM/CT"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            
            if vm_type == 'qemu':
                config_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            else:
                config_url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
            
            response = self._api_get(config_url)
            
            if response.status_code == 200:
                config = response.json().get('data', {})
                lock_reason = config.get('lock')
                
                # Map lock reasons to human-readable descriptions
                lock_descriptions = {
                    'migrate': 'Migration in progress',
                    'backup': 'Backup in progress',
                    'snapshot': 'Snapshot operation in progress',
                    'rollback': 'Snapshot rollback in progress',
                    'clone': 'Clone operation in progress',
                    'create': 'VM creation in progress',
                    'disk': 'Disk operation in progress',
                    'suspended': 'VM suspended to disk',
                    'suspending': 'VM is being suspended',
                    'copy': 'Copy operation in progress',
                }
                
                return {
                    'success': True,
                    'locked': bool(lock_reason),
                    'lock_reason': lock_reason,
                    'lock_description': lock_descriptions.get(lock_reason, f'Locked: {lock_reason}') if lock_reason else None
                }
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_vm_rrd(self, node: str, vmid: int, vm_type: str, timeframe: str = 'day') -> Dict[str, Any]:
        """Get RRD metrics data for a VM or container
        
        Proxmox stores historical data in RRD format.
        Timeframes: hour, day, week, month, year
        """
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            
            if vm_type == 'qemu':
                url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/rrddata"
            else:
                url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/rrddata"
            
            response = self._create_session().get(url, params={'timeframe': timeframe})
            
            if response.status_code == 200:
                rrd_data = response.json().get('data', [])
                
                # Process and format the data for charts
                formatted_data = {
                    'timeframe': timeframe,
                    'vmid': vmid,
                    'node': node,
                    'type': vm_type,
                    'metrics': {
                        'cpu': [],
                        'memory': [],
                        'disk_read': [],
                        'disk_write': [],
                        'net_in': [],
                        'net_out': []
                    },
                    'timestamps': []
                }
                
                for point in rrd_data:
                    if not point:
                        continue
                    
                    timestamp = point.get('time', 0)
                    formatted_data['timestamps'].append(timestamp)
                    
                    # CPU usage (0-1 -> 0-100%)
                    cpu = point.get('cpu', 0)
                    formatted_data['metrics']['cpu'].append(round((cpu or 0) * 100, 2))
                    
                    # Memory usage (bytes)
                    mem = point.get('mem', 0)
                    maxmem = point.get('maxmem', 1)
                    mem_percent = ((mem or 0) / (maxmem or 1)) * 100
                    formatted_data['metrics']['memory'].append(round(mem_percent, 2))
                    
                    # Disk I/O (bytes/s)
                    formatted_data['metrics']['disk_read'].append(point.get('diskread', 0) or 0)
                    formatted_data['metrics']['disk_write'].append(point.get('diskwrite', 0) or 0)
                    
                    # Network I/O (bytes/s)
                    formatted_data['metrics']['net_in'].append(point.get('netin', 0) or 0)
                    formatted_data['metrics']['net_out'].append(point.get('netout', 0) or 0)
                
                return {'success': True, 'data': formatted_data}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Get VM RRD error: {e}")
            return {'success': False, 'error': str(e)}
    
    def _parse_vm_config(self, config: Dict, vm_type: str) -> Dict:
        
        parsed = {
            'general': {},
            'hardware': {},
            'disks': [],
            'networks': [],
            'options': {},
            'unused_disks': []  # MK: Track unused/detached disks
        }
        
        if vm_type == 'qemu':
            # General
            parsed['general'] = {
                'name': config.get('name', ''),
                'description': config.get('description', ''),
                'vmgenid': config.get('vmgenid', ''),
                'tags': config.get('tags', ''),
            }
            
            # Hardware
            parsed['hardware'] = {
                'cores': config.get('cores', 1),
                'sockets': config.get('sockets', 1),
                'cpu': config.get('cpu', 'host'),
                'memory': config.get('memory', 512),
                'balloon': config.get('balloon', 0),
                'numa': config.get('numa', 0),
                'vga': config.get('vga', 'std'),
                'machine': config.get('machine', ''),
                'bios': config.get('bios', 'seabios'),
                'scsihw': config.get('scsihw', 'virtio-scsi-pci'),
            }
            
            # Options
            parsed['options'] = {
                'onboot': config.get('onboot', 0),
                'boot': config.get('boot', ''),
                'bootdisk': config.get('bootdisk', ''),
                'ostype': config.get('ostype', 'other'),
                'agent': config.get('agent', '0'),
                'protection': config.get('protection', 0),
                'tablet': config.get('tablet', 1),
                'hotplug': config.get('hotplug', ''),
                'acpi': config.get('acpi', 1),
                'kvm': config.get('kvm', 1),
                'smbios1': config.get('smbios1', ''),
            }
            
            # Parse disks and networks from config keys
            for key, value in config.items():
                # Disks: scsi0, virtio0, ide0, sata0, etc.
                if any(key.startswith(prefix) for prefix in ['scsi', 'virtio', 'ide', 'sata', 'efidisk', 'tpmstate']):
                    if isinstance(value, str) and ':' in value:
                        parsed['disks'].append({
                            'id': key,
                            'value': value,
                            **self._parse_disk_string(value)
                        })
                
                # MK: Unused disks - these are detached but not deleted
                if key.startswith('unused'):
                    if isinstance(value, str):
                        # Parse unused disk: "local-lvm:vm-100-disk-1" or similar
                        parts = value.split(':')
                        storage = parts[0] if len(parts) > 0 else ''
                        volume = parts[1] if len(parts) > 1 else value
                        parsed['unused_disks'].append({
                            'id': key,
                            'value': value,
                            'storage': storage,
                            'volume': volume,
                        })
                
                # Networks: net0, net1, etc.
                if key.startswith('net'):
                    parsed['networks'].append({
                        'id': key,
                        'value': value,
                        **self._parse_network_string(value, 'qemu')
                    })
                
                # CD/DVD
                if key in ['cdrom', 'ide2'] and 'media=cdrom' in str(value):
                    parsed['hardware']['cdrom'] = value
        
        else:  # LXC
            # General
            parsed['general'] = {
                'hostname': config.get('hostname', ''),
                'description': config.get('description', ''),
                'tags': config.get('tags', ''),
                'ostype': config.get('ostype', ''),
                'arch': config.get('arch', 'amd64'),
            }
            
            # Hardware/Resources
            parsed['hardware'] = {
                'cores': config.get('cores', 1),
                'cpulimit': config.get('cpulimit', 0),
                'cpuunits': config.get('cpuunits', 1024),
                'memory': config.get('memory', 512),
                'swap': config.get('swap', 512),
            }
            
            # Options
            parsed['options'] = {
                'onboot': config.get('onboot', 0),
                'protection': config.get('protection', 0),
                'unprivileged': config.get('unprivileged', 0),
                'features': config.get('features', ''),
                'startup': config.get('startup', ''),
                'nameserver': config.get('nameserver', ''),
                'searchdomain': config.get('searchdomain', ''),
            }
            
            # Parse storage and networks
            for key, value in config.items():
                # Storage: rootfs, mp0, mp1, etc.
                if key == 'rootfs' or key.startswith('mp'):
                    parsed['disks'].append({
                        'id': key,
                        'value': value,
                        **self._parse_lxc_storage_string(value)
                    })
                
                # MK: Unused disks for LXC too
                if key.startswith('unused'):
                    if isinstance(value, str):
                        parts = value.split(':')
                        storage = parts[0] if len(parts) > 0 else ''
                        volume = parts[1] if len(parts) > 1 else value
                        parsed['unused_disks'].append({
                            'id': key,
                            'value': value,
                            'storage': storage,
                            'volume': volume,
                        })
                
                # Networks: net0, net1, etc.
                if key.startswith('net'):
                    parsed['networks'].append({
                        'id': key,
                        'value': value,
                        **self._parse_network_string(value, 'lxc')
                    })
        
        return parsed
    
    def _parse_disk_string(self, disk_str: str) -> Dict:
        """Parse QEMU disk string like 'local-lvm:vm-100-disk-0,size=32G'"""
        result = {'storage': '', 'size': '', 'format': '', 'cache': '', 'iothread': 0, 'ssd': 0}
        parts = disk_str.split(',')
        
        if parts:
            # First part is storage:volume
            if ':' in parts[0]:
                storage_parts = parts[0].split(':')
                result['storage'] = storage_parts[0]
                result['volume'] = storage_parts[1] if len(storage_parts) > 1 else ''
        
        for part in parts[1:]:
            if '=' in part:
                key, value = part.split('=', 1)
                if key == 'size':
                    result['size'] = value
                elif key == 'cache':
                    result['cache'] = value
                elif key == 'format':
                    result['format'] = value
                elif key == 'iothread':
                    result['iothread'] = int(value)
                elif key == 'ssd':
                    result['ssd'] = int(value)
        
        return result
    
    def _parse_lxc_storage_string(self, storage_str: str) -> Dict:
        """Parse LXC storage string like 'local-lvm:vm-100-disk-0,size=8G'"""
        result = {'storage': '', 'size': '', 'mountpoint': ''}
        parts = storage_str.split(',')
        
        if parts:
            if ':' in parts[0]:
                storage_parts = parts[0].split(':')
                result['storage'] = storage_parts[0]
                result['volume'] = storage_parts[1] if len(storage_parts) > 1 else ''
        
        for part in parts[1:]:
            if '=' in part:
                key, value = part.split('=', 1)
                if key == 'size':
                    result['size'] = value
                elif key == 'mp':
                    result['mountpoint'] = value
        
        return result
    
    def _parse_network_string(self, net_str: str, vm_type: str) -> Dict:
        """Parse Proxmox network config string into a dict
        
        MK: Format varies between QEMU and LXC
        QEMU: virtio=AA:BB:CC:DD:EE:FF,bridge=vmbr0,firewall=1,link_down=1,queues=4
        LXC: name=eth0,bridge=vmbr0,ip=dhcp,hwaddr=AA:BB:CC:DD:EE:FF
        """
        result = {
            'bridge': '',
            'firewall': 0,
            'tag': '',
            'rate': '',
            'mtu': '',
            'queues': '',  # LW: multiqueue support
            'link_down': False,  # NS: track network disconnect state
        }
        
        if vm_type == 'qemu':
            result.update({'model': 'virtio', 'macaddr': ''})
        else:  # lxc
            result.update({'name': '', 'hwaddr': '', 'ip': '', 'gw': '', 'ip6': '', 'gw6': ''})
        
        parts = net_str.split(',')
        
        for i, part in enumerate(parts):
            if '=' in part:
                key, value = part.split('=', 1)
                key_lower = key.lower()
                
                # First part for QEMU: model=MAC (e.g., e1000=AA:BB:CC:DD:EE:FF)
                if i == 0 and vm_type == 'qemu' and ':' in value:
                    # This is model=MAC format
                    result['model'] = key
                    result['macaddr'] = value
                elif key == 'bridge':
                    result['bridge'] = value
                elif key == 'firewall':
                    result['firewall'] = int(value)
                elif key == 'tag':
                    result['tag'] = value
                elif key == 'rate':
                    result['rate'] = value
                elif key == 'mtu':
                    result['mtu'] = value
                elif key == 'queues':
                    result['queues'] = value
                elif key == 'model':
                    result['model'] = value
                elif key == 'macaddr' or key == 'hwaddr':
                    result['macaddr' if vm_type == 'qemu' else 'hwaddr'] = value
                elif key == 'name':
                    result['name'] = value
                elif key == 'ip':
                    result['ip'] = value
                elif key == 'gw':
                    result['gw'] = value
                elif key == 'ip6':
                    result['ip6'] = value
                elif key == 'gw6':
                    result['gw6'] = value
                elif key == 'link_down':
                    # LW: link_down=1 means cable is "unplugged"
                    result['link_down'] = value == '1'
        
        return result
    
    def update_vm_config(self, node: str, vmid: int, vm_type: str, config_updates: Dict) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
            
            self.logger.info(f"Updating {vm_type}/{vmid} config: {config_updates}")
            
            response = self._api_put(url, data=config_updates)
            
            if response.status_code == 200:
                self.logger.info(f"[OK] Config updated for {vm_type}/{vmid}")
                return {'success': True, 'message': 'Configuration updated'}
            else:
                error_msg = response.text
                self.logger.error(f"[ERROR] Config update failed: {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Update config error: {e}")
            return {'success': False, 'error': str(e)}
    
    def resize_vm_disk(self, node: str, vmid: int, vm_type: str, disk: str, size: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/resize"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/resize"
            
            data = {'disk': disk, 'size': size}
            response = self._api_put(url, data=data)
            
            if response.status_code == 200:
                self.logger.info(f"[OK] Disk {disk} resized to {size}")
                return {'success': True, 'message': f'Disk resized to {size}'}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_storage_list(self, node: str) -> List[Dict]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes/{node}/storage"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json()['data']
            return []
        except:
            return []
    
    def get_network_list(self, node: str) -> List[Dict]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes/{node}/network"
            response = self._api_get(url)
            
            if response.status_code == 200:
                networks = response.json()['data']
                # Filter to just bridges
                return [n for n in networks if n.get('type') == 'bridge']
            return []
        except:
            return []
    
    def get_iso_list(self, node: str, storage: str = None) -> List[Dict]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            host = self.current_host or self.config.host
            isos = []
            # Get all storage if not specified
            storages = [storage] if storage else [s['storage'] for s in self.get_storage_list(node) if 'iso' in s.get('content', '')]
            
            for stor in storages:
                url = f"https://{host}:8006/api2/json/nodes/{node}/storage/{stor}/content"
                response = self._create_session().get(url, params={'content': 'iso'})
                
                if response.status_code == 200:
                    content = response.json()['data']
                    for item in content:
                        item['storage'] = stor
                        isos.append(item)
            
            return isos
        except Exception as e:
            self.logger.error(f"Error getting ISO list: {e}")
            return []
    
    # MK: Resource Pools - Jan 2026
    def get_pools(self) -> List[Dict]:
        """Get all resource pools from Proxmox"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/pools"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting pools: {e}")
            return []
    
    def get_pool_members(self, pool_id: str) -> Dict:
        """Get pool details including members"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {}
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/pools/{pool_id}"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', {})
            return {}
        except Exception as e:
            self.logger.error(f"Error getting pool members: {e}")
            return {}
    
    def get_vm_pool(self, vmid: int, vm_type: str = 'qemu') -> str:
        """Get the pool a VM belongs to (if any)"""
        pools = self.get_pools()
        for pool in pools:
            pool_data = self.get_pool_members(pool['poolid'])
            members = pool_data.get('members', [])
            for member in members:
                if member.get('vmid') == vmid and member.get('type') == vm_type:
                    return pool['poolid']
        return None
    
    def add_disk(self, node: str, vmid: int, vm_type: str, disk_config: Dict) -> Dict[str, Any]:
        """Add a new disk to VM or container
        
        LW: This was a pain to get right - Proxmox disk strings are weird
        MK: Jan 2026 - Added bus type detection for iothread/ssd support
        MK: Fixed to use PUT instead of POST for config updates
        """
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            storage = disk_config.get('storage', 'local-lvm')
            size = str(disk_config.get('size', '32')).replace('G', '').replace('g', '')
            disk_id = disk_config.get('disk_id', 'scsi1')
            
            # MK: Determine bus type from disk_id (e.g., "scsi0" -> "scsi")
            bus_type = ''.join(c for c in disk_id if c.isalpha())
            # LW: iothread only works with virtio-scsi controller
            supports_iothread = bus_type in ['scsi', 'virtio']
            # MK: ssd emulation supported for scsi, virtio, sata (NOT ide - tried it, breaks)
            supports_ssd = bus_type in ['scsi', 'virtio', 'sata']
            
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
                
                # Build disk string - format is storage:size (size in GB without unit)
                disk_str = f"{storage}:{size}"
                
                # Add optional parameters (only if supported by bus type)
                if disk_config.get('cache'):
                    disk_str += f",cache={disk_config['cache']}"
                # MK: Only add iothread for scsi/virtio
                if disk_config.get('iothread') and supports_iothread:
                    disk_str += ",iothread=1"
                # MK: Only add ssd for scsi/virtio/sata (not ide)
                if disk_config.get('ssd') and supports_ssd:
                    disk_str += ",ssd=1"
                if disk_config.get('discard'):
                    disk_str += ",discard=on"
                if disk_config.get('backup') == False:
                    disk_str += ",backup=0"
                
                data = {disk_id: disk_str}
                
            else:  # LXC
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
                
                # For LXC mountpoints
                mp_str = f"{storage}:{size}"
                if disk_config.get('mountpoint'):
                    mp_str += f",mp={disk_config['mountpoint']}"
                if disk_config.get('backup') == False:
                    mp_str += ",backup=0"
                
                data = {disk_id: mp_str}
            
            # MK: Use PUT for config updates (not POST)
            response = self._api_put(url, data=data)
            
            if response.status_code == 200:
                self.logger.info(f"[OK] Added disk {disk_id} to {vm_type}/{vmid}")
                return {'success': True, 'message': f'Disk {disk_id} added'}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def remove_disk(self, node: str, vmid: int, vm_type: str, disk_id: str, delete_data: bool = False) -> Dict[str, Any]:
        """
        Remove disk from VM.
        If delete_data=False: Only detach (disk becomes unused)
        If delete_data=True: Detach AND delete the volume physically
        
        MK: Fixed - after detach, disk becomes 'unused0' etc., so we need to delete that
        """
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            # MK: First get the current disk config to know the volume path
            volume_path = None
            if delete_data:
                config_result = self.get_vm_config(node, vmid, vm_type)
                if config_result.get('success'):
                    # MK: Structure is config_result['config']['raw'] - that's where disk IDs are
                    parsed_config = config_result.get('config', {})
                    raw_config = parsed_config.get('raw', {})
                    disk_config = raw_config.get(disk_id, '')
                    self.logger.info(f"[DEBUG] delete_data=True, disk_id={disk_id}, disk_config={disk_config}")
                    # Parse volume path from disk config (e.g., "local-lvm:vm-100-disk-0,size=32G")
                    if disk_config and ':' in str(disk_config):
                        volume_path = disk_config.split(',')[0]  # Get storage:volume part
                        self.logger.info(f"[DEBUG] volume_path={volume_path}")
            
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
            
            # First detach the disk from VM config
            data = {'delete': disk_id}
            response = self._api_put(url, data=data)
            
            if response.status_code == 200:
                self.logger.info(f"[OK] Detached disk {disk_id} from {vm_type}/{vmid}")
                
                # MK: If delete_data is True, find the unused slot and delete it
                if delete_data and volume_path:
                    try:
                        import time
                        time.sleep(0.8)  # Wait for Proxmox to update config
                        
                        # Get updated config to find the unused slot with our volume
                        new_config = self.get_vm_config(node, vmid, vm_type)
                        if new_config.get('success'):
                            # MK: Structure is new_config['config']['raw']
                            parsed_config = new_config.get('config', {})
                            raw_config = parsed_config.get('raw', {})
                            
                            # Find the unused slot containing our volume
                            unused_slot = None
                            for key, value in raw_config.items():
                                if key.startswith('unused') and volume_path in str(value):
                                    unused_slot = key
                                    self.logger.info(f"[DEBUG] Found {volume_path} in {key}={value}")
                                    break
                            
                            if unused_slot:
                                # Delete the unused slot - this removes the volume
                                delete_data_req = {'delete': unused_slot}
                                delete_response = self._api_put(url, data=delete_data_req)
                                if delete_response.status_code == 200:
                                    self.logger.info(f"[OK] Deleted volume via {unused_slot}")
                                    return {'success': True, 'message': f'Disk {disk_id} removed and deleted'}
                                else:
                                    self.logger.warning(f"[WARN] Failed to delete {unused_slot}: {delete_response.text}")
                            else:
                                self.logger.warning(f"[WARN] Could not find {volume_path} in unused slots. Raw config keys: {list(raw_config.keys())}")
                                # Volume not found in unused - try direct storage API delete
                                storage_name = volume_path.split(':')[0] if ':' in volume_path else None
                                if storage_name:
                                    import urllib.parse
                                    encoded_volid = urllib.parse.quote(volume_path, safe='')
                                    delete_url = f"https://{self.host}:8006/api2/json/nodes/{node}/storage/{storage_name}/content/{encoded_volid}"
                                    delete_response = self._api_delete(delete_url)
                                    if delete_response.status_code == 200:
                                        self.logger.info(f"[OK] Deleted volume {volume_path} via storage API")
                                        return {'success': True, 'message': f'Disk {disk_id} removed and deleted'}
                                    else:
                                        self.logger.warning(f"[WARN] Storage API delete failed: {delete_response.text}")
                        
                        return {'success': True, 'message': f'Disk {disk_id} detached (volume may still exist)'}
                    except Exception as del_err:
                        self.logger.warning(f"[WARN] Could not delete volume: {del_err}")
                        return {'success': True, 'message': f'Disk {disk_id} detached (volume deletion failed)'}
                elif delete_data and not volume_path:
                    self.logger.warning(f"[WARN] delete_data=True but volume_path is None - could not extract volume path from config")
                
                return {'success': True, 'message': f'Disk {disk_id} detached'}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def move_disk(self, node: str, vmid: int, vm_type: str, disk_id: str, target_storage: str, delete_original: bool = True) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/move_disk"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/move_volume"
            
            data = {
                'disk': disk_id,
                'storage': target_storage,
                'delete': 1 if delete_original else 0
            }
            
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                task_id = response.json().get('data')
                self.logger.info(f"[OK] Moving disk {disk_id} to {target_storage} (Task: {task_id})")
                return {'success': True, 'message': f'Disk move started', 'task': task_id}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def set_cdrom(self, node: str, vmid: int, iso_path: str = None, drive: str = 'ide2') -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            
            if iso_path:
                # Mount ISO
                data = {drive: f"{iso_path},media=cdrom"}
            else:
                # Eject - set to none
                data = {drive: "none,media=cdrom"}
            
            response = self._api_put(url, data=data)
            
            if response.status_code == 200:
                action = "mounted" if iso_path else "ejected"
                self.logger.info(f"[OK] CD-ROM {action} for VM {vmid}")
                return {'success': True, 'message': f'CD-ROM {action}'}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def add_network(self, node: str, vmid: int, vm_type: str, net_config: Dict) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            net_id = net_config.get('net_id', 'net1')
            
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
                
                # Build network string: model=XX:XX:XX:XX:XX:XX,bridge=vmbr0,...
                model = net_config.get('model', 'virtio')
                parts = [model]
                
                if net_config.get('macaddr'):
                    parts[0] = f"{model}={net_config['macaddr']}"
                
                if net_config.get('bridge'):
                    parts.append(f"bridge={net_config['bridge']}")
                if net_config.get('tag'):
                    parts.append(f"tag={net_config['tag']}")
                if net_config.get('firewall'):
                    parts.append("firewall=1")
                if net_config.get('rate'):
                    parts.append(f"rate={net_config['rate']}")
                if net_config.get('queues'):
                    parts.append(f"queues={net_config['queues']}")
                if net_config.get('mtu'):
                    parts.append(f"mtu={net_config['mtu']}")
                
                net_str = ','.join(parts)
                
            else:  # LXC
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
                
                parts = []
                if net_config.get('name'):
                    parts.append(f"name={net_config['name']}")
                if net_config.get('bridge'):
                    parts.append(f"bridge={net_config['bridge']}")
                if net_config.get('hwaddr'):
                    parts.append(f"hwaddr={net_config['hwaddr']}")
                if net_config.get('ip'):
                    parts.append(f"ip={net_config['ip']}")
                if net_config.get('gw'):
                    parts.append(f"gw={net_config['gw']}")
                if net_config.get('ip6'):
                    parts.append(f"ip6={net_config['ip6']}")
                if net_config.get('gw6'):
                    parts.append(f"gw6={net_config['gw6']}")
                if net_config.get('tag'):
                    parts.append(f"tag={net_config['tag']}")
                if net_config.get('firewall'):
                    parts.append("firewall=1")
                if net_config.get('rate'):
                    parts.append(f"rate={net_config['rate']}")
                if net_config.get('mtu'):
                    parts.append(f"mtu={net_config['mtu']}")
                
                net_str = ','.join(parts)
            
            data = {net_id: net_str}
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                self.logger.info(f"[OK] Added network {net_id} to {vm_type}/{vmid}")
                return {'success': True, 'message': f'Network {net_id} added'}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def update_network(self, node: str, vmid: int, vm_type: str, net_id: str, net_config: Dict) -> Dict[str, Any]:
        """Update network configuration
        
        NS: Supports all Proxmox network options including:
        - link_down (disconnect simulation)
        - queues (multiqueue for VirtIO)
        - rate limit, MTU, VLAN tag
        """
        # Same as add but uses PUT
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
                
                model = net_config.get('model', 'virtio')
                parts = [model]
                
                if net_config.get('macaddr'):
                    parts[0] = f"{model}={net_config['macaddr']}"
                
                if net_config.get('bridge'):
                    parts.append(f"bridge={net_config['bridge']}")
                if net_config.get('tag'):
                    parts.append(f"tag={net_config['tag']}")
                if net_config.get('firewall'):
                    parts.append("firewall=1")
                if net_config.get('rate'):
                    parts.append(f"rate={net_config['rate']}")
                if net_config.get('queues'):
                    parts.append(f"queues={net_config['queues']}")
                if net_config.get('mtu'):
                    parts.append(f"mtu={net_config['mtu']}")
                # LW: link_down=1 simulates unplugged cable
                if net_config.get('link_down'):
                    parts.append("link_down=1")
                
                net_str = ','.join(parts)
                
            else:  # LXC
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
                
                parts = []
                if net_config.get('name'):
                    parts.append(f"name={net_config['name']}")
                if net_config.get('bridge'):
                    parts.append(f"bridge={net_config['bridge']}")
                if net_config.get('hwaddr'):
                    parts.append(f"hwaddr={net_config['hwaddr']}")
                if net_config.get('ip'):
                    parts.append(f"ip={net_config['ip']}")
                if net_config.get('gw'):
                    parts.append(f"gw={net_config['gw']}")
                if net_config.get('ip6'):
                    parts.append(f"ip6={net_config['ip6']}")
                if net_config.get('gw6'):
                    parts.append(f"gw6={net_config['gw6']}")
                if net_config.get('tag'):
                    parts.append(f"tag={net_config['tag']}")
                if net_config.get('firewall'):
                    parts.append("firewall=1")
                if net_config.get('rate'):
                    parts.append(f"rate={net_config['rate']}")
                if net_config.get('mtu'):
                    parts.append(f"mtu={net_config['mtu']}")
                
                net_str = ','.join(parts)
            
            data = {net_id: net_str}
            response = self._api_put(url, data=data)
            
            if response.status_code == 200:
                self.logger.info(f"[OK] Updated network {net_id} on {vm_type}/{vmid}")
                return {'success': True, 'message': f'Network {net_id} updated'}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def remove_network(self, node: str, vmid: int, vm_type: str, net_id: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            if vm_type == 'qemu':
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            else:
                url = f"https://{self.host}:8006/api2/json/nodes/{node}/lxc/{vmid}/config"
            
            data = {'delete': net_id}
            response = self._api_put(url, data=data)
            
            if response.status_code == 200:
                self.logger.info(f"[OK] Removed network {net_id} from {vm_type}/{vmid}")
                return {'success': True, 'message': f'Network {net_id} removed'}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def toggle_network_link(self, node: str, vmid: int, net_id: str, link_down: bool) -> Dict[str, Any]:
        """Toggle network link_down state (cable unplug simulation)
        
        NS: This is hot-pluggable on QEMU - no VM restart needed!
        Useful for testing failover scenarios or isolating VMs temporarily.
        
        MK: The trick is to get the current net config, then modify just the link_down part
        while keeping everything else (bridge, mac, model, etc.) intact.
        """
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            
            # First get current network config
            config_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            config_response = self._api_get(config_url)
            
            if config_response.status_code != 200:
                return {'success': False, 'error': 'Could not get VM config'}
            
            vm_config = config_response.json().get('data', {})
            current_net = vm_config.get(net_id, '')
            
            if not current_net:
                return {'success': False, 'error': f'Network {net_id} not found'}
            
            # Parse current network string (e.g. "virtio=AA:BB:CC:DD:EE:FF,bridge=vmbr0,firewall=1")
            # and modify link_down parameter
            parts = current_net.split(',')
            new_parts = []
            found_link_down = False
            
            for part in parts:
                if part.startswith('link_down='):
                    found_link_down = True
                    if link_down:
                        new_parts.append('link_down=1')
                    # If link_down=False, we just don't add it (remove from config)
                else:
                    new_parts.append(part)
            
            # Add link_down=1 if not found and we want it
            if link_down and not found_link_down:
                new_parts.append('link_down=1')
            
            new_net_config = ','.join(new_parts)
            
            # Update the network config
            update_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
            response = self._api_put(update_url, data={net_id: new_net_config})
            
            if response.status_code == 200:
                action = 'disconnected' if link_down else 'connected'
                self.logger.info(f"[OK] Network {net_id} {action} on QEMU/{vmid}")
                return {'success': True, 'message': f'Network {net_id} {action}'}
            else:
                return {'success': False, 'error': response.text}
                
        except Exception as e:
            self.logger.error(f"[ERROR] Toggle network link failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def get_cpu_types(self) -> List[str]:
        
        return [
            'host', 'kvm64', 'kvm32', 'qemu64', 'qemu32', 
            'max', 'x86-64-v2', 'x86-64-v2-AES', 'x86-64-v3', 'x86-64-v4',
            'Broadwell', 'Broadwell-IBRS', 'Broadwell-noTSX', 'Broadwell-noTSX-IBRS',
            'Cascadelake-Server', 'Cascadelake-Server-noTSX', 'Cascadelake-Server-v2',
            'Conroe', 'Cooperlake', 'Cooperlake-v2',
            'EPYC', 'EPYC-IBPB', 'EPYC-Milan', 'EPYC-Rome', 'EPYC-v3',
            'Haswell', 'Haswell-IBRS', 'Haswell-noTSX', 'Haswell-noTSX-IBRS',
            'Icelake-Client', 'Icelake-Client-noTSX', 'Icelake-Server', 'Icelake-Server-noTSX',
            'IvyBridge', 'IvyBridge-IBRS',
            'Nehalem', 'Nehalem-IBRS',
            'Opteron_G1', 'Opteron_G2', 'Opteron_G3', 'Opteron_G4', 'Opteron_G5',
            'Penryn', 'SandyBridge', 'SandyBridge-IBRS',
            'SapphireRapids', 'Skylake-Client', 'Skylake-Client-IBRS', 
            'Skylake-Server', 'Skylake-Server-IBRS', 'Skylake-Server-noTSX-IBRS',
            'Westmere', 'Westmere-IBRS', 'athlon', 'core2duo', 'coreduo',
            'n270', 'pentium', 'pentium2', 'pentium3', 'phenom'
        ]
    
    def get_scsi_controllers(self) -> List[Dict]:
        
        return [
            {'value': 'virtio-scsi-pci', 'label': 'VirtIO SCSI'},
            {'value': 'virtio-scsi-single', 'label': 'VirtIO SCSI Single'},
            {'value': 'lsi', 'label': 'LSI 53C895A'},
            {'value': 'lsi53c810', 'label': 'LSI 53C810'},
            {'value': 'megasas', 'label': 'MegaRAID SAS 8708EM2'},
            {'value': 'pvscsi', 'label': 'VMware PVSCSI'},
        ]
    
    def get_network_models(self) -> List[Dict]:
        
        return [
            {'value': 'virtio', 'label': 'VirtIO (paravirtualized)'},
            {'value': 'e1000', 'label': 'Intel E1000'},
            {'value': 'e1000e', 'label': 'Intel E1000E'},
            {'value': 'vmxnet3', 'label': 'VMware vmxnet3'},
            {'value': 'rtl8139', 'label': 'Realtek RTL8139'},
            {'value': 'ne2k_pci', 'label': 'NE2000 PCI'},
            {'value': 'pcnet', 'label': 'AMD PCnet'},
        ]
    
    def get_disk_bus_types(self) -> List[Dict]:
        
        return [
            {'value': 'scsi', 'label': 'SCSI', 'max': 30},
            {'value': 'virtio', 'label': 'VirtIO Block', 'max': 15},
            {'value': 'sata', 'label': 'SATA', 'max': 5},
            {'value': 'ide', 'label': 'IDE', 'max': 3},
        ]
    
    def get_cache_modes(self) -> List[Dict]:
        
        return [
            {'value': '', 'label': 'Default (No cache)'},
            {'value': 'none', 'label': 'No cache'},
            {'value': 'writethrough', 'label': 'Write through'},
            {'value': 'writeback', 'label': 'Write back'},
            {'value': 'unsafe', 'label': 'Write back (unsafe)'},
            {'value': 'directsync', 'label': 'Direct sync'},
        ]
    
    def get_machine_types(self) -> List[Dict]:
        """Get available QEMU machine types
        
        MK: q35 is recommended for modern systems (PCIe native)
        i440fx is the legacy fallback for older guests
        Updated Jan 2026 to include all versions from Proxmox 8.x
        """
        return [
            {'value': '', 'label': 'Default'},
            # q35 versions (modern, PCIe native)
            {'value': 'q35', 'label': 'q35 (Latest)'},
            {'value': 'pc-q35-10.1', 'label': 'q35 10.1'},
            {'value': 'pc-q35-10.0+pve1', 'label': 'q35 10.0+pve1'},
            {'value': 'pc-q35-10.0', 'label': 'q35 10.0'},
            {'value': 'pc-q35-9.2+pve1', 'label': 'q35 9.2+pve1'},
            {'value': 'pc-q35-9.2', 'label': 'q35 9.2'},
            {'value': 'pc-q35-9.1', 'label': 'q35 9.1'},
            {'value': 'pc-q35-9.0', 'label': 'q35 9.0'},
            {'value': 'pc-q35-8.2', 'label': 'q35 8.2'},
            {'value': 'pc-q35-8.1', 'label': 'q35 8.1'},
            {'value': 'pc-q35-8.0', 'label': 'q35 8.0'},
            {'value': 'pc-q35-7.2', 'label': 'q35 7.2'},
            {'value': 'pc-q35-7.1', 'label': 'q35 7.1'},
            {'value': 'pc-q35-7.0', 'label': 'q35 7.0'},
            {'value': 'pc-q35-6.2', 'label': 'q35 6.2'},
            {'value': 'pc-q35-6.1', 'label': 'q35 6.1'},
            {'value': 'pc-q35-6.0', 'label': 'q35 6.0'},
            {'value': 'pc-q35-5.2', 'label': 'q35 5.2'},
            {'value': 'pc-q35-5.1', 'label': 'q35 5.1'},
            {'value': 'pc-q35-5.0', 'label': 'q35 5.0'},
            {'value': 'pc-q35-4.2', 'label': 'q35 4.2'},
            {'value': 'pc-q35-4.1', 'label': 'q35 4.1'},
            {'value': 'pc-q35-4.0', 'label': 'q35 4.0'},
            {'value': 'pc-q35-3.1', 'label': 'q35 3.1'},
            {'value': 'pc-q35-3.0', 'label': 'q35 3.0'},
            {'value': 'pc-q35-2.12', 'label': 'q35 2.12'},
            {'value': 'pc-q35-2.11', 'label': 'q35 2.11'},
            {'value': 'pc-q35-2.10', 'label': 'q35 2.10'},
            # i440fx versions (legacy PCI)
            {'value': 'i440fx', 'label': 'i440fx (Latest)'},
            {'value': 'pc-i440fx-10.1', 'label': 'i440fx 10.1'},
            {'value': 'pc-i440fx-10.0+pve1', 'label': 'i440fx 10.0+pve1'},
            {'value': 'pc-i440fx-10.0', 'label': 'i440fx 10.0'},
            {'value': 'pc-i440fx-9.2+pve1', 'label': 'i440fx 9.2+pve1'},
            {'value': 'pc-i440fx-9.2', 'label': 'i440fx 9.2'},
            {'value': 'pc-i440fx-9.1', 'label': 'i440fx 9.1'},
            {'value': 'pc-i440fx-9.0', 'label': 'i440fx 9.0'},
            {'value': 'pc-i440fx-8.2', 'label': 'i440fx 8.2'},
            {'value': 'pc-i440fx-8.1', 'label': 'i440fx 8.1'},
            {'value': 'pc-i440fx-8.0', 'label': 'i440fx 8.0'},
            {'value': 'pc-i440fx-7.2', 'label': 'i440fx 7.2'},
            {'value': 'pc-i440fx-7.1', 'label': 'i440fx 7.1'},
            {'value': 'pc-i440fx-7.0', 'label': 'i440fx 7.0'},
            {'value': 'pc-i440fx-6.2', 'label': 'i440fx 6.2'},
            {'value': 'pc-i440fx-6.1', 'label': 'i440fx 6.1'},
            {'value': 'pc-i440fx-6.0', 'label': 'i440fx 6.0'},
            {'value': 'pc-i440fx-5.2', 'label': 'i440fx 5.2'},
            {'value': 'pc-i440fx-5.1', 'label': 'i440fx 5.1'},
            {'value': 'pc-i440fx-5.0', 'label': 'i440fx 5.0'},
            {'value': 'pc-i440fx-4.2', 'label': 'i440fx 4.2'},
            {'value': 'pc-i440fx-4.1', 'label': 'i440fx 4.1'},
            {'value': 'pc-i440fx-4.0', 'label': 'i440fx 4.0'},
            {'value': 'pc-i440fx-3.1', 'label': 'i440fx 3.1'},
            {'value': 'pc-i440fx-3.0', 'label': 'i440fx 3.0'},
            {'value': 'pc-i440fx-2.12', 'label': 'i440fx 2.12'},
            {'value': 'pc-i440fx-2.11', 'label': 'i440fx 2.11'},
            {'value': 'pc-i440fx-2.10', 'label': 'i440fx 2.10'},
        ]
    
    # ==================== NODE MANAGEMENT METHODS ====================
    
    def get_node_summary(self, node: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {}
        
        try:
            # Get node status
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/status"
            response = self._api_get(url)
            
            if response.status_code == 200:
                data = response.json().get('data', {})
                
                # Get PVE version
                version_url = f"https://{self.host}:8006/api2/json/nodes/{node}/version"
                version_response = self._create_session().get(version_url, timeout=15)
                version_data = {}
                if version_response.status_code == 200:
                    version_data = version_response.json().get('data', {})
                
                return {
                    'node': node,
                    'status': 'online',
                    'uptime': data.get('uptime', 0),
                    'cpu': data.get('cpu', 0),
                    'cpuinfo': data.get('cpuinfo', {}),
                    'memory': {
                        'total': data.get('memory', {}).get('total', 0),
                        'used': data.get('memory', {}).get('used', 0),
                        'free': data.get('memory', {}).get('free', 0),
                    },
                    'swap': {
                        'total': data.get('swap', {}).get('total', 0),
                        'used': data.get('swap', {}).get('used', 0),
                        'free': data.get('swap', {}).get('free', 0),
                    },
                    'rootfs': {
                        'total': data.get('rootfs', {}).get('total', 0),
                        'used': data.get('rootfs', {}).get('used', 0),
                        'free': data.get('rootfs', {}).get('free', 0),
                    },
                    'loadavg': data.get('loadavg', [0, 0, 0]),
                    'kversion': data.get('kversion', ''),
                    'pveversion': version_data.get('version', ''),
                    'maintenance_mode': node in self.nodes_in_maintenance,
                }
            return {}
        except Exception as e:
            self.logger.error(f"Error getting node summary: {e}")
            return {}
    
    def get_node_rrddata(self, node: str, timeframe: str = 'hour') -> Dict[str, Any]:
        """Get node performance metrics (RRD data) for charts
        
        NS: Added Jan 2026 - Same format as VM rrddata for consistency
        """
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect to Proxmox'}
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes/{node}/rrddata"
            
            response = self._create_session().get(url, params={'timeframe': timeframe})
            
            if response.status_code == 200:
                rrd_data = response.json().get('data', [])
                
                # Process and format the data for charts
                formatted_data = {
                    'timeframe': timeframe,
                    'node': node,
                    'metrics': {
                        'cpu': [],
                        'memory': [],
                        'swap': [],
                        'iowait': [],
                        'loadavg': [],
                        'net_in': [],
                        'net_out': [],
                        'rootfs': []
                    },
                    'timestamps': []
                }
                
                for point in rrd_data:
                    if not point:
                        continue
                    
                    timestamp = point.get('time', 0)
                    formatted_data['timestamps'].append(timestamp)
                    
                    # CPU usage (0-1 -> 0-100%)
                    cpu = point.get('cpu', 0)
                    formatted_data['metrics']['cpu'].append(round((cpu or 0) * 100, 2))
                    
                    # IO Wait
                    iowait = point.get('iowait', 0)
                    formatted_data['metrics']['iowait'].append(round((iowait or 0) * 100, 2))
                    
                    # Memory usage
                    memused = point.get('memused', 0)
                    memtotal = point.get('memtotal', 1)
                    mem_percent = ((memused or 0) / (memtotal or 1)) * 100
                    formatted_data['metrics']['memory'].append(round(mem_percent, 2))
                    
                    # Swap usage
                    swapused = point.get('swapused', 0)
                    swaptotal = point.get('swaptotal', 1)
                    if swaptotal and swaptotal > 0:
                        swap_percent = ((swapused or 0) / swaptotal) * 100
                    else:
                        swap_percent = 0
                    formatted_data['metrics']['swap'].append(round(swap_percent, 2))
                    
                    # Load average
                    loadavg = point.get('loadavg', 0)
                    formatted_data['metrics']['loadavg'].append(round(loadavg or 0, 2))
                    
                    # Network I/O (bytes/s)
                    netin = point.get('netin', 0)
                    netout = point.get('netout', 0)
                    formatted_data['metrics']['net_in'].append(round((netin or 0) / 1024, 2))  # KB/s
                    formatted_data['metrics']['net_out'].append(round((netout or 0) / 1024, 2))  # KB/s
                    
                    # Root FS usage
                    rootused = point.get('rootused', 0)
                    roottotal = point.get('roottotal', 1)
                    if roottotal and roottotal > 0:
                        rootfs_percent = ((rootused or 0) / roottotal) * 100
                    else:
                        rootfs_percent = 0
                    formatted_data['metrics']['rootfs'].append(round(rootfs_percent, 2))
                
                return formatted_data
            return {'error': 'Failed to get RRD data'}
        except Exception as e:
            self.logger.error(f"Error getting node RRD data: {e}")
            return {'error': str(e)}
    
    def get_node_network_config(self, node: str) -> List[Dict]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/network"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting network config: {e}")
            return []
    
    def update_node_network(self, node: str, iface: str, config: Dict) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/network/{iface}"
            response = self._api_put(url, data=config)
            
            if response.status_code == 200:
                return {'success': True, 'message': 'Network updated'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def create_node_network(self, node: str, iface: str, iface_type: str, config: Dict) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/network"
            data = {
                'iface': iface,
                'type': iface_type,
                **config
            }
            # Remove empty values
            data = {k: v for k, v in data.items() if v is not None and v != ''}
            
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                return {'success': True, 'message': f'Interface {iface} created'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def delete_node_network(self, node: str, iface: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/network/{iface}"
            response = self._api_delete(url)
            
            if response.status_code == 200:
                return {'success': True, 'message': f'Interface {iface} deleted'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def apply_node_network(self, node: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/network"
            response = self._create_session().put(url, timeout=15)
            
            if response.status_code == 200:
                return {'success': True, 'message': 'Network changes applied'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def revert_node_network(self, node: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/network"
            response = self._api_delete(url)
            
            if response.status_code == 200:
                return {'success': True, 'message': 'Network changes reverted'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_dns(self, node: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/dns"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', {})
            return {}
        except Exception as e:
            self.logger.error(f"Error getting DNS config: {e}")
            return {}
    
    def update_node_dns(self, node: str, dns_config: Dict) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/dns"
            response = self._api_put(url, data=dns_config)
            
            if response.status_code == 200:
                return {'success': True, 'message': 'DNS updated'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_hosts(self, node: str) -> str:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return ''
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/hosts"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', {}).get('data', '')
            return ''
        except Exception as e:
            self.logger.error(f"Error getting hosts: {e}")
            return ''
    
    def update_node_hosts(self, node: str, hosts_content: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/hosts"
            response = self._api_post(url, data={'data': hosts_content})
            
            if response.status_code == 200:
                return {'success': True, 'message': 'Hosts updated'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_time(self, node: str) -> Dict[str, Any]:
        
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/time"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', {})
            return {}
        except Exception as e:
            self.logger.error(f"Error getting time: {e}")
            return {}
    
    def update_node_time(self, node: str, timezone: str) -> Dict[str, Any]:
        """Update node timezone"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/time"
            response = self._api_put(url, data={'timezone': timezone})
            
            if response.status_code == 200:
                return {'success': True, 'message': 'Timezone updated'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_syslog(self, node: str, start: int = 0, limit: int = 500, since: int = 0) -> List[str]:
        """Get node system log - returns newest entries first"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/syslog"
            params = {'limit': limit}
            if since:
                params['since'] = since
            response = self._create_session().get(url, params=params)
            
            if response.status_code == 200:
                data = response.json().get('data', [])
                # Reverse to get newest first, then return
                lines = [line.get('t', '') for line in data]
                return list(reversed(lines))
            return []
        except Exception as e:
            self.logger.error(f"Error getting syslog: {e}")
            return []
    
    def get_node_certificates(self, node: str) -> List[Dict]:
        """Get node certificates"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/certificates/info"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting certificates: {e}")
            return []
    
    def renew_node_certificate(self, node: str, force: bool = False) -> Dict[str, Any]:
        """Renew node certificate using ACME"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/certificates/acme/certificate"
            data = {'force': 1} if force else {}
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                return {'success': True, 'task': response.json().get('data')}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def upload_node_certificate(self, node: str, certificates: str, key: str, restart: bool = True, force: bool = False) -> Dict[str, Any]:
        """Upload custom certificate to node"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/certificates/custom"
            data = {
                'certificates': certificates,
                'key': key,
                'restart': 1 if restart else 0,
                'force': 1 if force else 0
            }
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                return {'success': True, 'message': 'Certificate uploaded successfully'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def delete_node_certificate(self, node: str, restart: bool = True) -> Dict[str, Any]:
        """Delete custom certificate from node (reverts to self-signed)"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/certificates/custom"
            params = {'restart': 1 if restart else 0}
            response = self._create_session().delete(url, params=params)
            
            if response.status_code == 200:
                return {'success': True, 'message': 'Certificate deleted'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_disks(self, node: str) -> List[Dict]:
        """Get physical disks on node"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/list"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting disks: {e}")
            return []
    
    def get_node_disk_smart(self, node: str, disk: str) -> Dict[str, Any]:
        """Get SMART data for a disk"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/smart"
            response = self._create_session().get(url, params={'disk': disk})
            
            if response.status_code == 200:
                return response.json().get('data', {})
            return {}
        except Exception as e:
            self.logger.error(f"Error getting SMART data: {e}")
            return {}
    
    def get_node_lvm(self, node: str) -> List[Dict]:
        """Get LVM volume groups"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/lvm"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting LVM: {e}")
            return []
    
    def create_node_lvm(self, node: str, device: str, name: str, add_storage: bool = True) -> Dict[str, Any]:
        """Create LVM volume group"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/lvm"
            data = {
                'device': device,
                'name': name,
                'add_storage': 1 if add_storage else 0
            }
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                return {'success': True, 'task': response.json().get('data')}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_lvmthin(self, node: str) -> List[Dict]:
        """Get LVM-Thin pools"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/lvmthin"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting LVM-Thin: {e}")
            return []
    
    def create_node_lvmthin(self, node: str, device: str, name: str, add_storage: bool = True) -> Dict[str, Any]:
        """Create LVM-Thin pool"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/lvmthin"
            data = {
                'device': device,
                'name': name,
                'add_storage': 1 if add_storage else 0
            }
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                return {'success': True, 'task': response.json().get('data')}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_zfs(self, node: str) -> List[Dict]:
        """Get ZFS pools"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/zfs"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting ZFS: {e}")
            return []
    
    def create_node_zfs(self, node: str, name: str, devices: list, raidlevel: str = 'single', 
                         compression: str = 'on', ashift: int = 12, add_storage: bool = True) -> Dict[str, Any]:
        """Create ZFS pool
        
        NS: Updated Dec 2025 to support compression and ashift
        
        Args:
            node: Node name
            name: Pool name
            devices: List of device paths (e.g., ['/dev/sdb', '/dev/sdc'])
            raidlevel: RAID level (single, mirror, raid10, raidz, raidz2, raidz3)
            compression: Compression (on, off, lz4, lzjb, zstd, gzip, zle)
            ashift: Sector size (9=512, 12=4K, 13=8K)
            add_storage: Whether to add as PVE storage
        """
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/zfs"
            
            # devices must be comma-separated string for Proxmox API
            if isinstance(devices, list):
                devices_str = ' '.join(devices)
            else:
                devices_str = devices
            
            data = {
                'name': name,
                'devices': devices_str,
                'raidlevel': raidlevel,
                'compression': compression,
                'ashift': ashift,
                'add_storage': 1 if add_storage else 0
            }
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                return {'success': True, 'task': response.json().get('data')}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_directory_storage(self, node: str) -> List[Dict]:
        """Get directory storage locations"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/directory"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting directory storage: {e}")
            return []
    
    def create_node_directory(self, node: str, device: str, name: str, filesystem: str = 'ext4', add_storage: bool = True) -> Dict[str, Any]:
        """Create directory storage"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/directory"
            data = {
                'device': device,
                'name': name,
                'filesystem': filesystem,
                'add_storage': 1 if add_storage else 0
            }
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                return {'success': True, 'task': response.json().get('data')}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def init_disk_gpt(self, node: str, disk: str, uuid: str = None) -> Dict[str, Any]:
        """Initialize disk with GPT partition table"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/initgpt"
            data = {'disk': disk}
            if uuid:
                data['uuid'] = uuid
            response = self._api_post(url, data=data)
            
            if response.status_code == 200:
                return {'success': True, 'task': response.json().get('data')}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def wipe_disk(self, node: str, disk: str) -> Dict[str, Any]:
        """Wipe disk (remove partition table)"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/disks/wipedisk"
            response = self._api_post(url, data={'disk': disk})
            
            if response.status_code == 200:
                return {'success': True, 'task': response.json().get('data')}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_replication(self, node: str) -> List[Dict]:
        """Get replication jobs for node"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/replication"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting replication: {e}")
            return []
    
    def get_node_tasks(self, node: str, start: int = 0, limit: int = 50, errors: bool = False) -> List[Dict]:
        """Get task history for node"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/tasks"
            params = {'start': start, 'limit': limit}
            if errors:
                params['errors'] = 1
            response = self._create_session().get(url, params=params)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting tasks: {e}")
            return []
    
    def get_node_task_log(self, node: str, upid: str, start: int = 0, limit: int = 500) -> List[str]:
        """Get log for a specific task"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/tasks/{upid}/log"
            response = self._create_session().get(url, params={'start': start, 'limit': limit})
            
            if response.status_code == 200:
                data = response.json().get('data', [])
                return [line.get('t', '') for line in data]
            return []
        except Exception as e:
            self.logger.error(f"Error getting task log: {e}")
            return []
    
    def get_node_subscription(self, node: str) -> Dict[str, Any]:
        """Get node subscription status"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/subscription"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', {})
            return {}
        except Exception as e:
            self.logger.error(f"Error getting subscription: {e}")
            return {}
    
    def update_node_subscription(self, node: str, key: str) -> Dict[str, Any]:
        """Update subscription key"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/subscription"
            response = self._api_put(url, data={'key': key})
            
            if response.status_code == 200:
                return {'success': True, 'message': 'Subscription updated'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_options(self, node: str) -> Dict[str, Any]:
        """Get node options (from datacenter)"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {}
        
        try:
            # Node options are part of datacenter config for that node
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/config"
            response = self._api_get(url)
            
            if response.status_code == 200:
                return response.json().get('data', {})
            return {}
        except Exception as e:
            self.logger.error(f"Error getting node options: {e}")
            return {}
    
    def update_node_options(self, node: str, options: Dict) -> Dict[str, Any]:
        """Update node options"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            url = f"https://{self.host}:8006/api2/json/nodes/{node}/config"
            response = self._api_put(url, data=options)
            
            if response.status_code == 200:
                return {'success': True, 'message': 'Options updated'}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_node_apt_updates(self, node: str) -> List[Dict]:
        """Get available APT updates"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return []
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes/{node}/apt/update"
            response = self._create_session().get(url, timeout=15)
            
            if response.status_code == 200:
                return response.json().get('data', [])
            return []
        except Exception as e:
            self.logger.error(f"Error getting APT updates: {e}")
            return []
    
    def refresh_node_apt(self, node: str) -> Dict[str, Any]:
        """Refresh APT package database"""
        if not self.is_connected:
            if not self.connect_to_proxmox():
                return {'success': False, 'error': 'Could not connect'}
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/nodes/{node}/apt/update"
            response = self._create_session().post(url, timeout=15)
            
            if response.status_code == 200:
                return {'success': True, 'task': response.json().get('data')}
            return {'success': False, 'error': response.text}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_timezones(self) -> List[str]:
        """Get list of available timezones"""
        return [
            'UTC', 'Europe/Berlin', 'Europe/Vienna', 'Europe/Zurich', 'Europe/London',
            'Europe/Paris', 'Europe/Amsterdam', 'Europe/Brussels', 'Europe/Rome',
            'Europe/Madrid', 'Europe/Warsaw', 'Europe/Prague', 'Europe/Budapest',
            'Europe/Stockholm', 'Europe/Helsinki', 'Europe/Athens', 'Europe/Moscow',
            'America/New_York', 'America/Chicago', 'America/Denver', 'America/Los_Angeles',
            'America/Toronto', 'America/Vancouver', 'America/Sao_Paulo', 'America/Mexico_City',
            'Asia/Tokyo', 'Asia/Shanghai', 'Asia/Hong_Kong', 'Asia/Singapore', 'Asia/Seoul',
            'Asia/Dubai', 'Asia/Kolkata', 'Asia/Bangkok', 'Asia/Jakarta',
            'Australia/Sydney', 'Australia/Melbourne', 'Australia/Perth',
            'Pacific/Auckland', 'Pacific/Fiji',
            'Africa/Cairo', 'Africa/Johannesburg', 'Africa/Lagos',
        ]

    # ==================== END NODE MANAGEMENT METHODS ====================
    
    def run_balance_check(self):
        """
        Run a single balance check iteration
        
        NS: This is the main loadbalancer logic - runs every check_interval seconds
        1. Get node scores (CPU + RAM weighted)
        2. If difference > threshold, find a VM to migrate
        3. Move smallest suitable VM from loaded -> less loaded node
        
        Pretty simple but works well in practice
        """
        try:
            self.logger.info("=" * 60)
            self.logger.info(f"Starting balance check for cluster: {self.config.name}")
            self.logger.info(f"Settings: Migration Threshold={self.config.migration_threshold}%, Check Interval={self.config.check_interval}s, Dry Run={self.config.dry_run}")
            
            # Get node status
            node_status = self.get_node_status()
            if not node_status:
                self.logger.warning("Could not get node status")
                return
            
            # Log nodes in maintenance
            maintenance_nodes = [n for n, d in node_status.items() if d.get('maintenance_mode')]
            if maintenance_nodes:
                self.logger.info(f"[MAINT] Nodes in maintenance: {', '.join(maintenance_nodes)}")
            
            # check balancing is needed
            needs_balance, source_node, target_node = self.check_balance_needed(node_status)
            
            if needs_balance and self.config.auto_migrate:
                # Find VM to migrate
                vm = self.find_migration_candidate(source_node, target_node)
                
                if vm:
                    # Perform migration
                    self.migrate_vm(vm, target_node)
                else:
                    self.logger.info("No suitable VM found for migration")
            
            self.last_run = datetime.now()
            self.logger.info(f"Balance check completed at {self.last_run}")
            self.logger.info("=" * 60)
            
        except Exception as e:
            self.logger.error(f"Error in balance check: {e}")
    
    def daemon_loop(self):
        """Main daemon loop"""
        self.logger.info(f"PegaProx daemon started for cluster: {self.config.name}")
        
        # Initial connection with auto-discovery
        if not self.connect_to_proxmox():
            self.logger.error("Initial connection failed, will retry...")
        
        while not self.stop_event.is_set():
            if self.config.enabled:
                # Check connection and reconnect if needed
                if not self._check_connection():
                    self.logger.warning("Connection lost, attempting reconnect...")
                    self.session = None
                    if self.connect_to_proxmox():
                        self.logger.info("Reconnected successfully")
                    else:
                        self.logger.error("Reconnect failed, will retry next cycle")
                
                self.run_balance_check()
            else:
                # LW: Even when disabled, still verify connection for UI status
                # Just less frequently - only every 5th cycle
                self._disabled_check_counter += 1
                
                if self._disabled_check_counter >= 5:
                    self._disabled_check_counter = 0
                    if not self._check_connection():
                        # Try to reconnect silently
                        self.session = None
                        self.connect_to_proxmox()
                
                self.logger.debug("PegaProx is disabled, skipping check")
            
            # Wait for next interval or stop signal
            self.stop_event.wait(self.config.check_interval)
        
        self.logger.info(f"PegaProx daemon stopped for cluster: {self.config.name}")
    
    def _check_connection(self) -> bool:
        """Check if connection to Proxmox is still alive"""
        if not self.is_connected:
            return False
        
        try:
            host = self.current_host or self.config.host
            url = f"https://{host}:8006/api2/json/version"
            response = self._create_session().get(url, timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def start(self):
        """Start the PegaProx daemon"""
        if self.running:
            return
        
        self.stop_event.clear()
        self.thread = threading.Thread(target=self.daemon_loop)
        self.thread.daemon = True
        self.thread.start()
        self.running = True
        self.logger.info(f"Started PegaProx manager for {self.config.name}")
        
        # Start HA monitor if enabled
        if self.config.ha_enabled:
            self.start_ha_monitor()
    
    def stop(self):
        """Stop the PegaProx daemon"""
        if not self.running:
            return
        
        # Stop HA monitor
        self.stop_ha_monitor()
        
        self.stop_event.set()
        if self.thread:
            self.thread.join(timeout=5)
        self.running = False
        self.logger.info(f"Stopped PegaProx manager for {self.config.name}")

def get_or_create_encryption_key():
    """Get existing encryption key or create a new one"""
    if not ENCRYPTION_AVAILABLE:
        return None
    
    if os.path.exists(KEY_FILE):
        with open(KEY_FILE, 'rb') as f:
            return f.read()
    
    # Generate new key
    key = Fernet.generate_key()
    
    # Save key with restricted permissions
    with open(KEY_FILE, 'wb') as f:
        f.write(key)
    
    # Set file permissions to owner only (Unix)
    try:
        os.chmod(KEY_FILE, 0o600)
    except:
        pass
    
    logging.info("Generated new encryption key")
    return key

def get_fernet():
    """Get Fernet encryption instance"""
    if not ENCRYPTION_AVAILABLE:
        return None
    
    key = get_or_create_encryption_key()
    if key:
        return Fernet(key)
    return None

def load_config():
    """Load configuration from SQLite database
    
    refactored to use SQLite
    Automatically migrates existing JSON/encrypted files on first run
    """
    logging.info("=== Loading config from SQLite ===")
    
    try:
        db = get_db()
        config = db.get_all_clusters()
        
        if config:
            logging.info(f"✓ Loaded {len(config)} clusters from SQLite: {list(config.keys())}")
            return config
        else:
            logging.info("No clusters in database yet")
            return {}
    except Exception as e:
        logging.error(f"Failed to load config from database: {e}")
        import traceback
        logging.error(traceback.format_exc())
        
        # Emergency fallback to legacy files
        logging.info("Attempting legacy fallback...")
        return _load_config_legacy()


def _load_config_legacy():
    """Legacy config loader - used as fallback if database fails"""
    fernet = get_fernet()
    
    # Try encrypted file
    if fernet and os.path.exists(CONFIG_FILE_ENCRYPTED):
        try:
            with open(CONFIG_FILE_ENCRYPTED, 'rb') as f:
                encrypted_data = f.read()
            decrypted_data = fernet.decrypt(encrypted_data)
            config = json.loads(decrypted_data.decode('utf-8'))
            if config:
                logging.info(f"✓ Loaded {len(config)} clusters from legacy encrypted file")
                return config
        except Exception as e:
            logging.error(f"Failed to load legacy encrypted config: {e}")
    
    # Try unencrypted file
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r') as f:
                config = json.load(f)
            if config:
                logging.info(f"✓ Loaded {len(config)} clusters from legacy JSON file")
                return config
        except Exception as e:
            logging.error(f"Failed to load legacy config: {e}")
    
    return {}


def save_config():
    """Save configuration to SQLite database
    
    SQLite instead of JSON now
    """
    if not cluster_managers:
        logging.warning("save_config called with no clusters - skipping")
        return False
    
    try:
        db = get_db()
        
        for cluster_id, manager in cluster_managers.items():
            try:
                # Sanitize fallback_hosts
                fallback_hosts = manager.config.fallback_hosts or []
                if not isinstance(fallback_hosts, list):
                    fallback_hosts = []
                fallback_hosts = [str(h) for h in fallback_hosts if h]
                
                cluster_data = {
                    'name': manager.config.name,
                    'host': manager.config.host,
                    'user': manager.config.user,
                    'pass': manager.config.pass_,
                    'ssl_verification': manager.config.ssl_verification,
                    'migration_threshold': manager.config.migration_threshold,
                    'check_interval': manager.config.check_interval,
                    'auto_migrate': manager.config.auto_migrate,
                    'balance_containers': getattr(manager.config, 'balance_containers', False),
                    'balance_local_disks': getattr(manager.config, 'balance_local_disks', False),
                    'dry_run': manager.config.dry_run,
                    'enabled': manager.config.enabled,
                    'ha_enabled': manager.config.ha_enabled,
                    'fallback_hosts': fallback_hosts,
                    'ssh_user': getattr(manager.config, 'ssh_user', ''),
                    'ssh_key': getattr(manager.config, 'ssh_key', ''),
                    'ssh_port': getattr(manager.config, 'ssh_port', 22),
                    'ha_settings': getattr(manager.config, 'ha_settings', {}),
                    'excluded_nodes': getattr(manager.config, 'excluded_nodes', []),
                }
                
                db.save_cluster(cluster_id, cluster_data)
            except Exception as e:
                logging.error(f"Error saving cluster {cluster_id}: {e}")
                continue
        
        logging.debug(f"Saved {len(cluster_managers)} clusters to SQLite")
        return True
        
    except Exception as e:
        logging.error(f"Failed to save config to database: {e}")
        return False


# old version, keep for reference
# def save_config_v1(config):
#     with open(CONFIG_FILE, 'w') as f:
#         json.dump(config, f, indent=2)


# ============================================
# User Management System
# MK: Simple but secure - PBKDF2 with 100k iterations
# Considered bcrypt but this works fine for our scale
# ============================================

# ============================================
# Input sanitization - MK added after security review
# Prevent XSS and other injection attacks
# ============================================

import html
import re

def sanitize_string(value: str, max_length: int = 1000, allow_html: bool = False) -> str:
    """Sanitize a string input
    
    Args:
        value: Input string to sanitize
        max_length: Maximum allowed length
        allow_html: If False, HTML entities are escaped
    
    Returns:
        Sanitized string
    """
    if not isinstance(value, str):
        value = str(value) if value is not None else ''
    
    # Truncate to max length
    value = value[:max_length]
    
    # Strip null bytes and other control characters
    value = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', value)
    
    # Escape HTML if not allowed
    if not allow_html:
        value = html.escape(value)
    
    return value.strip()


def sanitize_identifier(value: str, max_length: int = 64) -> str:
    """Sanitize an identifier (username, cluster_id, etc.)
    
    Only allows alphanumeric, underscore, hyphen, dot
    """
    if not isinstance(value, str):
        value = str(value) if value is not None else ''
    
    # Only allow safe characters
    value = re.sub(r'[^a-zA-Z0-9_\-\.]', '', value)
    
    return value[:max_length]


def sanitize_int(value, default: int = 0, min_val: int = None, max_val: int = None) -> int:
    """Sanitize an integer input"""
    try:
        result = int(value)
        if min_val is not None and result < min_val:
            result = min_val
        if max_val is not None and result > max_val:
            result = max_val
        return result
    except (ValueError, TypeError):
        return default


def sanitize_bool(value, default: bool = False) -> bool:
    """Sanitize a boolean input"""
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        return value.lower() in ('true', '1', 'yes', 'on')
    if isinstance(value, int):
        return value != 0
    return default


def validate_email(email: str) -> bool:
    """Validate email format"""
    if not email or not isinstance(email, str):
        return False
    # Simple regex - not perfect but catches most issues
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return bool(re.match(pattern, email))


def validate_hostname(hostname: str) -> bool:
    """Validate hostname/IP format"""
    if not hostname or not isinstance(hostname, str):
        return False
    # Allow IP addresses and hostnames
    ip_pattern = r'^(\d{1,3}\.){3}\d{1,3}$'
    hostname_pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*$'
    return bool(re.match(ip_pattern, hostname) or re.match(hostname_pattern, hostname))


def hash_password(password: str, salt: bytes = None) -> tuple:
    """hash pw with argon2 or pbkdf2 fallback
    
    MK: Always prefer argon2 - install with: pip install argon2-cffi
    """
    if ARGON2_AVAILABLE:
        # argon2 is way better - MK
        ph = PasswordHasher(
            time_cost=3,
            memory_cost=65536,  # 64mb, makes gpu cracking hard
            parallelism=4,
            hash_len=32,
            salt_len=16,
            type=argon2.Type.ID
        )
        hash_string = ph.hash(password)
        return 'argon2', hash_string
    else:
        # fallback to pbkdf2 - still secure, just slower
        if salt is None:
            salt = os.urandom(32)
        
        # NS: 600k iterations now, used to be 100k - OWASP recommendation
        key = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 600000)
        return base64.b64encode(salt).decode('utf-8'), base64.b64encode(key).decode('utf-8')


def verify_password(password: str, salt_b64: str, hash_b64: str) -> bool:
    """verify pw - handles both argon2 and old pbkdf2
    
    NS: Order matters! salt first, then hash
    """
    try:
        # check for argon2
        if salt_b64 == 'argon2' or hash_b64.startswith('$argon2'):
            if not ARGON2_AVAILABLE:
                logging.error("argon2 hash but lib not installed??")
                return False
            
            ph = PasswordHasher()
            try:
                ph.verify(hash_b64, password)
                return True
            except VerifyMismatchError:
                return False
            except Exception as e:
                logging.error(f"argon2 error: {e}")
                return False
        
        # old pbkdf2
        salt = base64.b64decode(salt_b64)
        stored_hash = base64.b64decode(hash_b64)
        
        # MK: try new iteration count first, then old for backwards compat
        for iterations in [600000, 100000]:
            key = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, iterations)
            if key == stored_hash:
                return True
        
        return False
    except Exception as e:
        logging.error(f"pw verify error: {e}")
        return False


def needs_password_rehash(salt_b64: str, hash_b64: str) -> bool:
    """check if pw needs upgrade to argon2"""
    if not ARGON2_AVAILABLE:
        return False
    
    # already argon2?
    if salt_b64 == 'argon2' or (hash_b64 and hash_b64.startswith('$argon2')):
        return False
    
    # Old PBKDF2 format - should be upgraded
    return True


def _check_default_password_in_use() -> bool:
    """Check if any admin account still uses default password 'admin'
    
    NS: Security warning - default passwords are a major risk!
    This is called from security compliance check.
    """
    try:
        users_db = load_users()
        
        for username, user in users_db.items():
            # Only check admin accounts
            if user.get('role') != ROLE_ADMIN:
                continue
            
            # Check if password is 'admin'
            salt = user.get('password_salt', '')
            hash_val = user.get('password_hash', '')
            
            if salt and hash_val:
                if verify_password('admin', salt, hash_val):
                    logging.warning(f"SECURITY WARNING: Admin user '{username}' still uses default password!")
                    return True
        
        return False
    except Exception as e:
        logging.error(f"Error checking default passwords: {e}")
        return False  # Don't block on errors


def validate_password_policy(password: str) -> tuple:
    """
    Validate password against configured policy.
    Returns (is_valid: bool, error_message: str or None)
    """
    settings = load_server_settings()
    
    min_length = settings.get('password_min_length', 8)
    require_upper = settings.get('password_require_uppercase', True)
    require_lower = settings.get('password_require_lowercase', True)
    require_numbers = settings.get('password_require_numbers', True)
    require_special = settings.get('password_require_special', False)
    
    errors = []
    
    if len(password) < min_length:
        errors.append(f"at least {min_length} characters")
    
    if require_upper and not any(c.isupper() for c in password):
        errors.append("at least one uppercase letter")
    
    if require_lower and not any(c.islower() for c in password):
        errors.append("at least one lowercase letter")
    
    if require_numbers and not any(c.isdigit() for c in password):
        errors.append("at least one number")
    
    if require_special and not any(c in '!@#$%^&*()_+-=[]{}|;:,.<>?' for c in password):
        errors.append("at least one special character")
    
    if errors:
        return False, "Password must contain: " + ", ".join(errors)
    
    return True, None


def load_users() -> dict:
    """load users from db"""
    # MK: migrated from json files, was a pain
    try:
        db = get_db()
        users = db.get_all_users()
        
        if users:
            # MK: sanity check - had issues with corrupt user data once
            for username, userdata in users.items():
                if not isinstance(userdata, dict):
                    logging.error(f"User {username} has invalid data type: {type(userdata)}")
            return users
    except Exception as e:
        logging.error(f"db load failed: {e}")
        return _load_users_legacy()  # fallback to old format
    
    # no users found
    if os.path.exists(ADMIN_INITIALIZED_FILE):
        # dont recreate admin if it was deleted on purpose
        logging.error("users missing but admin was initialized - wont recreate")
        return {}
    
    logging.info("no users, creating default admin")
    default_users = create_default_users()
    save_users(default_users)
    return default_users


def _load_users_legacy() -> dict:
    """old json loader, just for migration"""
    fernet = get_fernet()
    
    if fernet and os.path.exists(USERS_FILE_ENCRYPTED):
        try:
            with open(USERS_FILE_ENCRYPTED, 'rb') as f:
                encrypted_data = f.read()
            decrypted_data = fernet.decrypt(encrypted_data)
            users = json.loads(decrypted_data.decode('utf-8'))
            logging.info(f"loaded {len(users)} users from legacy file")
            return users
        except Exception as e:
            logging.error(f"legacy load failed: {e}")
    
    return {}


def save_users(users: dict):
    """save users to db"""
    try:
        db = get_db()
        db.save_all_users(users)
        # logging.debug(f"saved {len(users)} users")
    except Exception as e:
        logging.error(f"save failed: {e}")

def mark_admin_initialized():
    """mark admin as customized so we dont recreate it"""
    try:
        with open(ADMIN_INITIALIZED_FILE, 'w') as f:
            f.write(datetime.now().isoformat())
        os.chmod(ADMIN_INITIALIZED_FILE, 0o600)
    except Exception as e:
        logging.error(f"couldnt mark admin init: {e}")

def create_default_users() -> dict:
    """create default admin - pw is 'admin', should be changed obv"""
    salt, password_hash = hash_password('admin')
    
    return {
        'pegaprox': {
            'password_salt': salt,
            'password_hash': password_hash,
            'role': ROLE_ADMIN,
            'created_at': datetime.now().isoformat(),
            'last_login': None,
            'display_name': 'PegaProx Admin',
            'email': '',
            'enabled': True,
            'is_default': True  # Flag to identify default admin
        }
    }

def generate_session_id() -> str:
    """Generate a secure session ID"""
    return base64.urlsafe_b64encode(os.urandom(32)).decode('utf-8')

# =============================================================================
# SESSION PERSISTENCE - NS: Added Dec 2025
# Sessions are now encrypted and persisted to survive server restarts
# LW: Finally got around to this after MK's TODO sat there for months
# =============================================================================

def save_sessions():
    """Save active sessions to SQLite database
    
    SQLite migration
    """
    global active_sessions
    
    try:
        # Clean up expired sessions first
        timeout = get_session_timeout()
        now = time.time()
        expired = [sid for sid, sess in active_sessions.items() 
                   if now - sess.get('last_activity', 0) > timeout]
        for sid in expired:
            del active_sessions[sid]
        
        # Save to database
        db = get_db()
        db.save_all_sessions(active_sessions)
        
    except Exception as e:
        logging.error(f"Failed to save sessions: {e}")


def load_sessions():
    """Load active sessions from SQLite database
    
    SQLite migration
    """
    global active_sessions
    
    try:
        db = get_db()
        active_sessions = db.get_all_sessions()
        
        # Clean up expired sessions
        timeout = get_session_timeout()
        now = time.time()
        expired = [sid for sid, sess in active_sessions.items() 
                   if now - sess.get('last_activity', 0) > timeout]
        for sid in expired:
            del active_sessions[sid]
            db.delete_session(sid)
        
        logging.info(f"Loaded {len(active_sessions)} sessions from SQLite")
        
    except Exception as e:
        logging.warning(f"Failed to load sessions from database: {e}")
        # Try legacy fallback
        _load_sessions_legacy()


def _load_sessions_legacy():
    """Legacy sessions loader - used as fallback"""
    global active_sessions
    fernet = get_fernet()
    
    if fernet and os.path.exists(SESSIONS_FILE_ENCRYPTED):
        try:
            with open(SESSIONS_FILE_ENCRYPTED, 'rb') as f:
                encrypted = f.read()
            decrypted = fernet.decrypt(encrypted)
            active_sessions = json.loads(decrypted.decode('utf-8'))
            logging.info(f"Loaded {len(active_sessions)} sessions from legacy encrypted file")
        except:
            pass
    elif os.path.exists(SESSIONS_FILE):
        try:
            with open(SESSIONS_FILE, 'r') as f:
                active_sessions = json.load(f)
            logging.info(f"Loaded {len(active_sessions)} sessions from legacy JSON file")
        except:
            pass

def create_session(username: str, role: str) -> str:
    """Create a new session for a user
    
    NS: Also does session rotation - invalidates old sessions for same user
    This prevents session fixation attacks and limits concurrent sessions
    """
    # Session rotation: invalidate existing sessions for this user
    # MK: keep max 3 sessions per user (browser, phone, etc)
    user_sessions = [(sid, sess) for sid, sess in active_sessions.items() 
                     if sess.get('user') == username]
    
    # Sort by last_activity, remove oldest if more than 2 (new one will be 3rd)
    if len(user_sessions) >= 3:
        user_sessions.sort(key=lambda x: x[1].get('last_activity', 0))
        # Remove oldest sessions, keep 2
        for sid, _ in user_sessions[:-2]:
            del active_sessions[sid]
            logging.debug(f"Session rotation: removed old session for {username}")
    
    session_id = generate_session_id()
    active_sessions[session_id] = {
        'user': username,
        'role': role,
        'created_at': time.time(),
        'last_activity': time.time(),
        'ip': request.remote_addr if request else None,  # track IP for auditing
        'user_agent': request.headers.get('User-Agent', '')[:200] if request else None
    }
    
    # Save sessions to disk
    save_sessions()
    
    return session_id

def validate_session(session_id: str) -> dict:
    """Validate a session and return user info if valid"""
    if not session_id or session_id not in active_sessions:
        return None
    
    session = active_sessions[session_id]
    
    # check session has expired
    if time.time() - session['last_activity'] > get_session_timeout():
        del active_sessions[session_id]
        save_sessions()
        return None
    
    # Update last activity
    session['last_activity'] = time.time()
    
    return session

def invalidate_session(session_id: str):
    """Invalidate a session (logout)"""
    if session_id in active_sessions:
        del active_sessions[session_id]
        save_sessions()

def invalidate_all_user_sessions(username: str, except_session: str = None):
    """Invalidate all sessions for a user (used when password changes)
    
    LW: This is important for security - when password changes, all sessions should die
    """
    global active_sessions
    sessions_removed = 0
    for sid in list(active_sessions.keys()):
        if active_sessions[sid].get('user') == username and sid != except_session:
            del active_sessions[sid]
            sessions_removed += 1
    
    if sessions_removed > 0:
        save_sessions()
        logging.info(f"Invalidated {sessions_removed} sessions for user '{username}'")
    
    return sessions_removed


def require_auth(roles: list = None, perms: list = None):
    """
    Decorator to require authentication for API endpoints.
    
    MK: This is the main auth guard - use on all protected routes.
    
    Args:
        roles: Optional list of allowed roles. If None, any authenticated user is allowed.
               Example: @require_auth(roles=['admin']) for admin-only endpoints
        perms: Optional list of required permissions. User needs ALL listed permissions.
               Example: @require_auth(perms=['vm.start', 'vm.stop'])
    
    Returns:
        401 if not authenticated
        403 if authenticated but wrong role/missing permissions
    """
    def decorator(f):
        from functools import wraps
        @wraps(f)
        def decorated_function(*args, **kwargs):
            # Get session from header or cookie
            session_id = request.headers.get('X-Session-ID') or request.cookies.get('session_id')
            
            session = validate_session(session_id)
            if not session:
                return jsonify({'error': 'Unauthorized', 'code': 'AUTH_REQUIRED'}), 401
            
            # Check role if specified
            if roles and session['role'] not in roles:
                return jsonify({'error': 'Forbidden', 'code': 'INSUFFICIENT_PERMISSIONS'}), 403
            
            # check permissions if specified
            if perms:
                users = load_users()
                user = users.get(session['user'], {})
                for p in perms:
                    if not has_permission(user, p):
                        return jsonify({'error': 'Permission denied', 'code': 'MISSING_PERMISSION', 'required': p}), 403
            
            # Add session info to request context
            request.session = session
            
            return f(*args, **kwargs)
        return decorated_function
    return decorator

def cleanup_expired_sessions():
    """Remove expired sessions"""
    current_time = time.time()
    expired = [sid for sid, session in active_sessions.items() 
               if current_time - session['last_activity'] > get_session_timeout()]
    for sid in expired:
        del active_sessions[sid]
    if expired:
        logging.debug(f"Cleaned up {len(expired)} expired sessions")

# ============================================
# Audit Log System (SQLite)
# SQLite migration
# ============================================

def load_audit_log():
    """Load audit log from SQLite database
    
    SQLite migration
    """
    global audit_log
    
    try:
        db = get_db()
        entries = db.get_audit_log(limit=10000)  # Load recent entries
        audit_log = entries
        logging.info(f"Loaded {len(audit_log)} audit log entries from SQLite")
    except Exception as e:
        logging.error(f"Failed to load audit log from database: {e}")
        # Legacy fallback
        _load_audit_log_legacy()


def _load_audit_log_legacy():
    """Legacy audit log loader"""
    global audit_log
    fernet = get_fernet()
    
    if fernet and os.path.exists(AUDIT_LOG_FILE_ENCRYPTED):
        try:
            with open(AUDIT_LOG_FILE_ENCRYPTED, 'rb') as f:
                encrypted_data = f.read()
            decrypted_data = fernet.decrypt(encrypted_data)
            audit_log = json.loads(decrypted_data.decode('utf-8'))
            logging.info(f"Loaded {len(audit_log)} audit entries from legacy encrypted file")
            return
        except:
            pass
    
    if os.path.exists(AUDIT_LOG_FILE):
        try:
            with open(AUDIT_LOG_FILE, 'r') as f:
                audit_log = json.load(f)
            logging.info(f"Loaded {len(audit_log)} audit entries from legacy JSON file")
            return
        except:
            pass
    
    audit_log = []


def save_audit_log():
    """Save audit log - now handled automatically by database
    
    kept for backwards compat
    Individual entries are saved directly to database via log_audit()
    """
    # In SQLite version, saving is handled per-entry
    # This function is kept for backwards compatibility
    pass


def cleanup_audit_log():
    """Remove audit entries older than retention period
    
    uses db.delete now
    """
    global audit_log
    
    try:
        db = get_db()
        deleted = db.cleanup_audit_log(days=AUDIT_RETENTION_DAYS)
        if deleted > 0:
            logging.info(f"Cleaned up {deleted} old audit log entries")
    except Exception as e:
        logging.error(f"Failed to cleanup audit log: {e}")

def log_audit(user: str, action: str, details: str = None, ip_address: str = None, cluster: str = None):
    """Add an entry to the audit log
    
    writes to db now
    """
    global audit_log
    
    entry = {
        'timestamp': datetime.now().isoformat(),
        'user': user,
        'action': action,
        'details': details,
        'ip_address': ip_address or get_client_ip(),
        'cluster': cluster  # Which cluster this action was performed on
    }
    
    # Add to in-memory list (for backwards compatibility)
    audit_log.insert(0, entry)
    if len(audit_log) > 10000:
        audit_log = audit_log[:10000]
    
    # Save to database
    try:
        db = get_db()
        db.add_audit_entry(
            user=user,
            action=action,
            details=f"{details}" + (f" [{cluster}]" if cluster else ""),
            ip=ip_address or get_client_ip()
        )
    except Exception as e:
        logging.error(f"Failed to save audit entry to database: {e}")
    
    cluster_info = f" [{cluster}]" if cluster else ""
    logging.info(f"Audit: {user} - {action}{cluster_info} - {details}")

def get_client_ip():
    """Get client IP address from request"""
    if request.headers.get('X-Forwarded-For'):
        return request.headers.get('X-Forwarded-For').split(',')[0].strip()
    elif request.headers.get('X-Real-IP'):
        return request.headers.get('X-Real-IP')
    else:
        return request.remote_addr

# Global users store (loaded at startup)
users_db = {}

# ============================================
# Authentication API Routes
# MK: Session-based auth, tokens stored server-side
# Added rate limiting after some script kiddie tried brute force
# Oct 2025: Added TOTP 2FA support (NS wanted this)
# ============================================

@app.route('/api/auth/login', methods=['POST'])
def auth_login():
    """login endpoint - MK"""
    global users_db, login_attempts_by_ip, login_attempts_by_user
    
    # get settings
    login_settings = get_login_settings()
    max_attempts = login_settings['max_attempts']
    lockout_time = login_settings['lockout_time']
    attempt_window = login_settings['attempt_window']
    
    # get ip (handle proxies)
    client_ip = request.headers.get('X-Forwarded-For', request.remote_addr)
    if client_ip:
        client_ip = client_ip.split(',')[0].strip()
    
    current_time = time.time()
    
    # check if ip is locked
    if client_ip in login_attempts_by_ip:
        attempt_info = login_attempts_by_ip[client_ip]
        if attempt_info.get('locked_until', 0) > current_time:
            remaining = int(attempt_info['locked_until'] - current_time)
            logging.warning(f"locked ip tried to login: {client_ip}")
            return jsonify({
                'error': f'Too many failed attempts. Try again in {remaining} seconds.',
                'locked': True,
                'retry_after': remaining
            }), 429
        # cleanup old attempts
        attempt_info['attempts'] = [t for t in attempt_info.get('attempts', []) 
                                    if current_time - t < attempt_window]
    
    data = request.get_json()
    if not data:
        return jsonify({'error': 'Invalid request body'}), 400
    
    # sanitize inputs (security stuff)
    username = sanitize_identifier(data.get('username', '').strip().lower(), max_length=64)
    password = data.get('password', '')[:256]  # limit to prevent DoS
    totp_code = sanitize_identifier(data.get('totp_code', ''), max_length=10)
    
    # print(f"login: {username}")  # DEBUG - remove before commit!! - NS
    
    if not username or not password:
        return jsonify({'error': 'Username and password required'}), 400
    
    if len(username) < 2:
        return jsonify({'error': 'Username too short'}), 400
    
    # Check if username is locked out (additional protection against distributed attacks)
    if username in login_attempts_by_user:
        user_attempt_info = login_attempts_by_user[username]
        if user_attempt_info.get('locked_until', 0) > current_time:
            remaining = int(user_attempt_info['locked_until'] - current_time)
            logging.warning(f"Login attempt for locked user: {username} from {client_ip}, {remaining}s remaining")
            return jsonify({
                'error': f'Account temporarily locked. Try again in {remaining} seconds.',
                'locked': True,
                'retry_after': remaining
            }), 429
        # Clean up old attempts
        user_attempt_info['attempts'] = [t for t in user_attempt_info.get('attempts', []) 
                                          if current_time - t < attempt_window]
    
    # Helper function to record failed attempt (both IP and username)
    def record_failed_attempt(target_username=None):
        locked = False
        
        # Track by IP
        if client_ip not in login_attempts_by_ip:
            login_attempts_by_ip[client_ip] = {'attempts': [], 'locked_until': 0}
        login_attempts_by_ip[client_ip]['attempts'].append(current_time)
        recent_ip = [t for t in login_attempts_by_ip[client_ip]['attempts'] 
                     if current_time - t < attempt_window]
        if len(recent_ip) >= max_attempts:
            login_attempts_by_ip[client_ip]['locked_until'] = current_time + lockout_time
            logging.warning(f"IP {client_ip} locked out after {len(recent_ip)} failed attempts")
            locked = True
        
        # Track by username (if provided and valid)
        if target_username:
            if target_username not in login_attempts_by_user:
                login_attempts_by_user[target_username] = {'attempts': [], 'locked_until': 0}
            login_attempts_by_user[target_username]['attempts'].append(current_time)
            recent_user = [t for t in login_attempts_by_user[target_username]['attempts'] 
                          if current_time - t < attempt_window]
            # Use slightly higher threshold for username (10 attempts) to prevent username enumeration lockout
            if len(recent_user) >= max_attempts * 2:
                login_attempts_by_user[target_username]['locked_until'] = current_time + lockout_time
                logging.warning(f"User '{target_username}' locked out after {len(recent_user)} failed attempts")
                locked = True
        
        return locked
    
    # Reload users in case they were updated
    # NS: had a bug where user changes werent reflected until restart
    users_db = load_users()

    # NS: External authentication support - Feb 2026
    # Check for explicit provider_id or user's auth_source
    provider_id = data.get('provider_id')
    auth_result = None
    used_provider = None
    used_provider_type = None

    if username in users_db:
        user = users_db[username]

        # check user is enabled
        if not user.get('enabled', True):
            logging.warning(f"Login attempt for disabled user: {username} from {client_ip}")
            return jsonify({'error': 'Account is disabled'}), 401

        auth_source = user.get('auth_source', 'local')

        if provider_id:
            # Explicit provider requested - use it
            if provider_id.startswith('ldap'):
                auth_result = authenticate_with_ldap(provider_id, username, password)
                if auth_result.get('success'):
                    used_provider = provider_id
                    db = get_db()
                    provider = db.get_auth_provider(provider_id)
                    used_provider_type = provider.get('type') if provider else 'ldap_ad'
        elif auth_source != 'local' and ':' in auth_source:
            # User has external auth source (e.g., "ldap_ad:provider123")
            parts = auth_source.split(':', 1)
            if len(parts) == 2:
                ext_type, ext_provider_id = parts
                if ext_type in ('ldap_ad', 'ldap_openldap'):
                    auth_result = authenticate_with_ldap(ext_provider_id, username, password)
                    if auth_result.get('success'):
                        used_provider = ext_provider_id
                        used_provider_type = ext_type
                    elif not auth_result.get('success'):
                        # External auth failed
                        logging.warning(f"External auth failed for user: {username} from {client_ip}")
                        locked = record_failed_attempt(username)
                        if locked:
                            return jsonify({
                                'error': f'Too many failed attempts. Try again in {lockout_time} seconds.',
                                'locked': True,
                                'retry_after': lockout_time
                            }), 429
                        return jsonify({'error': 'Invalid credentials'}), 401
        else:
            # Local authentication
            if not verify_password(password, user['password_salt'], user['password_hash']):
                logging.warning(f"Failed login attempt for user: {username} from {client_ip}")
                locked = record_failed_attempt(username)
                if locked:
                    return jsonify({
                        'error': f'Too many failed attempts. Try again in {lockout_time} seconds.',
                        'locked': True,
                        'retry_after': lockout_time
                    }), 429
                return jsonify({'error': 'Invalid credentials'}), 401
    else:
        # User not found locally - try enabled LDAP providers
        ldap_providers = get_enabled_ldap_providers()

        for provider in ldap_providers:
            auth_result = authenticate_with_ldap(provider['id'], username, password)
            if auth_result.get('success'):
                used_provider = provider['id']
                used_provider_type = provider['type']
                break

        if not auth_result or not auth_result.get('success'):
            # No LDAP provider authenticated the user
            logging.warning(f"Login attempt for unknown user: {username} from {client_ip}")
            locked = record_failed_attempt()
            if locked:
                return jsonify({
                    'error': f'Too many failed attempts. Try again in {lockout_time} seconds.',
                    'locked': True,
                    'retry_after': lockout_time
                }), 429
            return jsonify({'error': 'Invalid credentials'}), 401

        # Handle auto-provisioning for new external user
        db = get_db()
        provider = db.get_auth_provider(used_provider)

        if provider and provider.get('auto_create_users'):
            success = create_external_user(
                username=username,
                provider_id=used_provider,
                external_id=auth_result.get('external_id') or auth_result.get('user_dn'),
                attributes=auth_result.get('attributes', {}),
                groups=auth_result.get('groups', []),
                provider_type=used_provider_type
            )
            if success:
                users_db = load_users()  # Reload to get new user
            else:
                return jsonify({'error': 'Failed to create user'}), 500
        else:
            return jsonify({'error': 'User not provisioned in PegaProx'}), 401

    # Sync user from external provider if authenticated externally
    if used_provider and auth_result:
        sync_user_from_external(username, used_provider, auth_result)
        users_db = load_users()  # Reload to get updated role

    user = users_db[username]
    
    # check 2FA is required
    if user.get('totp_enabled') and user.get('totp_secret'):
        if not totp_code:
            # Return that 2FA is required
            return jsonify({
                'requires_2fa': True,
                'message': '2FA code required'
            }), 200
        
        # Verify TOTP code
        if TOTP_AVAILABLE:
            totp = pyotp.TOTP(user['totp_secret'])
            if not totp.verify(totp_code):
                logging.warning(f"Invalid 2FA code for user: {username} from {client_ip}")
                locked = record_failed_attempt(username)
                if locked:
                    return jsonify({
                        'error': f'Too many failed attempts. Try again in {lockout_time} seconds.',
                        'locked': True,
                        'retry_after': lockout_time
                    }), 429
                return jsonify({'error': 'Invalid 2FA code'}), 401
        else:
            return jsonify({'error': '2FA is enabled but pyotp is not installed on server'}), 500
    
    # Clear failed attempts on successful login (both IP and username)
    if client_ip in login_attempts_by_ip:
        del login_attempts_by_ip[client_ip]
    if username in login_attempts_by_user:
        del login_attempts_by_user[username]
    
    # NS: Auto-migrate password to Argon2id if using old PBKDF2 format - Jan 2026
    if needs_password_rehash(user.get('password_salt', ''), user.get('password_hash', '')):
        try:
            new_salt, new_hash = hash_password(password)
            user['password_salt'] = new_salt
            user['password_hash'] = new_hash
            save_users(users_db)
            logging.info(f"Migrated password for user '{username}' to Argon2id (Military Grade)")
        except Exception as e:
            logging.warning(f"Failed to migrate password for {username}: {e}")
    
    # Create session
    session_id = create_session(username, user['role'])
    
    # Update last login
    user['last_login'] = datetime.now().isoformat()
    save_users(users_db)
    
    logging.info(f"User '{username}' logged in successfully")
    log_audit(username, 'user.login', f"User logged in" + (" (with 2FA)" if user.get('totp_enabled') else ""))
    
    # Auto-allow this origin for CORS (Open Source convenience)
    # Only authenticated users from valid origins get added
    origin = request.headers.get('Origin')
    if origin:
        add_allowed_origin(origin)
    
    response = jsonify({
        'success': True,
        'user': {
            'username': username,
            'role': user['role'],
            'display_name': user.get('display_name', username),
            'email': user.get('email', ''),
            'totp_enabled': user.get('totp_enabled', False),
            'theme': user.get('theme', ''),
            'language': user.get('language', ''),
            'ui_layout': user.get('ui_layout', 'modern')
        },
        'session_id': session_id,
        # NS: Security warning if using default password
        'security_warning': 'DEFAULT_PASSWORD' if (user['role'] == ROLE_ADMIN and password == 'admin') else None
    })
    
    # Set session cookie with security flags
    # NS: Secure flag only when using HTTPS (important for production!)
    is_secure = request.is_secure or request.headers.get('X-Forwarded-Proto') == 'https'
    response.set_cookie(
        'session_id', 
        session_id, 
        httponly=True,       # JS cant access this cookie
        samesite='Strict',   # CSRF protection
        secure=is_secure,    # only send over HTTPS
        max_age=get_session_timeout()
    )
    
    return response

@app.route('/api/auth/logout', methods=['POST'])
def auth_logout():
    """Logout user and invalidate session"""
    session_id = request.headers.get('X-Session-ID') or request.cookies.get('session_id')
    
    if session_id:
        session = validate_session(session_id)
        if session:
            logging.info(f"User '{session['user']}' logged out")
            log_audit(session['user'], 'user.logout', f"User logged out")
        invalidate_session(session_id)
    
    response = jsonify({'success': True})
    response.delete_cookie('session_id')
    return response


# ============================================
# OAuth2/OIDC Flow Endpoints
# NS: Feb 2026 - Support for external identity providers
# ============================================

# Store OIDC state temporarily (in production, use Redis or database)
_oidc_states = {}

@app.route('/api/auth/oidc/<provider_id>/authorize', methods=['GET'])
def oidc_authorize(provider_id):
    """Start OAuth2/OIDC authorization flow

    Redirects user to the identity provider for authentication.
    """
    if not OIDC_AVAILABLE:
        return jsonify({'error': 'OIDC not available - install authlib'}), 500

    try:
        db = get_db()
        provider = db.get_auth_provider(provider_id)

        if not provider or not provider.get('enabled'):
            return jsonify({'error': 'Provider not found or disabled'}), 404

        if provider.get('type') != AUTH_PROVIDER_OIDC:
            return jsonify({'error': 'Provider is not OIDC type'}), 400

        config = provider.get('config', {})
        authenticator = OIDCAuthenticator(config, provider_id)

        # Build redirect URI
        # Use request host to support different access methods
        scheme = 'https' if (request.is_secure or request.headers.get('X-Forwarded-Proto') == 'https') else 'http'
        host = request.headers.get('X-Forwarded-Host') or request.host
        redirect_uri = f"{scheme}://{host}/api/auth/oidc/{provider_id}/callback"

        # Get authorization URL with PKCE
        auth_url, state, code_verifier = authenticator.get_authorization_url(redirect_uri)

        if not auth_url:
            return jsonify({'error': 'Failed to generate authorization URL'}), 500

        # Store state and code_verifier temporarily
        _oidc_states[state] = {
            'provider_id': provider_id,
            'code_verifier': code_verifier,
            'redirect_uri': redirect_uri,
            'created_at': time.time()
        }

        # Clean up old states (older than 5 minutes)
        current_time = time.time()
        expired = [s for s, v in _oidc_states.items() if current_time - v['created_at'] > 300]
        for s in expired:
            del _oidc_states[s]

        # Redirect to IdP
        from flask import redirect
        return redirect(auth_url)

    except Exception as e:
        logging.error(f"OIDC authorize error: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/auth/oidc/<provider_id>/callback', methods=['GET'])
def oidc_callback(provider_id):
    """Handle OAuth2/OIDC callback from identity provider

    Exchanges authorization code for tokens and creates session.
    """
    from flask import redirect

    if not OIDC_AVAILABLE:
        return redirect('/?error=oidc_unavailable')

    try:
        code = request.args.get('code')
        state = request.args.get('state')
        error = request.args.get('error')
        error_description = request.args.get('error_description', '')

        if error:
            logging.warning(f"OIDC error from provider {provider_id}: {error} - {error_description}")
            return redirect(f'/?error={error}&error_description={url_quote(error_description)}')

        if not code or not state:
            return redirect('/?error=missing_code_or_state')

        # Validate state
        if state not in _oidc_states:
            logging.warning(f"OIDC invalid state for provider {provider_id}")
            return redirect('/?error=invalid_state')

        state_data = _oidc_states.pop(state)

        if state_data['provider_id'] != provider_id:
            logging.warning(f"OIDC state provider mismatch")
            return redirect('/?error=provider_mismatch')

        db = get_db()
        provider = db.get_auth_provider(provider_id)

        if not provider or not provider.get('enabled'):
            return redirect('/?error=provider_disabled')

        config = provider.get('config', {})
        authenticator = OIDCAuthenticator(config, provider_id)

        # Exchange code for tokens
        tokens = authenticator.exchange_code(
            code,
            state_data['redirect_uri'],
            state_data['code_verifier']
        )

        if 'error' in tokens:
            logging.error(f"OIDC token exchange failed: {tokens['error']}")
            return redirect(f"/?error=token_exchange_failed")

        # Get user info
        access_token = tokens.get('access_token')
        user_info = authenticator.get_user_info(access_token)

        if not user_info:
            # Try to get info from ID token
            id_token = tokens.get('id_token')
            if id_token:
                user_info = authenticator.validate_id_token(id_token)

        if not user_info or not user_info.get('sub'):
            logging.error("OIDC: Could not get user info")
            return redirect('/?error=no_user_info')

        # Extract user attributes using configured claims
        claims = config.get('claims', {})
        external_id = user_info.get('sub')
        username_claim = claims.get('username', 'preferred_username')
        groups_claim = claims.get('groups', 'groups')

        raw_username = user_info.get(username_claim) or user_info.get('preferred_username') or user_info.get('email', '').split('@')[0]
        username = sanitize_identifier(raw_username.lower(), max_length=64)
        groups = user_info.get(groups_claim, [])

        if not isinstance(groups, list):
            groups = [groups] if groups else []

        # Check if external ID is already linked to a user
        existing_username = db.find_user_by_external_id(provider_id, external_id)
        if existing_username:
            username = existing_username

        # Load or create user
        users_db = load_users()

        if username not in users_db:
            # Auto-provision if enabled
            if provider.get('auto_create_users'):
                success = create_external_user(
                    username=username,
                    provider_id=provider_id,
                    external_id=external_id,
                    attributes={
                        'username': raw_username,
                        'email': user_info.get('email', ''),
                        'display_name': user_info.get('name', raw_username)
                    },
                    groups=groups,
                    provider_type=AUTH_PROVIDER_OIDC
                )
                if not success:
                    return redirect('/?error=user_creation_failed')
                users_db = load_users()
            else:
                logging.warning(f"OIDC: User {username} not provisioned and auto-create disabled")
                return redirect('/?error=user_not_provisioned')

        user = users_db[username]

        # Check user is enabled
        if not user.get('enabled', True):
            return redirect('/?error=account_disabled')

        # Sync user from OIDC
        sync_user_from_external(username, provider_id, {
            'external_id': external_id,
            'groups': groups,
            'attributes': user_info
        })

        # Reload user to get updated role
        users_db = load_users()
        user = users_db[username]

        # Create session
        session_id = create_session(username, user['role'])

        # Update last login
        user['last_login'] = datetime.now().isoformat()
        save_users(users_db)

        logging.info(f"User '{username}' logged in via OIDC provider {provider_id}")
        log_audit(username, 'user.login', f"User logged in via OIDC ({provider.get('name', provider_id)})")

        # Redirect to app with session cookie
        response = make_response(redirect('/'))
        is_secure = request.is_secure or request.headers.get('X-Forwarded-Proto') == 'https'
        response.set_cookie(
            'session_id',
            session_id,
            httponly=True,
            samesite='Strict',
            secure=is_secure,
            max_age=get_session_timeout()
        )

        return response

    except Exception as e:
        logging.error(f"OIDC callback error: {e}")
        return redirect(f'/?error=callback_error')


@app.route('/api/auth/check', methods=['GET'])
def auth_check():
    """Check if current session is valid"""
    session_id = request.headers.get('X-Session-ID') or request.cookies.get('session_id')
    
    session = validate_session(session_id)
    if not session:
        return jsonify({'authenticated': False}), 401
    
    # Get user info - always fresh from database
    users_db = load_users()
    user = users_db.get(session['user'], {})
    
    logging.debug(f"auth_check for {session['user']}: ui_layout = {user.get('ui_layout')}")
    
    # LW: Check password expiry status - Dec 2025
    password_expiry = None
    settings = load_server_settings()
    
    # Check if user should be subject to password expiry
    # MK: admins are exempt by default, but can opt-in via settings
    is_admin = session['role'] == ROLE_ADMIN
    include_admins = settings.get('password_expiry_include_admins', False)
    should_check_expiry = settings.get('password_expiry_enabled') and (not is_admin or include_admins)
    
    if should_check_expiry:
        expiry_days = settings.get('password_expiry_days', 90)
        warning_days = settings.get('password_expiry_warning_days', 14)
        
        changed_at = user.get('password_changed_at')
        if changed_at:
            try:
                changed_date = datetime.fromisoformat(changed_at.replace('Z', '+00:00'))
                # handle naive datetime
                if changed_date.tzinfo:
                    changed_date = changed_date.replace(tzinfo=None)
                days_since = (datetime.now() - changed_date).days
                days_until_expiry = expiry_days - days_since
                
                password_expiry = {
                    'enabled': True,
                    'days_until_expiry': days_until_expiry,
                    'expired': days_until_expiry <= 0,
                    'warning': days_until_expiry <= warning_days and days_until_expiry > 0,
                    'expiry_days': expiry_days
                }
            except:
                pass  # couldn't parse date, skip
        else:
            # no password_changed_at means old user, treat as expired
            password_expiry = {
                'enabled': True,
                'days_until_expiry': 0,
                'expired': True,
                'warning': False,
                'expiry_days': expiry_days
            }
    
    # Get default theme from settings
    default_theme = settings.get('default_theme', 'proxmoxDark')
    
    return jsonify({
        'authenticated': True,
        'session_id': session_id,  # MK: Return session_id for WebSocket auth
        'user': {
            'username': session['user'],
            'role': session['role'],
            'display_name': user.get('display_name', session['user']),
            'email': user.get('email', ''),
            'theme': user.get('theme', '') or default_theme,
            'language': user.get('language', ''),
            'ui_layout': user.get('ui_layout', 'modern')
        },
        'password_expiry': password_expiry,
        'default_theme': default_theme
    })


@app.route('/api/auth/validate', methods=['GET'])
def auth_validate():
    """Simple session validation for WebSocket auth (shell, VNC)
    
    MK: Shell/VNC WebSocket needs a simple endpoint to validate session
    Returns 200 if valid, 401 if not
    """
    # Check both cookie (sent via requests.get with cookies=) and header
    session_id = request.cookies.get('session') or request.cookies.get('session_id') or request.headers.get('X-Session-ID')
    
    if not session_id:
        return jsonify({'valid': False, 'error': 'No session'}), 401
    
    session = validate_session(session_id)
    if not session:
        return jsonify({'valid': False, 'error': 'Invalid session'}), 401
    
    return jsonify({
        'valid': True,
        'user': session['user'],
        'role': session['role']
    })


@app.route('/api/internal/cluster-creds/<cluster_id>', methods=['GET'])
def get_cluster_creds_internal(cluster_id):
    """Internal endpoint for shell/VNC WebSocket to get node connection info
    
    MK: Returns node IPs for SSH connections
    For single-node setups, we use the cluster host directly
    """
    # Check session from cookie
    session_id = request.cookies.get('session') or request.cookies.get('session_id')
    
    if not session_id:
        return jsonify({'error': 'No session'}), 401
    
    session = validate_session(session_id)
    if not session:
        return jsonify({'error': 'Invalid session'}), 401
    
    # Check if cluster exists
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    # Check cluster access for this user
    users_db = load_users()
    user_data = users_db.get(session['user'], {})
    is_admin = user_data.get('role') == ROLE_ADMIN
    user_clusters = user_data.get('clusters', [])
    
    if not is_admin and user_clusters and cluster_id not in user_clusters:
        return jsonify({'error': 'Access denied to this cluster'}), 403
    
    # Get node IPs - the cluster_host is our reliable fallback
    node_ips = {}
    cluster_host = mgr.current_host or mgr.config.host
    
    logging.info(f"[CLUSTER-CREDS] Getting node IPs for cluster {cluster_id}, host={cluster_host}")
    
    try:
        host = cluster_host
        
        # Method 1: Get from Proxmox cluster status API (has IPs for clustered nodes)
        status_url = f"https://{host}:8006/api2/json/cluster/status"
        r = mgr._create_session().get(status_url, timeout=10)
        
        if r.status_code == 200:
            status_data = r.json().get('data', [])
            logging.info(f"[CLUSTER-CREDS] Cluster status returned {len(status_data)} items")
            for item in status_data:
                if item.get('type') == 'node':
                    node_name = item.get('name', '')
                    node_ip = item.get('ip')
                    logging.info(f"[CLUSTER-CREDS] Cluster status node: {node_name}, ip={node_ip}")
                    if node_name and node_ip:
                        # Store with original case and lowercase for matching
                        node_ips[node_name] = node_ip
                        node_ips[node_name.lower()] = node_ip
        
        # Method 2: Get all nodes and ensure they have IPs
        resources = mgr.get_cluster_resources()
        if resources.get('success'):
            for node in resources.get('nodes', []):
                node_name = node.get('node', '')
                node_lower = node_name.lower()
                
                # Already have IP?
                if node_lower in [k.lower() for k in node_ips.keys() if node_ips.get(k)]:
                    continue
                    
                logging.info(f"[CLUSTER-CREDS] Node {node_name} needs IP lookup")
                
                # Try network config API
                try:
                    net_url = f"https://{host}:8006/api2/json/nodes/{node_name}/network"
                    r = mgr._create_session().get(net_url, timeout=5)
                    if r.status_code == 200:
                        for iface in r.json().get('data', []):
                            # Look for interface with IP
                            iface_type = iface.get('type', '')
                            addr = iface.get('address', '')
                            cidr = iface.get('cidr', '')
                            
                            # Extract IP from CIDR if no address
                            if not addr and cidr:
                                addr = cidr.split('/')[0]
                            
                            if addr and iface_type in ['bridge', 'eth', 'bond', 'OVSBridge', 'vlan']:
                                node_ips[node_name] = addr
                                node_ips[node_lower] = addr
                                logging.info(f"[CLUSTER-CREDS] Node {node_name} IP from network: {addr}")
                                break
                except Exception as e:
                    logging.warning(f"[CLUSTER-CREDS] Network API failed for {node_name}: {e}")
                
                # Fallback: use cluster host (the host we connect to Proxmox API through)
                if node_name not in node_ips:
                    node_ips[node_name] = cluster_host
                    node_ips[node_lower] = cluster_host
                    logging.info(f"[CLUSTER-CREDS] Node {node_name} using cluster host: {cluster_host}")
                        
    except Exception as e:
        logging.error(f"[CLUSTER-CREDS] Error getting node IPs: {e}")
    
    # Final fallback: if no nodes found, use cluster host
    if not node_ips:
        node_ips['_default'] = cluster_host
        logging.info(f"[CLUSTER-CREDS] No nodes found, using default: {cluster_host}")
    
    return jsonify({
        'host': cluster_host,
        'user': mgr.config.user,
        'password': mgr.config.password,
        'node_ips': node_ips
    })


@app.route('/api/auth/change-password', methods=['POST'])
@require_auth()
def auth_change_password():
    """Change current user's password
    
    NS: Security feature - invalidates all other sessions after password change
    This way if someone stole your session, changing password kicks them out
    """
    global users_db
    
    data = request.get_json()
    current_password = data.get('current_password', '')
    new_password = data.get('new_password', '')
    
    if not current_password or not new_password:
        return jsonify({'error': 'Current and new password required'}), 400
    
    # Validate password policy
    is_valid, error_msg = validate_password_policy(new_password)
    if not is_valid:
        return jsonify({'error': error_msg}), 400
    
    username = request.session['user']
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    user = users_db[username]
    
    # Verify current password
    if not verify_password(current_password, user['password_salt'], user['password_hash']):
        log_audit(username, 'user.password_change_failed', 'Incorrect current password')
        return jsonify({'error': 'Current password is incorrect'}), 401
    
    # Update password
    salt, password_hash = hash_password(new_password)
    user['password_salt'] = salt
    user['password_hash'] = password_hash
    user['password_changed_at'] = datetime.now().isoformat()  # LW: reset expiry timer
    
    # Mark admin initialized if this is the default admin
    if user.get('is_default'):
        user['is_default'] = False
        mark_admin_initialized()
    
    save_users(users_db)
    
    # Invalidate all other sessions for this user (keep current session)
    current_session_id = request.cookies.get('session_id') or request.headers.get('X-Session-ID')
    sessions_removed = invalidate_all_user_sessions(username, except_session=current_session_id)
    
    logging.info(f"User '{username}' changed their password")
    log_audit(username, 'user.password_changed', f"Password changed, {sessions_removed} other sessions invalidated")
    
    return jsonify({'success': True, 'sessions_invalidated': sessions_removed})


# ============================================
# TOTP 2FA API Routes
# ============================================

@app.route('/api/auth/2fa/setup', methods=['POST'])
@require_auth()
def setup_2fa():
    """Generate TOTP secret and QR code for 2FA setup"""
    global users_db
    
    if not TOTP_AVAILABLE:
        return jsonify({'error': '2FA not available. Please install pyotp and qrcode: pip install pyotp qrcode[pil]'}), 500
    
    username = request.session['user']
    logging.info(f"2FA setup requested for user: {username}")  # MK: Debug
    users_db = load_users()
    
    if username not in users_db:
        logging.warning(f"2FA setup failed - user not found: {username}")
        return jsonify({'error': 'User not found'}), 404
    
    user = users_db[username]
    
    # Generate new secret
    secret = pyotp.random_base32()
    
    # Store pending secret (not activated yet)
    user['totp_pending_secret'] = secret
    save_users(users_db)
    logging.info(f"2FA setup: saved pending secret for user {username}")  # MK: Debug
    
    # Generate provisioning URI
    totp = pyotp.TOTP(secret)
    uri = totp.provisioning_uri(name=username, issuer_name='PegaProx')
    
    # Generate QR code as base64
    qr = qrcode.QRCode(version=1, box_size=10, border=5)
    qr.add_data(uri)
    qr.make(fit=True)
    img = qr.make_image(fill_color="black", back_color="white")
    
    buffer = io.BytesIO()
    img.save(buffer, format='PNG')
    qr_base64 = base64.b64encode(buffer.getvalue()).decode()
    
    return jsonify({
        'secret': secret,
        'qr_code': f'data:image/png;base64,{qr_base64}',
        'uri': uri
    })


@app.route('/api/auth/2fa/verify', methods=['POST'])
@require_auth()
def verify_2fa_setup():
    """Verify TOTP code and activate 2FA"""
    global users_db
    
    if not TOTP_AVAILABLE:
        return jsonify({'error': '2FA not available'}), 500
    
    data = request.get_json()
    code = data.get('code', '') if data else ''
    
    if not code:
        logging.warning("2FA verify: no code provided")  # MK: Debug
        return jsonify({'error': 'TOTP code required'}), 400
    
    username = request.session['user']
    logging.info(f"2FA verify requested for user: {username}, code length: {len(code)}")  # MK: Debug
    users_db = load_users()
    
    if username not in users_db:
        logging.warning(f"2FA verify: user not found: {username}")
        return jsonify({'error': 'User not found'}), 404
    
    user = users_db[username]
    pending_secret = user.get('totp_pending_secret')
    
    if not pending_secret:
        logging.warning(f"2FA verify: no pending secret for user {username}. User keys: {list(user.keys())}")  # MK: Debug
        return jsonify({'error': 'No pending 2FA setup'}), 400
    
    # Verify the code
    totp = pyotp.TOTP(pending_secret)
    if not totp.verify(code):
        return jsonify({'error': 'Invalid TOTP code'}), 401
    
    # Activate 2FA
    user['totp_secret'] = pending_secret
    user['totp_enabled'] = True
    del user['totp_pending_secret']
    save_users(users_db)
    
    logging.info(f"User '{username}' enabled 2FA")
    log_audit(username, '2fa.enabled', "User enabled 2FA")
    
    return jsonify({'success': True, 'message': '2FA erfolgreich aktiviert'})


@app.route('/api/auth/2fa/disable', methods=['POST'])
@require_auth()
def disable_2fa():
    """Disable 2FA for current user"""
    global users_db
    
    data = request.get_json()
    password = data.get('password', '')
    
    if not password:
        return jsonify({'error': 'Password required to disable 2FA'}), 400
    
    username = request.session['user']
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    user = users_db[username]
    
    # Verify password
    if not verify_password(password, user['password_salt'], user['password_hash']):
        return jsonify({'error': 'Invalid password'}), 401
    
    # Disable 2FA
    user['totp_enabled'] = False
    user.pop('totp_secret', None)
    user.pop('totp_pending_secret', None)
    save_users(users_db)
    
    logging.info(f"User '{username}' disabled 2FA")
    log_audit(username, '2fa.disabled', "User disabled 2FA")
    
    return jsonify({'success': True, 'message': '2FA disabled'})


@app.route('/api/auth/2fa/status', methods=['GET'])
@require_auth()
def get_2fa_status():
    """Get 2FA status for current user"""
    username = request.session['user']
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    user = users_db[username]
    
    return jsonify({
        'enabled': user.get('totp_enabled', False),
        'available': TOTP_AVAILABLE
    })


# =====================================================
# USER PREFERENCES - LW
# Per-user settings (theme, language)
# =====================================================

@app.route('/api/user/preferences', methods=['GET'])
@require_auth()
def get_user_preferences():
    """Get current user's preferences (theme, language, ui_layout)"""
    username = request.session['user']
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    user = users_db[username]
    settings = load_server_settings()
    default_theme = settings.get('default_theme', 'proxmoxDark')
    
    return jsonify({
        'theme': user.get('theme', '') or default_theme,
        'language': user.get('language', ''),
        'ui_layout': user.get('ui_layout', 'modern'),
        'default_theme': default_theme
    })


@app.route('/api/user/preferences', methods=['PUT'])
@require_auth()
def update_user_preferences():
    """Update current user's preferences (theme, language, ui_layout)"""
    global users_db
    
    username = request.session['user']
    data = request.get_json() or {}
    
    logging.info(f"update_user_preferences: user={username}, data={data}")
    
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    user = users_db[username]
    
    logging.info(f"update_user_preferences: user before update: ui_layout={user.get('ui_layout')}")
    
    # Only allow specific fields to be updated
    allowed_themes = [
        'proxmoxDark', 'proxmoxLight', 'midnight', 'forest', 'rose', 'ocean',
        'highContrast', 'dracula', 'nord', 'monokai', 'matrix', 'sunset',
        'cyberpunk', 'github', 'solarizedDark', 'gruvbox',
        'corporateDark', 'corporateLight', 'enterpriseBlue'  # NS: Corporate themes
    ]
    
    if 'theme' in data:
        theme = data['theme']
        if theme == '' or theme in allowed_themes:
            user['theme'] = theme
        else:
            return jsonify({'error': f'Invalid theme: {theme}'}), 400
    
    if 'language' in data:
        # Allow common language codes
        lang = data['language']
        if lang == '' or lang in ['en', 'de', 'es', 'fr', 'it', 'pt', 'nl', 'pl', 'ru', 'zh', 'ja', 'ko']:
            user['language'] = lang
        else:
            return jsonify({'error': f'Invalid language: {lang}'}), 400
    
    # NS: UI Layout - Jan 2026
    if 'ui_layout' in data:
        layout = data['ui_layout']
        if layout in ['modern', 'classic']:
            user['ui_layout'] = layout
            logging.info(f"update_user_preferences: Setting ui_layout to '{layout}' for user '{username}'")
        else:
            return jsonify({'error': f'Invalid layout: {layout}'}), 400
    
    # Save only this user, not all users
    db = get_db()
    db.save_user(username, user)
    
    logging.info(f"User '{username}' updated preferences: theme={user.get('theme')}, language={user.get('language')}, ui_layout={user.get('ui_layout')}")
    log_audit(username, 'user.preferences_updated', f"Updated preferences: theme={user.get('theme')}, layout={user.get('ui_layout')}")
    
    settings = load_server_settings()
    default_theme = settings.get('default_theme', 'proxmoxDark')
    
    return jsonify({
        'success': True,
        'theme': user.get('theme', '') or default_theme,
        'language': user.get('language', ''),
        'ui_layout': user.get('ui_layout', 'modern'),
        'default_theme': default_theme
    })


@app.route('/api/users/<username>/2fa', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def admin_disable_2fa(username):
    """Admin: Disable 2FA for a user"""
    global users_db
    
    username = username.lower()
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    user = users_db[username]
    user['totp_enabled'] = False
    user.pop('totp_secret', None)
    user.pop('totp_pending_secret', None)
    save_users(users_db)
    
    logging.info(f"Admin '{request.session['user']}' disabled 2FA for user '{username}'")
    log_audit(request.session['user'], '2fa.admin_disabled', f"Admin disabled 2FA for user: {username}")
    
    return jsonify({'success': True, 'message': f'2FA for {username} disabled'})


@app.route('/api/users/<username>/password', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def admin_change_password(username):
    """Admin: Change password for any user
    
    MK: Important - this invalidates ALL sessions for the user
    Even if admin is resetting their own password (edge case but possible)
    """
    global users_db
    
    username = username.lower()
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    data = request.get_json()
    new_password = data.get('password', '')
    
    # Validate password policy
    is_valid, error_msg = validate_password_policy(new_password)
    if not is_valid:
        return jsonify({'error': error_msg}), 400
    
    user = users_db[username]
    salt, password_hash = hash_password(new_password)
    user['password_salt'] = salt
    user['password_hash'] = password_hash
    user['password_changed_at'] = datetime.now().isoformat()  # LW: reset expiry
    
    # Mark that admin has been customized (prevents reset on restart)
    if user.get('is_default'):
        user['is_default'] = False
        mark_admin_initialized()
    
    save_users(users_db)
    
    # Invalidate ALL sessions for this user (security: force re-login)
    # NS: use helper function which also persists the change
    sessions_removed = invalidate_all_user_sessions(username)
    
    logging.info(f"Admin '{request.session['user']}' changed password for user '{username}'")
    log_audit(request.session['user'], 'user.password_reset', f"Admin reset password for user: {username} ({sessions_removed} sessions invalidated)")
    
    return jsonify({'success': True, 'message': f'Password for {username} changed', 'sessions_invalidated': sessions_removed})


# ============================================
# Global Datacenter API (Cross-Cluster)
# datacenter-wide features (like vCenter)
# ============================================

# User favorites storage
FAVORITES_FILE = os.path.join(CONFIG_DIR, 'user_favorites.json')  # Legacy

def load_favorites():
    """Load user favorites from SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM user_favorites')
        
        favorites = {}
        for row in cursor.fetchall():
            username = row['username']
            if username not in favorites:
                favorites[username] = []
            favorites[username].append({
                'cluster_id': row['cluster_id'],
                'vmid': row['vmid'],
                'vm_type': row['vm_type'],
                'vm_name': row['vm_name'],
            })
        
        return favorites
    except Exception as e:
        logging.error(f"Error loading favorites from database: {e}")
        # Legacy fallback
        try:
            if os.path.exists(FAVORITES_FILE):
                with open(FAVORITES_FILE, 'r') as f:
                    return json.load(f)
        except:
            pass
    return {}


def save_favorites(favorites):
    """Save user favorites to SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        # Clear existing favorites
        cursor.execute('DELETE FROM user_favorites')
        
        now = datetime.now().isoformat()
        for username, user_favs in favorites.items():
            for fav in user_favs:
                cursor.execute('''
                    INSERT INTO user_favorites 
                    (username, cluster_id, vmid, vm_type, vm_name, added_at)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    username,
                    fav.get('cluster_id'),
                    fav.get('vmid'),
                    fav.get('vm_type', fav.get('type', '')),
                    fav.get('vm_name', fav.get('name', '')),
                    now
                ))
        
        db.conn.commit()
    except Exception as e:
        logging.error(f"Error saving favorites: {e}")


@app.route('/api/global/search', methods=['GET'])
@require_auth()
def global_search():
    """Search across all clusters for VMs, containers, and nodes
    
    Query params:
    - q: Search query (name, vmid, ip, node name)
    - type: Filter by type (vm, ct, node, all) - default: all
    
    Returns matches from all accessible clusters
    
    LW: This is one of the most used features - people love being able
    to find a VM without knowing which cluster its on.
    """
    query = request.args.get('q', '').lower().strip()
    search_type = request.args.get('type', 'all').lower()
    
    if not query or len(query) < 2:
        return jsonify({'error': 'Search query must be at least 2 characters'}), 400
    
    results = []
    user = request.session.get('user', '')
    users_db = load_users()
    user_data = users_db.get(user, {})
    user_data['username'] = user
    is_admin = user_data.get('role') == ROLE_ADMIN
    
    # Get user's accessible clusters (respect RBAC)
    user_clusters = user_data.get('clusters', [])
    
    for cluster_id, mgr in cluster_managers.items():
        # Check cluster access - NS: important for multi-tenant setups
        if not is_admin and user_clusters and cluster_id not in user_clusters:
            continue
        
        if not mgr.is_connected:
            continue
        
        cluster_name = mgr.config.name or cluster_id
        
        # Search VMs and Containers
        if search_type in ['all', 'vm', 'ct']:
            try:
                resources = mgr.get_vm_resources()
                for r in resources:
                    name = (r.get('name') or '').lower()
                    vmid = str(r.get('vmid', ''))
                    node = (r.get('node') or '').lower()
                    ip = (r.get('ip') or '').lower()
                    
                    # Match against query
                    if (query in name or 
                        query in vmid or 
                        query in node or
                        query in ip):
                        
                        # Type filter
                        vm_type = r.get('type', 'qemu')
                        if search_type == 'vm' and vm_type != 'qemu':
                            continue
                        if search_type == 'ct' and vm_type != 'lxc':
                            continue
                        
                        results.append({
                            'type': 'vm' if vm_type == 'qemu' else 'ct',
                            'cluster_id': cluster_id,
                            'cluster_name': cluster_name,
                            'vmid': r.get('vmid'),
                            'name': r.get('name'),
                            'node': r.get('node'),
                            'status': r.get('status'),
                            'ip': r.get('ip'),
                            'cpu': r.get('cpu'),
                            'mem': r.get('mem'),
                            'maxmem': r.get('maxmem'),
                        })
            except Exception as e:
                logging.debug(f"Error searching cluster {cluster_id}: {e}")
        
        # Search Nodes
        if search_type in ['all', 'node']:
            try:
                for node_name, node_data in (mgr.nodes or {}).items():
                    if query in node_name.lower():
                        results.append({
                            'type': 'node',
                            'cluster_id': cluster_id,
                            'cluster_name': cluster_name,
                            'name': node_name,
                            'status': node_data.get('status', 'unknown'),
                            'cpu': node_data.get('cpu'),
                            'mem': node_data.get('mem'),
                            'maxmem': node_data.get('maxmem'),
                        })
            except Exception as e:
                logging.debug(f"Error searching nodes in {cluster_id}: {e}")
    
    # Sort by relevance (exact matches first, then partial)
    def sort_key(r):
        name = (r.get('name') or str(r.get('vmid', ''))).lower()
        if name == query:
            return (0, name)
        elif name.startswith(query):
            return (1, name)
        else:
            return (2, name)
    
    results.sort(key=sort_key)
    
    return jsonify({
        'query': query,
        'count': len(results),
        'results': results[:100]  # Limit to 100 results
    })


@app.route('/api/global/summary', methods=['GET'])
@require_auth()
def global_summary():
    """Get summary statistics across all accessible clusters
    
    Returns aggregate stats for a datacenter-level overview
    """
    try:
        user = request.session.get('user', '')
        users_db = load_users()
        user_data = users_db.get(user, {})
        is_admin = user_data.get('role') == ROLE_ADMIN
        user_clusters = user_data.get('clusters', [])
        
        summary = {
            'clusters': {
                'total': 0,
                'online': 0,
                'offline': 0
            },
            'nodes': {
                'total': 0,
                'online': 0,
                'offline': 0
            },
            'vms': {
                'total': 0,
                'running': 0,
                'stopped': 0,
                'paused': 0
            },
            'containers': {
                'total': 0,
                'running': 0,
                'stopped': 0
            },
            'resources': {
                'cpu_total': 0,
                'cpu_used': 0,
                'mem_total': 0,
                'mem_used': 0,
                'storage_total': 0,
                'storage_used': 0
            },
            'by_cluster': []
        }
        
        for cluster_id, mgr in cluster_managers.items():
            # Check cluster access
            if not is_admin and user_clusters and cluster_id not in user_clusters:
                continue
            
            summary['clusters']['total'] += 1
            
            cluster_stats = {
                'id': cluster_id,
                'name': getattr(mgr.config, 'name', None) or cluster_id,
                'online': mgr.is_connected if mgr else False,
                'nodes': 0,
                'vms': 0,
                'containers': 0
            }
            
            if mgr and mgr.is_connected:
                summary['clusters']['online'] += 1
                
                # Count nodes - safely handle None
                nodes = getattr(mgr, 'nodes', None) or {}
                for node_name, node_data in nodes.items():
                    if not node_data:
                        continue
                    summary['nodes']['total'] += 1
                    cluster_stats['nodes'] += 1
                    
                    if node_data.get('status') == 'online':
                        summary['nodes']['online'] += 1
                        # Aggregate resources from online nodes
                        summary['resources']['cpu_total'] += node_data.get('maxcpu', 0) or 0
                        cpu_val = node_data.get('cpu', 0) or 0
                        maxcpu_val = node_data.get('maxcpu', 0) or 0
                        summary['resources']['cpu_used'] += cpu_val * maxcpu_val
                        summary['resources']['mem_total'] += node_data.get('maxmem', 0) or 0
                        summary['resources']['mem_used'] += node_data.get('mem', 0) or 0
                    else:
                        summary['nodes']['offline'] += 1
                
                # Count VMs
                try:
                    resources = mgr.get_vm_resources() or []
                    for r in resources:
                        if not r:
                            continue
                        if r.get('type') == 'qemu':
                            summary['vms']['total'] += 1
                            cluster_stats['vms'] += 1
                            status = (r.get('status') or '').lower()
                            if status == 'running':
                                summary['vms']['running'] += 1
                            elif status == 'paused':
                                summary['vms']['paused'] += 1
                            else:
                                summary['vms']['stopped'] += 1
                        else:
                            summary['containers']['total'] += 1
                            cluster_stats['containers'] += 1
                            status = (r.get('status') or '').lower()
                            if status == 'running':
                                summary['containers']['running'] += 1
                            else:
                                summary['containers']['stopped'] += 1
                except Exception as e:
                    app.logger.warning(f"Error getting VM resources for {cluster_id}: {e}")
            else:
                summary['clusters']['offline'] += 1
            
            summary['by_cluster'].append(cluster_stats)
        
        return jsonify(summary)
    except Exception as e:
        app.logger.error(f"global_summary error: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/user/favorites', methods=['GET'])
@require_auth()
def get_favorites():
    """Get user's favorite/pinned VMs and nodes"""
    user = request.session.get('user', '')
    favorites = load_favorites()
    
    user_favs = favorites.get(user, {
        'vms': [],      # [{cluster_id, vmid, type}]
        'nodes': [],    # [{cluster_id, node}]
        'clusters': []  # [cluster_id]
    })
    
    return jsonify(user_favs)


@app.route('/api/user/favorites', methods=['POST'])
@require_auth()
def update_favorites():
    """Add or remove a favorite
    
    Body:
    - action: 'add' or 'remove'
    - type: 'vm', 'node', or 'cluster'
    - cluster_id: Cluster ID
    - vmid: (for vm) VM ID
    - vm_type: (for vm) 'qemu' or 'lxc'
    - node: (for node) Node name
    """
    user = request.session.get('user', '')
    data = request.json or {}
    
    action = data.get('action', 'add')
    fav_type = data.get('type')
    cluster_id = data.get('cluster_id')
    
    if not fav_type or not cluster_id:
        return jsonify({'error': 'type and cluster_id required'}), 400
    
    favorites = load_favorites()
    
    if user not in favorites:
        favorites[user] = {'vms': [], 'nodes': [], 'clusters': []}
    
    user_favs = favorites[user]
    
    if fav_type == 'vm':
        vmid = data.get('vmid')
        vm_type = data.get('vm_type', 'qemu')
        if not vmid:
            return jsonify({'error': 'vmid required for vm favorites'}), 400
        
        fav_entry = {'cluster_id': cluster_id, 'vmid': vmid, 'type': vm_type}
        
        if action == 'add':
            # Check if already exists
            if not any(f['cluster_id'] == cluster_id and f['vmid'] == vmid for f in user_favs['vms']):
                user_favs['vms'].append(fav_entry)
        else:
            user_favs['vms'] = [f for f in user_favs['vms'] 
                               if not (f['cluster_id'] == cluster_id and f['vmid'] == vmid)]
    
    elif fav_type == 'node':
        node = data.get('node')
        if not node:
            return jsonify({'error': 'node required for node favorites'}), 400
        
        fav_entry = {'cluster_id': cluster_id, 'node': node}
        
        if action == 'add':
            if not any(f['cluster_id'] == cluster_id and f['node'] == node for f in user_favs['nodes']):
                user_favs['nodes'].append(fav_entry)
        else:
            user_favs['nodes'] = [f for f in user_favs['nodes']
                                 if not (f['cluster_id'] == cluster_id and f['node'] == node)]
    
    elif fav_type == 'cluster':
        if action == 'add':
            if cluster_id not in user_favs['clusters']:
                user_favs['clusters'].append(cluster_id)
        else:
            user_favs['clusters'] = [c for c in user_favs['clusters'] if c != cluster_id]
    
    else:
        return jsonify({'error': 'Invalid type. Use vm, node, or cluster'}), 400
    
    save_favorites(favorites)
    
    return jsonify({'success': True, 'favorites': user_favs})


# ============================================
# VM Tags / Labels
# should have added this sooner tbh
# Tags are stored per-cluster, per-VM in a simple JSON file
# ============================================

TAGS_FILE = os.path.join(CONFIG_DIR, 'vm_tags.json')  # Legacy

def load_vm_tags():
    """Load VM tags from SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM vm_tags')
        
        tags = {}
        for row in cursor.fetchall():
            cluster_id = row['cluster_id']
            vmid = str(row['vmid'])
            
            if cluster_id not in tags:
                tags[cluster_id] = {}
            if vmid not in tags[cluster_id]:
                tags[cluster_id][vmid] = []
            
            tags[cluster_id][vmid].append({
                'name': row['tag_name'],
                'color': row['tag_color'] or TAG_COLORS[hash(row['tag_name']) % len(TAG_COLORS)]
            })
        
        return tags
    except Exception as e:
        logging.error(f"Error loading VM tags from database: {e}")
        # Legacy fallback
        try:
            if os.path.exists(TAGS_FILE):
                with open(TAGS_FILE, 'r') as f:
                    return json.load(f)
        except:
            pass
    return {}


def save_vm_tags(tags):
    """Save VM tags to SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        # Clear existing tags
        cursor.execute('DELETE FROM vm_tags')
        
        # Insert all tags
        for cluster_id, vms in tags.items():
            for vmid, vm_tags in vms.items():
                for tag in vm_tags:
                    tag_name = tag.get('name', tag) if isinstance(tag, dict) else tag
                    tag_color = tag.get('color', '') if isinstance(tag, dict) else ''
                    
                    cursor.execute('''
                        INSERT INTO vm_tags (cluster_id, vmid, tag_name, tag_color)
                        VALUES (?, ?, ?, ?)
                    ''', (cluster_id, int(vmid), tag_name, tag_color))
        
        db.conn.commit()
    except Exception as e:
        logging.error(f"Error saving VM tags: {e}")

# LW: Global tag colors - keeps things consistent
TAG_COLORS = [
    '#ef4444', '#f97316', '#eab308', '#22c55e', '#14b8a6', 
    '#3b82f6', '#8b5cf6', '#ec4899', '#6b7280', '#78716c'
]

@app.route('/api/clusters/<cluster_id>/tags', methods=['GET'])
@require_auth()
def get_cluster_tags(cluster_id):
    """Get all tags used in this cluster
    
    Returns unique tags with their colors and usage count
    """
    tags_db = load_vm_tags()
    cluster_tags = tags_db.get(cluster_id, {})
    
    # Count tag usage
    tag_counts = {}
    for vm_key, vm_tags in cluster_tags.items():
        for tag in vm_tags:
            tag_name = tag.get('name', tag) if isinstance(tag, dict) else tag
            if tag_name not in tag_counts:
                tag_counts[tag_name] = {
                    'name': tag_name,
                    'color': tag.get('color', TAG_COLORS[hash(tag_name) % len(TAG_COLORS)]) if isinstance(tag, dict) else TAG_COLORS[hash(tag_name) % len(TAG_COLORS)],
                    'count': 0
                }
            tag_counts[tag_name]['count'] += 1
    
    return jsonify(list(tag_counts.values()))


@app.route('/api/clusters/<cluster_id>/vms/<vmid>/tags', methods=['GET'])
@require_auth()
def get_vm_tags(cluster_id, vmid):
    """Get tags for a specific VM"""
    tags_db = load_vm_tags()
    cluster_tags = tags_db.get(cluster_id, {})
    vm_tags = cluster_tags.get(str(vmid), [])
    
    return jsonify(vm_tags)


@app.route('/api/clusters/<cluster_id>/vms/<vmid>/tags', methods=['POST'])
@require_auth(perms=['vm.config'])
def update_vm_tags(cluster_id, vmid):
    """Add or update tags for a VM
    
    Body:
    - tags: Array of tag objects [{name: 'prod', color: '#ef4444'}, ...]
    
    Or simple add:
    - tag: Single tag name to add
    - color: Optional color for the tag
    """
    data = request.json or {}
    tags_db = load_vm_tags()
    
    if cluster_id not in tags_db:
        tags_db[cluster_id] = {}
    
    vm_key = str(vmid)
    
    # Full replacement
    if 'tags' in data:
        tags_db[cluster_id][vm_key] = data['tags']
    # Single tag add
    elif 'tag' in data:
        tag_name = data['tag'].strip()
        if not tag_name:
            return jsonify({'error': 'Tag name required'}), 400
        
        current_tags = tags_db[cluster_id].get(vm_key, [])
        
        # Check if tag already exists
        existing = next((t for t in current_tags if (t.get('name') if isinstance(t, dict) else t) == tag_name), None)
        if not existing:
            new_tag = {
                'name': tag_name,
                'color': data.get('color', TAG_COLORS[hash(tag_name) % len(TAG_COLORS)])
            }
            current_tags.append(new_tag)
            tags_db[cluster_id][vm_key] = current_tags
    
    save_vm_tags(tags_db)
    
    user = request.session.get('user', 'system')
    cluster_name = cluster_managers[cluster_id].config.name if cluster_id in cluster_managers else cluster_id
    log_audit(user, 'vm.tags_updated', f"Updated tags for VM {vmid}", cluster=cluster_name)
    
    return jsonify({
        'success': True,
        'tags': tags_db[cluster_id].get(vm_key, [])
    })


@app.route('/api/clusters/<cluster_id>/vms/<vmid>/tags/<tag_name>', methods=['DELETE'])
@require_auth(perms=['vm.config'])
def remove_vm_tag(cluster_id, vmid, tag_name):
    """Remove a tag from a VM"""
    tags_db = load_vm_tags()
    
    if cluster_id not in tags_db:
        return jsonify({'error': 'No tags for this cluster'}), 404
    
    vm_key = str(vmid)
    if vm_key not in tags_db[cluster_id]:
        return jsonify({'error': 'No tags for this VM'}), 404
    
    # Remove the tag
    current_tags = tags_db[cluster_id][vm_key]
    tags_db[cluster_id][vm_key] = [
        t for t in current_tags 
        if (t.get('name') if isinstance(t, dict) else t) != tag_name
    ]
    
    # Cleanup empty entries
    if not tags_db[cluster_id][vm_key]:
        del tags_db[cluster_id][vm_key]
    
    save_vm_tags(tags_db)
    
    return jsonify({'success': True})


@app.route('/api/tags/search', methods=['GET'])
@require_auth()
def search_vms_by_tag():
    """Search VMs across all clusters by tag
    
    Query params:
    - tag: Tag name to search for
    - cluster_id: Optional - limit to specific cluster
    """
    tag_name = request.args.get('tag', '').strip()
    filter_cluster = request.args.get('cluster_id')
    
    if not tag_name:
        return jsonify({'error': 'Tag parameter required'}), 400
    
    tags_db = load_vm_tags()
    results = []
    
    user = request.session.get('user', '')
    users_db = load_users()
    user_data = users_db.get(user, {})
    is_admin = user_data.get('role') == ROLE_ADMIN
    user_clusters = user_data.get('clusters', [])
    
    for cluster_id, cluster_tags in tags_db.items():
        # Check access
        if not is_admin and user_clusters and cluster_id not in user_clusters:
            continue
        if filter_cluster and cluster_id != filter_cluster:
            continue
        
        mgr = cluster_managers.get(cluster_id)
        cluster_name = mgr.config.name if mgr else cluster_id
        
        for vm_key, vm_tags in cluster_tags.items():
            # Check if this VM has the tag
            has_tag = any(
                (t.get('name') if isinstance(t, dict) else t) == tag_name 
                for t in vm_tags
            )
            
            if has_tag:
                # Try to get VM details
                vm_info = {'vmid': vm_key, 'cluster_id': cluster_id, 'cluster_name': cluster_name}
                
                if mgr and mgr.is_connected:
                    try:
                        resources = mgr.get_vm_resources()
                        vm_data = next((r for r in resources if str(r.get('vmid')) == vm_key), None)
                        if vm_data:
                            vm_info.update({
                                'name': vm_data.get('name'),
                                'node': vm_data.get('node'),
                                'status': vm_data.get('status'),
                                'type': vm_data.get('type')
                            })
                    except:
                        pass
                
                vm_info['tags'] = vm_tags
                results.append(vm_info)
    
    return jsonify(results)


# ============================================
# Scheduled VM Actions
# MK: Jan 2026 - Users keep asking for this
# Simple approach: store schedules in JSON, check every minute
# Not using APScheduler because we want to keep dependencies minimal
# ============================================

SCHEDULES_FILE = os.path.join(CONFIG_DIR, 'scheduled_actions.json')
_scheduler_thread = None
_scheduler_running = False

def load_schedules():
    """Load scheduled actions from SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM scheduled_actions')
        
        actions = []
        last_id = 0
        for row in cursor.fetchall():
            if row['id'] > last_id:
                last_id = row['id']
            actions.append({
                'id': row['id'],
                'cluster_id': row['cluster_id'],
                'vmid': row['vmid'],
                'action': row['action'],
                'schedule_type': row['schedule_type'],
                'time': row['schedule_time'],
                'days': json.loads(row['schedule_days'] or '[]'),
                'date': row['schedule_date'],
                'enabled': bool(row['enabled']),
                'last_run': row['last_run'],
                'created_by': row['created_by'],
            })
        
        return {'actions': actions, 'last_id': last_id}
    except Exception as e:
        logging.error(f"Error loading schedules from database: {e}")
        # Legacy fallback
        try:
            if os.path.exists(SCHEDULES_FILE):
                with open(SCHEDULES_FILE, 'r') as f:
                    return json.load(f)
        except:
            pass
    return {'actions': [], 'last_id': 0}


def save_schedules(schedules):
    """Save scheduled actions to SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        # Clear existing schedules
        cursor.execute('DELETE FROM scheduled_actions')
        
        now = datetime.now().isoformat()
        for action in schedules.get('actions', []):
            cursor.execute('''
                INSERT INTO scheduled_actions 
                (id, cluster_id, vmid, action, schedule_type, schedule_time, 
                 schedule_days, schedule_date, enabled, last_run, created_by, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                action.get('id'),
                action.get('cluster_id'),
                action.get('vmid'),
                action.get('action', ''),
                action.get('schedule_type', 'daily'),
                action.get('time', ''),
                json.dumps(action.get('days', [])),
                action.get('date'),
                1 if action.get('enabled', True) else 0,
                action.get('last_run'),
                action.get('created_by'),
                now
            ))
        
        db.conn.commit()
    except Exception as e:
        logging.error(f"Error saving schedules: {e}")


def check_schedules():
    """Check if any scheduled actions need to run
    
    Called every minute by the scheduler thread
    This is deliberately simple - no cron expressions, just specific times
    """
    global _scheduler_running
    
    while _scheduler_running:
        try:
            schedules = load_schedules()
            now = datetime.now()
            current_time = now.strftime('%H:%M')
            current_day = now.strftime('%A').lower()
            current_date = now.strftime('%Y-%m-%d')
            
            modified = False
            
            for action in schedules.get('actions', []):
                if not action.get('enabled', True):
                    continue
                
                should_run = False
                schedule_type = action.get('schedule_type', 'daily')
                schedule_time = action.get('time', '')
                
                # Check if it's time to run
                if schedule_time == current_time:
                    if schedule_type == 'once':
                        # One-time schedule - check date
                        if action.get('date') == current_date:
                            should_run = True
                            action['enabled'] = False  # Disable after running
                            modified = True
                    
                    elif schedule_type == 'daily':
                        should_run = True
                    
                    elif schedule_type == 'weekly':
                        # Check if today is in the selected days
                        days = action.get('days', [])
                        if current_day in days:
                            should_run = True
                    
                    elif schedule_type == 'weekdays':
                        if current_day not in ['saturday', 'sunday']:
                            should_run = True
                    
                    elif schedule_type == 'weekends':
                        if current_day in ['saturday', 'sunday']:
                            should_run = True
                
                if should_run:
                    # Check if we already ran this minute (prevent double execution)
                    last_run = action.get('last_run', '')
                    if last_run == f"{current_date} {current_time}":
                        continue
                    
                    # Execute the action
                    execute_scheduled_action(action)
                    action['last_run'] = f"{current_date} {current_time}"
                    action['run_count'] = action.get('run_count', 0) + 1
                    modified = True
            
            if modified:
                save_schedules(schedules)
            
            # MK: Check for scheduled rolling updates
            try:
                check_scheduled_updates()
            except Exception as e:
                logging.error(f"[SCHEDULER] Scheduled updates check error: {e}")
            
            # Daily cleanup tasks at 03:00
            if current_time == '03:00':
                try:
                    # Cleanup soft-deleted scripts after 20 days
                    cleanup_deleted_scripts()
                    # MK: Cleanup orphaned excluded VMs (VMs that no longer exist)
                    cleanup_orphaned_excluded_vms()
                    logging.info("[SCHEDULER] Daily cleanup completed")
                except Exception as e:
                    logging.error(f"[SCHEDULER] Daily cleanup error: {e}")
        
        except Exception as e:
            logging.error(f"Scheduler error: {e}")
        
        # Sleep for 60 seconds (check every minute)
        for _ in range(60):
            if not _scheduler_running:
                break
            time.sleep(1)


def execute_scheduled_action(action):
    """Execute a scheduled VM action
    
    LW: This is basically the same as the manual action endpoints
    but called from the scheduler
    
    MK: Added rolling_update for automatic node updates
    """
    cluster_id = action.get('cluster_id')
    vmid = action.get('vmid')
    vm_type = action.get('vm_type', 'qemu')
    action_type = action.get('action')
    
    logging.info(f"[SCHEDULER] Executing {action_type} on {vm_type}/{vmid} in {cluster_id}")
    
    if cluster_id not in cluster_managers:
        logging.error(f"[SCHEDULER] Cluster {cluster_id} not found")
        return
    
    mgr = cluster_managers[cluster_id]
    if not mgr.is_connected:
        logging.error(f"[SCHEDULER] Cluster {cluster_id} not connected")
        return
    
    try:
        # MK: Handle rolling_update action type separately
        if action_type == 'rolling_update':
            execute_scheduled_rolling_update(mgr, cluster_id, action)
            return
        
        # Find the node where the VM is running
        resources = mgr.get_vm_resources()
        vm = next((r for r in resources if r.get('vmid') == vmid), None)
        
        if not vm:
            logging.error(f"[SCHEDULER] VM {vmid} not found")
            return
        
        node = vm.get('node')
        host = mgr.current_host or mgr.config.host
        
        # Build the API URL based on action
        if action_type == 'start':
            url = f"https://{host}:8006/api2/json/nodes/{node}/{vm_type}/{vmid}/status/start"
        elif action_type == 'stop':
            url = f"https://{host}:8006/api2/json/nodes/{node}/{vm_type}/{vmid}/status/stop"
        elif action_type == 'shutdown':
            url = f"https://{host}:8006/api2/json/nodes/{node}/{vm_type}/{vmid}/status/shutdown"
        elif action_type == 'reboot':
            url = f"https://{host}:8006/api2/json/nodes/{node}/{vm_type}/{vmid}/status/reboot"
        elif action_type == 'snapshot':
            # Create a snapshot with timestamp
            snap_name = f"scheduled_{datetime.now().strftime('%Y%m%d_%H%M')}"
            url = f"https://{host}:8006/api2/json/nodes/{node}/{vm_type}/{vmid}/snapshot"
            response = mgr._create_session().post(url, data={'snapname': snap_name})
            logging.info(f"[SCHEDULER] Snapshot result: {response.status_code}")
            return
        else:
            logging.error(f"[SCHEDULER] Unknown action: {action_type}")
            return
        
        response = mgr._create_session().post(url)
        logging.info(f"[SCHEDULER] {action_type} result: {response.status_code}")
        
        # Log the action
        log_audit('scheduler', f'scheduled.{action_type}', 
                 f"Scheduled {action_type} executed on VM {vmid} in {cluster_id}")
        
    except Exception as e:
        logging.error(f"[SCHEDULER] Failed to execute {action_type}: {e}")


def execute_scheduled_rolling_update(mgr, cluster_id: str, action: dict):
    """Execute a scheduled rolling update on a cluster
    
    MK: This runs the same rolling update logic but triggered by scheduler
    """
    try:
        # Get update config from action
        config = action.get('config', {})
        include_reboot = config.get('include_reboot', False)
        skip_evacuation = config.get('skip_evacuation', False)
        skip_up_to_date = config.get('skip_up_to_date', True)
        evacuation_timeout = config.get('evacuation_timeout', 1800)
        
        logging.info(f"[SCHEDULER] Starting scheduled rolling update for cluster {cluster_id}")
        logging.info(f"[SCHEDULER] Config: reboot={include_reboot}, skip_evacuation={skip_evacuation}")
        
        # Check if already running
        if hasattr(mgr, '_rolling_update') and mgr._rolling_update and mgr._rolling_update.get('status') == 'running':
            logging.warning(f"[SCHEDULER] Rolling update already in progress, skipping")
            return
        
        # Get nodes
        node_status = mgr.get_node_status()
        nodes_to_update = list(node_status.keys()) if node_status else []
        
        if not nodes_to_update:
            logging.warning(f"[SCHEDULER] No nodes available for update")
            return
        
        # Initialize rolling update state
        mgr._rolling_update = {
            'status': 'running',
            'started_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'include_reboot': include_reboot,
            'skip_up_to_date': skip_up_to_date,
            'skip_evacuation': skip_evacuation,
            'force_all': False,
            'evacuation_timeout': evacuation_timeout,
            'update_timeout': 900,
            'reboot_timeout': 600,
            'nodes': nodes_to_update,
            'current_index': 0,
            'current_node': nodes_to_update[0],
            'current_step': 'starting',
            'completed_nodes': [],
            'skipped_nodes': [],
            'failed_nodes': [],
            'logs': [f"[{time.strftime('%H:%M:%S')}] Scheduled rolling update started"],
            'scheduled': True  # Mark as scheduled
        }
        
        # Run the update in a background thread
        def run_scheduled_update():
            try:
                for idx, node_name in enumerate(nodes_to_update):
                    if not hasattr(mgr, '_rolling_update') or mgr._rolling_update.get('status') != 'running':
                        break
                    
                    mgr._rolling_update['current_index'] = idx
                    mgr._rolling_update['current_node'] = node_name
                    mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Processing {node_name}")
                    
                    # Check for updates if skip_up_to_date
                    if skip_up_to_date:
                        try:
                            mgr.refresh_node_apt(node_name)
                            time.sleep(3)
                            updates = mgr.get_node_apt_updates(node_name)
                            if not updates:
                                mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ⏭ {node_name} up-to-date, skipping")
                                mgr._rolling_update['skipped_nodes'].append(node_name)
                                continue
                        except Exception as e:
                            logging.warning(f"[SCHEDULER] Could not check updates for {node_name}: {e}")
                    
                    # Enter maintenance mode
                    mgr._rolling_update['current_step'] = 'maintenance'
                    maintenance_task = mgr.enter_maintenance_mode(node_name, skip_evacuation=skip_evacuation)
                    
                    if not skip_evacuation:
                        # Wait for evacuation
                        waited = 0
                        while waited < evacuation_timeout:
                            if node_name in mgr.nodes_in_maintenance:
                                task = mgr.nodes_in_maintenance[node_name]
                                if task.status in ['completed', 'completed_with_errors']:
                                    break
                            time.sleep(5)
                            waited += 5
                    
                    # Start update
                    mgr._rolling_update['current_step'] = 'updating'
                    update_task = mgr.start_node_update(node_name, reboot=include_reboot, force=True)
                    
                    if update_task:
                        # Wait for update to complete
                        max_wait = 1800 if include_reboot else 900
                        waited = 0
                        while waited < max_wait:
                            if update_task.status in ['completed', 'failed']:
                                break
                            time.sleep(10)
                            waited += 10
                        
                        if update_task.status == 'completed':
                            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ✓ {node_name} updated successfully")
                            mgr._rolling_update['completed_nodes'].append(node_name)
                        else:
                            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ✗ {node_name} update failed")
                            mgr._rolling_update['failed_nodes'].append(node_name)
                    
                    # Exit maintenance mode
                    mgr.exit_maintenance_mode(node_name)
                
                # Finished
                mgr._rolling_update['status'] = 'completed'
                mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Scheduled rolling update completed")
                
                # Log audit
                log_audit('scheduler', 'scheduled.rolling_update', 
                         f"Scheduled rolling update completed: {len(mgr._rolling_update['completed_nodes'])} updated, "
                         f"{len(mgr._rolling_update['skipped_nodes'])} skipped, "
                         f"{len(mgr._rolling_update['failed_nodes'])} failed")
                
            except Exception as e:
                logging.error(f"[SCHEDULER] Rolling update error: {e}")
                if hasattr(mgr, '_rolling_update'):
                    mgr._rolling_update['status'] = 'failed'
                    mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ERROR: {e}")
        
        update_thread = threading.Thread(target=run_scheduled_update, daemon=True)
        update_thread.start()
        
        logging.info(f"[SCHEDULER] Rolling update thread started for {cluster_id}")
        
    except Exception as e:
        logging.error(f"[SCHEDULER] Failed to start scheduled rolling update: {e}")


def start_scheduler():
    """Start the scheduler background thread"""
    global _scheduler_thread, _scheduler_running
    
    if _scheduler_thread and _scheduler_thread.is_alive():
        return
    
    _scheduler_running = True
    _scheduler_thread = threading.Thread(target=check_schedules, daemon=True)
    _scheduler_thread.start()
    logging.info("Scheduler started")


def stop_scheduler():
    """Stop the scheduler"""
    global _scheduler_running
    _scheduler_running = False


# API endpoints for scheduled actions

@app.route('/api/schedules', methods=['GET'])
@require_auth()
def get_schedules():
    """Get all scheduled actions
    
    Filters by user's accessible clusters unless admin
    """
    schedules = load_schedules()
    
    user = request.session.get('user', '')
    users_db = load_users()
    user_data = users_db.get(user, {})
    is_admin = user_data.get('role') == ROLE_ADMIN
    user_clusters = user_data.get('clusters', [])
    
    # Filter actions by accessible clusters
    if is_admin:
        return jsonify(schedules.get('actions', []))
    
    filtered = [
        a for a in schedules.get('actions', [])
        if not user_clusters or a.get('cluster_id') in user_clusters
    ]
    
    return jsonify(filtered)


@app.route('/api/schedules', methods=['POST'])
@require_auth(perms=['vm.start'])  # Need at least VM start permission
def create_schedule():
    """Create a new scheduled action
    
    Body:
    - cluster_id: Cluster ID
    - vmid: VM ID
    - vm_type: 'qemu' or 'lxc'
    - action: 'start', 'stop', 'shutdown', 'reboot', 'snapshot'
    - schedule_type: 'once', 'daily', 'weekly', 'weekdays', 'weekends'
    - time: 'HH:MM' format
    - date: (for once) 'YYYY-MM-DD' format
    - days: (for weekly) ['monday', 'wednesday', 'friday']
    - name: Optional friendly name
    """
    data = request.json or {}
    
    required = ['cluster_id', 'vmid', 'action', 'schedule_type', 'time']
    for field in required:
        if not data.get(field):
            return jsonify({'error': f'{field} is required'}), 400
    
    # Validate time format
    time_str = data.get('time', '')
    try:
        datetime.strptime(time_str, '%H:%M')
    except ValueError:
        return jsonify({'error': 'Time must be in HH:MM format'}), 400
    
    # Validate action
    valid_actions = ['start', 'stop', 'shutdown', 'reboot', 'snapshot']
    if data['action'] not in valid_actions:
        return jsonify({'error': f'Action must be one of: {valid_actions}'}), 400
    
    # Validate schedule type
    valid_types = ['once', 'daily', 'weekly', 'weekdays', 'weekends']
    if data['schedule_type'] not in valid_types:
        return jsonify({'error': f'Schedule type must be one of: {valid_types}'}), 400
    
    # For 'once' type, require date
    if data['schedule_type'] == 'once' and not data.get('date'):
        return jsonify({'error': 'Date is required for one-time schedules'}), 400
    
    # For 'weekly' type, require days
    if data['schedule_type'] == 'weekly' and not data.get('days'):
        return jsonify({'error': 'Days are required for weekly schedules'}), 400
    
    schedules = load_schedules()
    
    # Generate new ID
    new_id = schedules.get('last_id', 0) + 1
    schedules['last_id'] = new_id
    
    # Create the schedule
    new_schedule = {
        'id': new_id,
        'cluster_id': data['cluster_id'],
        'vmid': int(data['vmid']),
        'vm_type': data.get('vm_type', 'qemu'),
        'action': data['action'],
        'schedule_type': data['schedule_type'],
        'time': time_str,
        'date': data.get('date'),
        'days': data.get('days', []),
        'name': data.get('name', f"{data['action']} VM {data['vmid']}"),
        'enabled': True,
        'created_by': request.session.get('user', 'unknown'),
        'created_at': datetime.now().isoformat(),
        'run_count': 0
    }
    
    if 'actions' not in schedules:
        schedules['actions'] = []
    
    schedules['actions'].append(new_schedule)
    save_schedules(schedules)
    
    log_audit(request.session.get('user', 'system'), 'schedule.created', 
             f"Created schedule '{new_schedule['name']}' for VM {data['vmid']}")
    
    return jsonify({'success': True, 'schedule': new_schedule})


@app.route('/api/schedules/<int:schedule_id>', methods=['PUT'])
@require_auth(perms=['vm.start'])
def update_schedule(schedule_id):
    """Update a scheduled action"""
    data = request.json or {}
    schedules = load_schedules()
    
    # Find the schedule
    schedule = next((s for s in schedules.get('actions', []) if s.get('id') == schedule_id), None)
    if not schedule:
        return jsonify({'error': 'Schedule not found'}), 404
    
    # Update fields
    updatable = ['name', 'action', 'schedule_type', 'time', 'date', 'days', 'enabled']
    for field in updatable:
        if field in data:
            schedule[field] = data[field]
    
    save_schedules(schedules)
    
    return jsonify({'success': True, 'schedule': schedule})


@app.route('/api/schedules/<int:schedule_id>', methods=['DELETE'])
@require_auth(perms=['vm.start'])
def delete_schedule(schedule_id):
    """Delete a scheduled action"""
    schedules = load_schedules()
    
    original_len = len(schedules.get('actions', []))
    schedules['actions'] = [s for s in schedules.get('actions', []) if s.get('id') != schedule_id]
    
    if len(schedules['actions']) == original_len:
        return jsonify({'error': 'Schedule not found'}), 404
    
    save_schedules(schedules)
    
    log_audit(request.session.get('user', 'system'), 'schedule.deleted', 
             f"Deleted schedule ID {schedule_id}")
    
    return jsonify({'success': True})


# ============================================
# Resource Reports / Analytics
# resource tracking stuff
# Stores periodic snapshots of cluster/VM metrics
# Nothing fancy, just enough to show trends
# ============================================

METRICS_HISTORY_FILE = os.path.join(CONFIG_DIR, 'metrics_history.json')
_metrics_collector_running = False

def load_metrics_history():
    """Load historical metrics from SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM metrics_history ORDER BY timestamp DESC LIMIT 1000')
        
        snapshots = []
        for row in cursor.fetchall():
            try:
                data = json.loads(row['data'])
                data['timestamp'] = row['timestamp']
                snapshots.append(data)
            except:
                pass
        
        return {'snapshots': snapshots, 'last_cleanup': None}
    except Exception as e:
        logging.error(f"Error loading metrics history from database: {e}")
        # Legacy fallback
        try:
            if os.path.exists(METRICS_HISTORY_FILE):
                with open(METRICS_HISTORY_FILE, 'r') as f:
                    return json.load(f)
        except:
            pass
    return {'snapshots': [], 'last_cleanup': None}


def save_metrics_history(history):
    """Save metrics history - now saves individual snapshots
    
    SQLite migration
    This function is kept for backwards compatibility
    """
    # In SQLite version, snapshots are saved individually
    pass


def save_metrics_snapshot(snapshot):
    """Save a single metrics snapshot to SQLite
    
    new SQLite function
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        timestamp = snapshot.get('timestamp', datetime.now().isoformat())
        data = json.dumps({k: v for k, v in snapshot.items() if k != 'timestamp'})
        
        cursor.execute('''
            INSERT INTO metrics_history (timestamp, data)
            VALUES (?, ?)
        ''', (timestamp, data))
        
        # Cleanup old entries (keep last 1000)
        cursor.execute('''
            DELETE FROM metrics_history 
            WHERE id NOT IN (
                SELECT id FROM metrics_history 
                ORDER BY timestamp DESC LIMIT 1000
            )
        ''')
        
        db.conn.commit()
    except Exception as e:
        logging.error(f"Error saving metrics snapshot: {e}")


def collect_metrics_snapshot():
    """Collect current metrics from all clusters
    
    Called periodically to build historical data
    """
    snapshot = {
        'timestamp': datetime.now().isoformat(),
        'clusters': {}
    }
    
    for cluster_id, mgr in cluster_managers.items():
        if not mgr.is_connected:
            continue
        
        try:
            cluster_data = {
                'name': mgr.config.name,
                'nodes': {},
                'totals': {
                    'vms_running': 0,
                    'vms_stopped': 0,
                    'cts_running': 0,
                    'cts_stopped': 0,
                    'cpu_used': 0,
                    'cpu_total': 0,
                    'mem_used': 0,
                    'mem_total': 0
                }
            }
            
            # Collect node metrics
            for node_name, node_data in (mgr.nodes or {}).items():
                if node_data.get('status') != 'online':
                    continue
                
                cluster_data['nodes'][node_name] = {
                    'cpu': round(node_data.get('cpu', 0) * 100, 1),
                    'mem_percent': round(node_data.get('mem', 0) / max(node_data.get('maxmem', 1), 1) * 100, 1),
                    'maxcpu': node_data.get('maxcpu', 0),
                    'maxmem': node_data.get('maxmem', 0)
                }
                
                cluster_data['totals']['cpu_total'] += node_data.get('maxcpu', 0)
                cluster_data['totals']['cpu_used'] += node_data.get('cpu', 0) * node_data.get('maxcpu', 0)
                cluster_data['totals']['mem_total'] += node_data.get('maxmem', 0)
                cluster_data['totals']['mem_used'] += node_data.get('mem', 0)
            
            # Count VMs
            try:
                resources = mgr.get_vm_resources()
                for r in resources:
                    if r.get('type') == 'qemu':
                        if r.get('status') == 'running':
                            cluster_data['totals']['vms_running'] += 1
                        else:
                            cluster_data['totals']['vms_stopped'] += 1
                    else:
                        if r.get('status') == 'running':
                            cluster_data['totals']['cts_running'] += 1
                        else:
                            cluster_data['totals']['cts_stopped'] += 1
            except:
                pass
            
            snapshot['clusters'][cluster_id] = cluster_data
            
        except Exception as e:
            logging.debug(f"Failed to collect metrics for {cluster_id}: {e}")
    
    return snapshot


def metrics_collector_loop():
    """Background thread that collects metrics every 5 minutes
    
    updated for SQLite
    """
    global _metrics_collector_running
    
    while _metrics_collector_running:
        try:
            snapshot = collect_metrics_snapshot()
            
            # Save directly to SQLite
            save_metrics_snapshot(snapshot)
            
        except Exception as e:
            logging.error(f"Metrics collector error: {e}")
        
        # Sleep 5 minutes
        for _ in range(300):
            if not _metrics_collector_running:
                break
            time.sleep(1)


def start_metrics_collector():
    """Start the metrics collector"""
    global _metrics_collector_running
    
    _metrics_collector_running = True
    thread = threading.Thread(target=metrics_collector_loop, daemon=True)
    thread.start()
    logging.info("Metrics collector started")


@app.route('/api/reports/summary', methods=['GET'])
@require_auth()
def get_reports_summary():
    """Get summary report across all clusters
    
    Query params:
    - period: 'hour', 'day', 'week' (default: day)
    """
    period = request.args.get('period', 'day')
    
    history = load_metrics_history()
    snapshots = history.get('snapshots', [])
    
    if not snapshots:
        return jsonify({'error': 'No historical data available yet'}), 404
    
    # Filter by period
    now = datetime.now()
    if period == 'hour':
        cutoff = now - timedelta(hours=1)
    elif period == 'week':
        cutoff = now - timedelta(days=7)
    else:  # day
        cutoff = now - timedelta(days=1)
    
    cutoff_str = cutoff.isoformat()
    filtered = [s for s in snapshots if s.get('timestamp', '') >= cutoff_str]
    
    if not filtered:
        return jsonify({'error': f'No data for the last {period}'}), 404
    
    # Calculate averages and trends
    report = {
        'period': period,
        'data_points': len(filtered),
        'start_time': filtered[0].get('timestamp'),
        'end_time': filtered[-1].get('timestamp'),
        'clusters': {}
    }
    
    # Aggregate per cluster
    for snapshot in filtered:
        for cluster_id, cluster_data in snapshot.get('clusters', {}).items():
            if cluster_id not in report['clusters']:
                report['clusters'][cluster_id] = {
                    'name': cluster_data.get('name', cluster_id),
                    'cpu_samples': [],
                    'mem_samples': [],
                    'vm_samples': []
                }
            
            totals = cluster_data.get('totals', {})
            if totals.get('cpu_total', 0) > 0:
                cpu_percent = totals['cpu_used'] / totals['cpu_total'] * 100
                report['clusters'][cluster_id]['cpu_samples'].append(cpu_percent)
            
            if totals.get('mem_total', 0) > 0:
                mem_percent = totals['mem_used'] / totals['mem_total'] * 100
                report['clusters'][cluster_id]['mem_samples'].append(mem_percent)
            
            vm_count = totals.get('vms_running', 0) + totals.get('cts_running', 0)
            report['clusters'][cluster_id]['vm_samples'].append(vm_count)
    
    # Calculate stats
    for cluster_id, data in report['clusters'].items():
        cpu = data.pop('cpu_samples', [])
        mem = data.pop('mem_samples', [])
        vms = data.pop('vm_samples', [])
        
        data['cpu'] = {
            'avg': round(sum(cpu) / len(cpu), 1) if cpu else 0,
            'min': round(min(cpu), 1) if cpu else 0,
            'max': round(max(cpu), 1) if cpu else 0,
            'current': round(cpu[-1], 1) if cpu else 0
        }
        
        data['memory'] = {
            'avg': round(sum(mem) / len(mem), 1) if mem else 0,
            'min': round(min(mem), 1) if mem else 0,
            'max': round(max(mem), 1) if mem else 0,
            'current': round(mem[-1], 1) if mem else 0
        }
        
        data['vms_running'] = {
            'avg': round(sum(vms) / len(vms), 1) if vms else 0,
            'min': min(vms) if vms else 0,
            'max': max(vms) if vms else 0,
            'current': vms[-1] if vms else 0
        }
    
    return jsonify(report)


@app.route('/api/reports/timeline', methods=['GET'])
@require_auth()
def get_reports_timeline():
    """Get timeline data for charts
    
    Query params:
    - period: 'hour', 'day', 'week'
    - cluster_id: Optional - filter to specific cluster
    - metric: 'cpu', 'memory', 'vms' (default: all)
    """
    period = request.args.get('period', 'day')
    filter_cluster = request.args.get('cluster_id')
    metric = request.args.get('metric', 'all')
    
    history = load_metrics_history()
    snapshots = history.get('snapshots', [])
    
    if not snapshots:
        return jsonify({'error': 'No historical data available'}), 404
    
    # Filter by period
    now = datetime.now()
    if period == 'hour':
        cutoff = now - timedelta(hours=1)
    elif period == 'week':
        cutoff = now - timedelta(days=7)
    else:
        cutoff = now - timedelta(days=1)
    
    cutoff_str = cutoff.isoformat()
    filtered = [s for s in snapshots if s.get('timestamp', '') >= cutoff_str]
    
    # Build timeline
    timeline = {
        'period': period,
        'timestamps': [],
        'data': {}
    }
    
    for snapshot in filtered:
        timestamp = snapshot.get('timestamp', '')
        timeline['timestamps'].append(timestamp)
        
        for cluster_id, cluster_data in snapshot.get('clusters', {}).items():
            if filter_cluster and cluster_id != filter_cluster:
                continue
            
            if cluster_id not in timeline['data']:
                timeline['data'][cluster_id] = {
                    'name': cluster_data.get('name', cluster_id),
                    'cpu': [],
                    'memory': [],
                    'vms': []
                }
            
            totals = cluster_data.get('totals', {})
            
            # CPU
            if metric in ['all', 'cpu']:
                cpu = 0
                if totals.get('cpu_total', 0) > 0:
                    cpu = round(totals['cpu_used'] / totals['cpu_total'] * 100, 1)
                timeline['data'][cluster_id]['cpu'].append(cpu)
            
            # Memory
            if metric in ['all', 'memory']:
                mem = 0
                if totals.get('mem_total', 0) > 0:
                    mem = round(totals['mem_used'] / totals['mem_total'] * 100, 1)
                timeline['data'][cluster_id]['memory'].append(mem)
            
            # VMs
            if metric in ['all', 'vms']:
                vms = totals.get('vms_running', 0) + totals.get('cts_running', 0)
                timeline['data'][cluster_id]['vms'].append(vms)
    
    return jsonify(timeline)


@app.route('/api/reports/top-vms', methods=['GET'])
@require_auth()
def get_top_vms():
    """Get top VMs by resource usage
    
    Query params:
    - metric: 'cpu' or 'memory' (default: cpu)
    - limit: Number of results (default: 10)
    """
    metric = request.args.get('metric', 'cpu')
    limit = int(request.args.get('limit', 10))
    
    vms = []
    
    for cluster_id, mgr in cluster_managers.items():
        if not mgr.is_connected:
            continue
        
        try:
            resources = mgr.get_vm_resources()
            for r in resources:
                if r.get('status') != 'running':
                    continue
                
                vm_data = {
                    'cluster_id': cluster_id,
                    'cluster_name': mgr.config.name,
                    'vmid': r.get('vmid'),
                    'name': r.get('name'),
                    'node': r.get('node'),
                    'type': r.get('type'),
                    'cpu': r.get('cpu', 0),
                    'mem': r.get('mem', 0),
                    'maxmem': r.get('maxmem', 0),
                    'mem_percent': round(r.get('mem', 0) / max(r.get('maxmem', 1), 1) * 100, 1)
                }
                vms.append(vm_data)
        except:
            pass
    
    # Sort by metric
    if metric == 'memory':
        vms.sort(key=lambda x: x.get('mem_percent', 0), reverse=True)
    else:
        vms.sort(key=lambda x: x.get('cpu', 0), reverse=True)
    
    return jsonify(vms[:limit])


# Start background threads when server starts
# MK: Move this to main() later, for now it's fine here
threading.Thread(target=lambda: (time.sleep(5), start_scheduler()), daemon=True).start()
threading.Thread(target=lambda: (time.sleep(10), start_metrics_collector()), daemon=True).start()


# ============================================
# Legacy Fallback Endpoints
# old tags endpoints, kept for compat, these prevent 404s
# ============================================

@app.route('/api/tags', methods=['GET'])
@require_auth()
def get_tags_legacy():
    """Legacy: Returns empty for old Settings UI"""
    return jsonify({'tags': {}, 'available_tags': []})

@app.route('/api/tags/available', methods=['GET'])
@require_auth()
def get_available_tags_legacy():
    """Legacy: Returns empty list"""
    return jsonify([])

@app.route('/api/tags/available', methods=['POST'])
@require_auth()
def create_tag_legacy():
    """Legacy: Redirect to cluster-based tags"""
    return jsonify({'error': 'Please use VM-based tags (click tag icon on VMs)'}), 400

@app.route('/api/tags/available/<tag_name>', methods=['DELETE'])
@require_auth()
def delete_tag_legacy(tag_name):
    """Legacy: No-op"""
    return jsonify({'success': True})


# ============================================
# Cluster-Based Reports Endpoint
# reports are now per-cluster, not global
# ============================================

@app.route('/api/clusters/<cluster_id>/reports/summary', methods=['GET'])
@require_auth()
def get_cluster_report_summary(cluster_id):
    """Get report summary for a specific cluster
    
    Returns both historical data (if available) and current live data
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: 
        return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    period = request.args.get('period', 'day')
    
    # Get LIVE current data from cluster using get_node_status()
    live_cpu = 0
    live_vms = 0
    live_cts = 0
    cpu_total = 0
    mem_total = 0
    mem_used = 0
    nodes_online = 0
    
    if mgr.is_connected:
        try:
            # Use get_node_status which actually fetches live data from Proxmox
            node_status = mgr.get_node_status()
            
            for node_name, node_data in node_status.items():
                if not node_data:
                    continue
                # Check status
                status = node_data.get('status', '')
                if status in ['online', 'running']:
                    nodes_online += 1
                    # CPU and memory from get_node_status are already percentages
                    cpu_pct = node_data.get('cpu_percent', 0) or 0
                    mem_pct = node_data.get('mem_percent', 0) or 0
                    mem_t = node_data.get('mem_total', 0) or 0
                    mem_u = node_data.get('mem_used', 0) or 0
                    
                    # Accumulate (we'll average later)
                    live_cpu += cpu_pct
                    mem_total += mem_t
                    mem_used += mem_u
            
            # Average CPU across nodes
            if nodes_online > 0:
                live_cpu = live_cpu / nodes_online
        except Exception as e:
            app.logger.error(f"Error getting node status for reports: {e}")
        
        # Count running VMs
        try:
            resources = mgr.get_vm_resources() or []
            for r in resources:
                if r and r.get('status') == 'running':
                    if r.get('type') == 'qemu':
                        live_vms += 1
                    else:
                        live_cts += 1
        except Exception as e:
            app.logger.error(f"Error getting VM resources: {e}")
    
    # Calculate live percentages
    live_cpu_pct = round(live_cpu, 1)
    live_mem_pct = round(mem_used / max(mem_total, 1) * 100, 1) if mem_total > 0 else 0
    
    # Load historical metrics
    history = load_metrics_history()
    snapshots = history.get('snapshots', [])
    
    # Filter by period
    now = datetime.now()
    if period == 'hour':
        cutoff = now - timedelta(hours=1)
    elif period == 'week':
        cutoff = now - timedelta(days=7)
    else:
        cutoff = now - timedelta(days=1)
    
    cutoff_str = cutoff.isoformat()
    filtered = [s for s in snapshots if s.get('timestamp', '') >= cutoff_str]
    
    # Extract data for this cluster only
    report = {
        'period': period,
        'cluster_id': cluster_id,
        'cluster_name': getattr(mgr.config, 'name', None) or cluster_id,
        'data_points': 0,
        'cpu': {'avg': live_cpu_pct, 'min': live_cpu_pct, 'max': live_cpu_pct, 'current': live_cpu_pct, 'samples': []},
        'memory': {'avg': live_mem_pct, 'min': live_mem_pct, 'max': live_mem_pct, 'current': live_mem_pct, 'samples': []},
        'vms_running': {'avg': live_vms + live_cts, 'min': live_vms + live_cts, 'max': live_vms + live_cts, 'current': live_vms + live_cts, 'samples': []},
        'timestamps': [],
        # Add live data section
        'live': {
            'cpu_percent': live_cpu_pct,
            'mem_percent': live_mem_pct,
            'vms_running': live_vms,
            'cts_running': live_cts,
            'cpu_total': cpu_total,
            'mem_total': mem_total,
            'mem_used': mem_used
        }
    }
    
    for snapshot in filtered:
        cluster_data = snapshot.get('clusters', {}).get(cluster_id)
        if not cluster_data:
            continue
        
        report['timestamps'].append(snapshot.get('timestamp', ''))
        report['data_points'] += 1
        
        totals = cluster_data.get('totals', {})
        
        # CPU
        if totals.get('cpu_total', 0) > 0:
            cpu = round(totals['cpu_used'] / totals['cpu_total'] * 100, 1)
            report['cpu']['samples'].append(cpu)
        
        # Memory
        if totals.get('mem_total', 0) > 0:
            mem = round(totals['mem_used'] / totals['mem_total'] * 100, 1)
            report['memory']['samples'].append(mem)
        
        # VMs
        vms = totals.get('vms_running', 0) + totals.get('cts_running', 0)
        report['vms_running']['samples'].append(vms)
    
    # Calculate stats from historical data (if available)
    for metric in ['cpu', 'memory', 'vms_running']:
        samples = report[metric].pop('samples', [])
        if samples:
            report[metric]['avg'] = round(sum(samples) / len(samples), 1)
            report[metric]['min'] = round(min(samples), 1)
            report[metric]['max'] = round(max(samples), 1)
            report[metric]['current'] = round(samples[-1], 1) if samples else report[metric]['current']
    
    return jsonify(report)


@app.route('/api/clusters/<cluster_id>/reports/top-vms', methods=['GET'])
@require_auth()
def get_cluster_top_vms(cluster_id):
    """Get top VMs by resource usage for a specific cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: 
        return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    metric = request.args.get('metric', 'cpu')
    limit = int(request.args.get('limit', 10))
    
    if not mgr.is_connected:
        return jsonify([])
    
    vms = []
    try:
        resources = mgr.get_vm_resources()
        for r in resources:
            if r.get('status') != 'running':
                continue
            
            vm_data = {
                'vmid': r.get('vmid'),
                'name': r.get('name'),
                'node': r.get('node'),
                'type': r.get('type'),
                'cpu': r.get('cpu', 0),
                'mem': r.get('mem', 0),
                'maxmem': r.get('maxmem', 0),
                'mem_percent': round(r.get('mem', 0) / max(r.get('maxmem', 1), 1) * 100, 1)
            }
            vms.append(vm_data)
    except:
        pass
    
    # Sort by metric
    if metric == 'memory':
        vms.sort(key=lambda x: x.get('mem_percent', 0), reverse=True)
    else:
        vms.sort(key=lambda x: x.get('cpu', 0), reverse=True)
    
    return jsonify(vms[:limit])


# ============================================
# Cluster-Based Alerts Endpoints
# moved to per-cluster
# ============================================

def load_cluster_alerts():
    """Load alerts config from SQLite database
    
    NS: Migrated from JSON to SQLite Jan 2026
    MK: keeps falling back to json if db fails which is kinda nice for debugging
    
    Returns: {cluster_id: [list of alert objects]}
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM cluster_alerts WHERE enabled = 1')
        
        alerts = {}
        for row in cursor.fetchall():
            cluster_id = row['cluster_id']
            if cluster_id not in alerts:
                alerts[cluster_id] = []
            
            try:
                # config contains the full alert object as JSON
                alert_data = json.loads(row['config'] or '{}')
                # ensure id is present
                if 'id' not in alert_data:
                    alert_data['id'] = row['alert_type']
                alerts[cluster_id].append(alert_data)
            except:
                # fallback for old format where config was just settings
                alerts[cluster_id].append({
                    'id': row['alert_type'],
                    'name': row['alert_type'],
                    'config': row['config']
                })
        
        return alerts
    except Exception as e:
        logging.error(f"Error loading cluster alerts from DB: {e}")
        # Fallback to JSON for backwards compat
        try:
            alerts_file = os.path.join(CONFIG_DIR, 'cluster_alerts.json')
            if os.path.exists(alerts_file):
                with open(alerts_file, 'r') as f:
                    return json.load(f)
        except:
            pass
        return {}

def save_cluster_alerts(alerts):
    """Save alerts config to SQLite database
    
    NS: stores each alert as a row with config containing full alert object
    
    Expects: {cluster_id: [list of alert objects]}
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        now = datetime.now().isoformat()
        
        for cluster_id, alert_list in alerts.items():
            # Handle list format (from API)
            if isinstance(alert_list, list):
                for alert in alert_list:
                    alert_id = alert.get('id', str(uuid.uuid4())[:8])
                    cursor.execute('''
                        INSERT OR REPLACE INTO cluster_alerts 
                        (cluster_id, alert_type, config, enabled, updated_at)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (
                        cluster_id,
                        alert_id,
                        json.dumps(alert),
                        1 if alert.get('enabled', True) else 0,
                        now
                    ))
            # Handle dict format (legacy)
            elif isinstance(alert_list, dict):
                for alert_type, config in alert_list.items():
                    cursor.execute('''
                        INSERT OR REPLACE INTO cluster_alerts 
                        (cluster_id, alert_type, config, enabled, updated_at)
                        VALUES (?, ?, ?, 1, ?)
                    ''', (
                        cluster_id,
                        alert_type,
                        json.dumps(config) if isinstance(config, dict) else str(config),
                        now
                    ))
        
        db.conn.commit()
    except Exception as e:
        logging.error(f"Error saving cluster alerts to DB: {e}")

@app.route('/api/clusters/<cluster_id>/alerts', methods=['GET'])
@require_auth()
def get_cluster_alerts(cluster_id):
    """Get alerts for a specific cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok:
        return err
    
    try:
        alerts = load_cluster_alerts()
        cluster_alerts = alerts.get(cluster_id, [])
        return jsonify({'alerts': cluster_alerts})
    except Exception as e:
        logging.error(f"Error getting cluster alerts: {e}")
        return jsonify({'alerts': [], 'error': str(e)})

@app.route('/api/clusters/<cluster_id>/alerts', methods=['POST'])
@require_auth()
def create_cluster_alert(cluster_id):
    """Create a new alert for a cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok:
        return err
    
    data = request.get_json()
    if not data:
        return jsonify({'error': 'No data provided'}), 400
    
    alerts = load_cluster_alerts()
    if cluster_id not in alerts:
        alerts[cluster_id] = []
    
    alert = {
        'id': str(uuid.uuid4())[:8],
        'name': data.get('name', 'Unnamed Alert'),
        'metric': data.get('metric', 'cpu'),
        'operator': data.get('operator', '>'),
        'threshold': data.get('threshold', 80),
        'target_type': data.get('target_type', 'cluster'),
        'target_id': data.get('target_id'),
        'action': data.get('action', 'log'),
        'enabled': data.get('enabled', True),
        'created_at': datetime.now().isoformat()
    }
    
    alerts[cluster_id].append(alert)
    save_cluster_alerts(alerts)
    
    return jsonify({'success': True, 'alert': alert})

@app.route('/api/clusters/<cluster_id>/alerts/<alert_id>', methods=['PUT'])
@require_auth()
def update_cluster_alert(cluster_id, alert_id):
    """Update an alert"""
    ok, err = check_cluster_access(cluster_id)
    if not ok:
        return err
    
    data = request.get_json()
    alerts = load_cluster_alerts()
    cluster_alerts = alerts.get(cluster_id, [])
    
    for alert in cluster_alerts:
        if alert['id'] == alert_id:
            if 'enabled' in data:
                alert['enabled'] = data['enabled']
            if 'name' in data:
                alert['name'] = data['name']
            if 'threshold' in data:
                alert['threshold'] = data['threshold']
            save_cluster_alerts(alerts)
            return jsonify({'success': True, 'alert': alert})
    
    return jsonify({'error': 'Alert not found'}), 404

@app.route('/api/clusters/<cluster_id>/alerts/<alert_id>', methods=['DELETE'])
@require_auth()
def delete_cluster_alert(cluster_id, alert_id):
    """Delete an alert"""
    ok, err = check_cluster_access(cluster_id)
    if not ok:
        return err
    
    # NS: delete directly from DB for efficiency
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('DELETE FROM cluster_alerts WHERE cluster_id = ? AND alert_type = ?',
                      (cluster_id, alert_id))
        db.conn.commit()
        deleted = cursor.rowcount > 0
        return jsonify({'success': True, 'deleted': deleted})
    except Exception as e:
        logging.error(f"Error deleting cluster alert: {e}")
        return jsonify({'error': str(e)}), 500


# ============================================
# Cluster-Based Affinity Rules Endpoints
# moved to per-cluster
# ============================================

def load_cluster_affinity_rules():
    """Load affinity rules from SQLite database
    
    MK: affinity = keep VMs together, anti-affinity = keep them apart
    useful for HA where you want replicas on different hosts
    NS: reuses the affinity_rules table we already had
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM affinity_rules WHERE enabled = 1')
        
        rules = {}
        for row in cursor.fetchall():
            cluster_id = row['cluster_id']
            if cluster_id not in rules:
                rules[cluster_id] = []
            
            vms_list = json.loads(row['vms'] or '[]')
            rules[cluster_id].append({
                'id': row['id'],
                'name': row['name'],
                'type': row['type'],
                'vms': vms_list,
                'vm_ids': vms_list,  # NS: alias for frontend compatibility
                'enabled': bool(row['enabled']),
                'created_at': row['created_at']
            })
        
        return rules
    except Exception as e:
        logging.error(f"Error loading affinity rules from DB: {e}")
        # Fallback to JSON for backwards compat
        try:
            rules_file = os.path.join(CONFIG_DIR, 'cluster_affinity_rules.json')
            if os.path.exists(rules_file):
                with open(rules_file, 'r') as f:
                    return json.load(f)
        except:
            pass
        return {}

def save_cluster_affinity_rules(rules):
    """Save affinity rules to SQLite database
    
    NS: uses upsert pattern, handles both 'vms' and 'vm_ids' field names
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        now = datetime.now().isoformat()
        
        for cluster_id, cluster_rules in rules.items():
            for rule in cluster_rules:
                # generate id if missing (old rules might not have one)
                rule_id = rule.get('id', str(uuid.uuid4()))
                # NS: handle both 'vms' and 'vm_ids' field names
                vms_data = rule.get('vms') or rule.get('vm_ids') or []
                cursor.execute('''
                    INSERT OR REPLACE INTO affinity_rules 
                    (id, cluster_id, name, type, vms, enabled, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    rule_id,
                    cluster_id,
                    rule.get('name', ''),
                    rule.get('type', 'affinity'),
                    json.dumps(vms_data),
                    1 if rule.get('enabled', True) else 0,
                    rule.get('created_at', now)
                ))
        
        db.conn.commit()
    except Exception as e:
        logging.error(f"Error saving affinity rules to DB: {e}")

@app.route('/api/clusters/<cluster_id>/affinity-rules', methods=['GET'])
@require_auth()
def get_cluster_affinity_rules(cluster_id):
    """Get affinity rules for a specific cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok:
        return err
    
    try:
        rules = load_cluster_affinity_rules()
        cluster_rules = rules.get(cluster_id, [])
        return jsonify({'rules': cluster_rules})
    except Exception as e:
        logging.error(f"Error getting affinity rules: {e}")
        return jsonify({'rules': [], 'error': str(e)})

@app.route('/api/clusters/<cluster_id>/affinity-rules', methods=['POST'])
@require_auth()
def create_cluster_affinity_rule(cluster_id):
    """Create a new affinity rule for a cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok:
        return err
    
    data = request.get_json()
    if not data:
        return jsonify({'error': 'No data provided'}), 400
    
    rules = load_cluster_affinity_rules()
    if cluster_id not in rules:
        rules[cluster_id] = []
    
    # NS: get vms from either 'vm_ids' or 'vms' field
    vms_data = data.get('vm_ids') or data.get('vms') or []
    
    rule = {
        'id': str(uuid.uuid4())[:8],
        'name': data.get('name', f"Rule {len(rules[cluster_id]) + 1}"),
        'type': data.get('type', 'together'),  # 'together' or 'separate'
        'vms': vms_data,
        'vm_ids': vms_data,  # alias for frontend
        'enforce': data.get('enforce', False),
        'enabled': True,
        'created_at': datetime.now().isoformat()
    }
    
    rules[cluster_id].append(rule)
    save_cluster_affinity_rules(rules)
    
    return jsonify({'success': True, 'rule': rule})

@app.route('/api/clusters/<cluster_id>/affinity-rules/<rule_id>', methods=['DELETE'])
@require_auth()
def delete_cluster_affinity_rule(cluster_id, rule_id):
    """Delete an affinity rule"""
    ok, err = check_cluster_access(cluster_id)
    if not ok:
        return err
    
    # NS: Delete directly from DB instead of load/filter/save
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('DELETE FROM affinity_rules WHERE id = ? AND cluster_id = ?', 
                      (rule_id, cluster_id))
        db.conn.commit()
        deleted = cursor.rowcount > 0
        return jsonify({'success': True, 'deleted': deleted})
    except Exception as e:
        logging.error(f"Error deleting affinity rule: {e}")
        return jsonify({'error': str(e)}), 500


# ============================================
# User Management API Routes (Admin only)
# ============================================

@app.route('/api/users', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_users():
    """Get list of all users (admin only)"""
    users_db = load_users()
    
    # Return users without password info
    users_list = []
    for username, user in users_db.items():
        users_list.append({
            'username': username,
            'role': user['role'],
            'display_name': user.get('display_name', username),
            'email': user.get('email', ''),
            'enabled': user.get('enabled', True),
            'totp_enabled': user.get('totp_enabled', False),
            'created_at': user.get('created_at'),
            'last_login': user.get('last_login'),
            'tenant_id': user.get('tenant_id', DEFAULT_TENANT_ID),  # MK: Added for tenant display
        })
    
    return jsonify(users_list)


# ============================================
# Locked IPs Management (Brute Force Protection)
# ============================================

@app.route('/api/security/locked-ips', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_locked_ips():
    """Get list of currently locked IPs and usernames (admin only)
    
    MK: Updated to show both IP and username lockouts
    """
    current_time = time.time()
    locked_ips = []
    locked_users = []
    
    # Get locked IPs
    for ip, info in login_attempts_by_ip.items():
        locked_until = info.get('locked_until', 0)
        if locked_until > current_time:
            locked_ips.append({
                'ip': ip,
                'locked_until': locked_until,
                'remaining_seconds': int(locked_until - current_time),
                'attempt_count': len(info.get('attempts', []))
            })
    
    # Get locked usernames
    for username, info in login_attempts_by_user.items():
        locked_until = info.get('locked_until', 0)
        if locked_until > current_time:
            locked_users.append({
                'username': username,
                'locked_until': locked_until,
                'remaining_seconds': int(locked_until - current_time),
                'attempt_count': len(info.get('attempts', []))
            })
    
    return jsonify({
        'locked_ips': locked_ips,
        'locked_users': locked_users,
        'total_tracked_ips': len(login_attempts_by_ip),
        'total_tracked_users': len(login_attempts_by_user)
    })


@app.route('/api/security/locked-ips/<ip_address>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def unlock_ip(ip_address):
    # NS: admin-only endpoint to unlock IPs manually
    global login_attempts_by_ip
    
    # Normalize IP (replace URL-encoded dots if needed)
    ip_address = ip_address.replace('%2E', '.')
    
    if ip_address in login_attempts_by_ip:
        del login_attempts_by_ip[ip_address]
        logging.info(f"Admin manually unlocked IP: {ip_address}")
        log_audit(request.headers.get('X-Username', 'admin'), 'security.unlock_ip', f"Manually unlocked IP: {ip_address}")
        return jsonify({'success': True, 'message': f'IP {ip_address} unlocked'})
    else:
        return jsonify({'error': 'IP not found in locked list'}), 404


@app.route('/api/security/locked-users/<username>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def unlock_user(username):
    """Unlock a specific username (admin only)
    
    MK: New endpoint for username-based lockout management
    """
    global login_attempts_by_user
    
    username = username.lower()
    
    if username in login_attempts_by_user:
        del login_attempts_by_user[username]
        logging.info(f"Admin manually unlocked user: {username}")
        log_audit(request.session.get('user', 'admin'), 'security.unlock_user', f"Manually unlocked user: {username}")
        return jsonify({'success': True, 'message': f'User {username} unlocked'})
    else:
        return jsonify({'error': 'User not found in locked list'}), 404


@app.route('/api/security/locked-ips', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def unlock_all_ips():
    """Unlock all IP addresses (admin only)"""
    global login_attempts_by_ip
    
    count = len(login_attempts_by_ip)
    login_attempts_by_ip = {}
    
    logging.info(f"Admin manually unlocked all IPs ({count} entries cleared)")
    log_audit(request.session.get('user', 'admin'), 'security.unlock_all_ips', f"Cleared all {count} locked IPs")
    
    return jsonify({'success': True, 'message': f'All {count} IPs unlocked'})


@app.route('/api/security/locked-users', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def unlock_all_users():
    """Unlock all usernames (admin only)
    
    MK: New endpoint for clearing all username lockouts
    """
    global login_attempts_by_user
    
    count = len(login_attempts_by_user)
    login_attempts_by_user = {}
    
    logging.info(f"Admin manually unlocked all users ({count} entries cleared)")
    log_audit(request.session.get('user', 'admin'), 'security.unlock_all_users', f"Cleared all {count} locked users")
    
    return jsonify({'success': True, 'message': f'All {count} users unlocked'})


# LW: Reset password expiry for all users - Dec 2025
# NS: Requested by admins who want to force everyone to change passwords after a breach
@app.route('/api/security/password-expiry/reset-all', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def reset_all_password_expiry():
    """Reset password_changed_at for all users, forcing everyone to change passwords
    
    MK: This is useful after a security incident or when rotating passwords company-wide
    Can include admins too if the admin explicitly asks for it
    """
    data = request.json or {}
    include_admins = data.get('include_admins', False)  # opt-in for admins
    
    users_db = load_users()
    reset_count = 0
    skipped_admins = 0
    
    # Set password_changed_at to a date far in the past
    # this makes all passwords appear expired
    old_date = (datetime.now() - timedelta(days=9999)).isoformat()
    
    for username, user in users_db.items():
        if user.get('role') == ROLE_ADMIN and not include_admins:
            skipped_admins += 1
            continue
        if not user.get('enabled', True):
            continue  # skip disabled users
            
        user['password_changed_at'] = old_date
        reset_count += 1
    
    save_users(users_db)
    
    admin_user = request.session.get('user', 'unknown')
    log_audit(admin_user, 'security.password_reset_all', 
              f"Reset password expiry for {reset_count} users (include_admins={include_admins}, skipped={skipped_admins})")
    logging.info(f"Admin {admin_user} reset password expiry for {reset_count} users")
    
    return jsonify({
        'success': True,
        'reset_count': reset_count,
        'skipped_admins': skipped_admins,
        'message': f'Password expiry reset for {reset_count} users' + (f' ({skipped_admins} admins skipped)' if skipped_admins > 0 else '')
    })


@app.route('/api/clusters/<cluster_id>/security/audit', methods=['GET'])
@require_auth(perms=['admin.audit'])
def get_security_audit(cluster_id):
    """Get security audit info for a cluster"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    
    try:
        host = manager.current_host or manager.config.host
        session = manager._create_session()
        
        # Get nodes
        nodes_url = f"https://{host}:8006/api2/json/nodes"
        nodes_resp = session.get(nodes_url, timeout=10)
        nodes = [n.get('node') for n in nodes_resp.json().get('data', []) if n.get('status') == 'online']
        
        result = {
            'firewall': {
                'cluster_enabled': False,
                'nodes': {}
            },
            'updates': {
                'total_security': 0,
                'nodes': {}
            },
            'ssh': {
                'issues': [],
                'nodes': {}
            },
            'fail2ban': {
                'total_banned': 0,
                'nodes': {}
            },
            'twoFactor': {
                'enabled': False
            }
        }
        
        # Check cluster firewall
        try:
            fw_url = f"https://{host}:8006/api2/json/cluster/firewall/options"
            fw_resp = session.get(fw_url, timeout=5)
            if fw_resp.status_code == 200:
                fw_data = fw_resp.json().get('data', {})
                result['firewall']['cluster_enabled'] = fw_data.get('enable', 0) == 1
        except Exception as e:
            logging.debug(f"Could not get cluster firewall: {e}")
        
        # Check each node
        for node in nodes:
            # Node firewall
            try:
                node_fw_url = f"https://{host}:8006/api2/json/nodes/{node}/firewall/options"
                node_fw_resp = session.get(node_fw_url, timeout=5)
                if node_fw_resp.status_code == 200:
                    node_fw_data = node_fw_resp.json().get('data', {})
                    
                    # Count rules
                    rules_url = f"https://{host}:8006/api2/json/nodes/{node}/firewall/rules"
                    rules_resp = session.get(rules_url, timeout=5)
                    rules_count = len(rules_resp.json().get('data', [])) if rules_resp.status_code == 200 else 0
                    
                    result['firewall']['nodes'][node] = {
                        'enabled': node_fw_data.get('enable', 0) == 1,
                        'rules': rules_count
                    }
            except Exception as e:
                logging.debug(f"Could not get firewall for {node}: {e}")
                result['firewall']['nodes'][node] = {'enabled': False, 'rules': 0}
            
            # Security updates
            try:
                updates = manager.get_node_apt_updates(node)
                security_pkgs = [
                    u.get('Package') for u in updates 
                    if u.get('Origin', '').lower().find('security') >= 0 or
                       u.get('Section', '').lower().find('security') >= 0 or
                       'security' in u.get('Package', '').lower()
                ]
                result['updates']['nodes'][node] = {
                    'total_updates': len(updates),
                    'security_updates': len(security_pkgs),
                    'security_packages': security_pkgs[:10]  # Limit to 10
                }
                result['updates']['total_security'] += len(security_pkgs)
            except Exception as e:
                logging.debug(f"Could not get updates for {node}: {e}")
                result['updates']['nodes'][node] = {'total_updates': 0, 'security_updates': 0, 'security_packages': []}
            
            # SSH config (via execute - requires SSH access)
            result['ssh']['nodes'][node] = {
                'permit_root_login': 'unknown',
                'password_auth': 'unknown',
                'port': '22',
                'pubkey_auth': 'unknown'
            }
            
            # Fail2ban status
            result['fail2ban']['nodes'][node] = {
                'installed': False,
                'jails': [],
                'total_banned': 0
            }
        
        # Check 2FA status
        try:
            tfa_url = f"https://{host}:8006/api2/json/access/tfa"
            tfa_resp = session.get(tfa_url, timeout=5)
            if tfa_resp.status_code == 200:
                tfa_data = tfa_resp.json().get('data', [])
                result['twoFactor']['enabled'] = len(tfa_data) > 0
                result['twoFactor']['users_with_2fa'] = len(tfa_data)
        except Exception as e:
            logging.debug(f"Could not get 2FA status: {e}")
        
        # Aggregate SSH issues
        for node, ssh_config in result['ssh']['nodes'].items():
            if ssh_config.get('permit_root_login') not in ['no', 'unknown']:
                if 'PermitRootLogin enabled' not in result['ssh']['issues']:
                    result['ssh']['issues'].append('PermitRootLogin enabled')
        
        return jsonify(result)
        
    except Exception as e:
        logging.error(f"Security audit error: {e}")
        return jsonify({'error': str(e)}), 500


def is_valid_role(role_id):
    """Check if a role is valid (builtin or custom)
    
    LW: Added to support custom roles in user management
    MK: Always reload from disk to avoid cache issues
    """
    # check builtin roles first
    if role_id in BUILTIN_ROLES:
        return True
    
    # check custom roles - reload fresh to avoid stale cache
    custom = load_custom_roles()
    
    # global custom roles
    if role_id in custom.get('global', {}):
        return True
    
    # tenant-specific custom roles
    for tenant_roles in custom.get('tenants', {}).values():
        if role_id in tenant_roles:
            return True
    
    return False


@app.route('/api/users', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_user():
    """Create a new user (admin only)"""
    global users_db
    
    data = request.get_json()
    username = data.get('username', '').strip().lower()
    password = data.get('password', '')
    role = data.get('role', ROLE_USER)
    display_name = data.get('display_name', username)
    email = data.get('email', '')
    tenant_id = data.get('tenant_id', DEFAULT_TENANT_ID)
    permissions = data.get('permissions', [])  # extra perms
    denied_permissions = data.get('denied_permissions', [])  # denied perms
    
    if not username or not password:
        return jsonify({'error': 'Username and password required'}), 400
    
    if len(username) < 3:
        return jsonify({'error': 'Username must be at least 3 characters'}), 400
    
    # Validate password policy
    is_valid, error_msg = validate_password_policy(password)
    if not is_valid:
        return jsonify({'error': error_msg}), 400
    
    # NS: Updated to support custom roles
    if not is_valid_role(role):
        return jsonify({'error': 'Invalid role'}), 400
    
    # MK: Auto-set tenant_id if role belongs to a specific tenant
    if role not in BUILTIN_ROLES:
        custom_roles = load_custom_roles()
        for tid, roles in custom_roles.get('tenants', {}).items():
            if role in roles:
                tenant_id = tid  # override with role's tenant
                break
    
    # validate tenant exists
    tenants = load_tenants()
    if tenant_id not in tenants:
        return jsonify({'error': 'Invalid tenant_id'}), 400
    
    # validate permissions are valid
    for p in permissions + denied_permissions:
        if p not in PERMISSIONS:
            return jsonify({'error': f'Invalid permission: {p}'}), 400
    
    users_db = load_users()
    
    if username in users_db:
        return jsonify({'error': 'Username already exists'}), 409
    
    # Create user
    salt, password_hash = hash_password(password)
    users_db[username] = {
        'password_salt': salt,
        'password_hash': password_hash,
        'password_changed_at': datetime.now().isoformat(),  # LW: for expiry tracking
        'role': role,
        'display_name': display_name,
        'email': email,
        'enabled': True,
        'created_at': datetime.now().isoformat(),
        'last_login': None,
        'tenant_id': tenant_id,
        'permissions': permissions,
        'denied_permissions': denied_permissions,
    }
    
    save_users(users_db)
    
    logging.info(f"Admin '{request.session['user']}' created user '{username}' with role '{role}'")
    log_audit(request.session['user'], 'user.created', f"Created user: {username} (role: {role}, tenant: {tenant_id})")
    
    return jsonify({
        'success': True,
        'user': {
            'username': username,
            'role': role,
            'display_name': display_name,
            'email': email,
            'tenant_id': tenant_id,
            'permissions': permissions,
            'denied_permissions': denied_permissions,
        }
    })

@app.route('/api/users/<username>', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def update_user(username):
    """Update a user (admin only)"""
    global users_db
    
    username = username.lower()
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    data = request.get_json()
    user = users_db[username]
    
    # Update fields
    if 'role' in data:
        # NS: Updated to support custom roles
        if not is_valid_role(data['role']):
            return jsonify({'error': 'Invalid role'}), 400
        # Prevent last admin from losing admin role
        if user['role'] == ROLE_ADMIN and data['role'] != ROLE_ADMIN:
            admin_count = sum(1 for u in users_db.values() if u['role'] == ROLE_ADMIN and u.get('enabled', True))
            if admin_count <= 1:
                return jsonify({'error': 'Cannot remove admin role from last admin'}), 400
        user['role'] = data['role']
        
        # MK: Auto-set tenant_id when assigning a tenant-specific role
        # This ensures the user is properly associated with the tenant
        if data['role'] not in BUILTIN_ROLES:
            custom_roles = load_custom_roles()
            # check if role belongs to a tenant
            found_tenant = False
            for tid, roles in custom_roles.get('tenants', {}).items():
                if data['role'] in roles:
                    user['tenant_id'] = tid
                    found_tenant = True
                    logging.info(f"Auto-set tenant_id={tid} for user with role {data['role']}")
                    break
            
            # LW: Also check global roles (they don't change tenant)
            if not found_tenant and data['role'] in custom_roles.get('global', {}):
                logging.debug(f"Role {data['role']} is global, keeping existing tenant_id")
    
    if 'display_name' in data:
        user['display_name'] = data['display_name']
    
    if 'email' in data:
        user['email'] = data['email']
    
    if 'enabled' in data:
        # Prevent disabling last admin
        if user['role'] == ROLE_ADMIN and not data['enabled']:
            admin_count = sum(1 for u in users_db.values() if u['role'] == ROLE_ADMIN and u.get('enabled', True))
            if admin_count <= 1:
                return jsonify({'error': 'Cannot disable last admin'}), 400
        user['enabled'] = data['enabled']
    
    # NS: Added tenant_id update support
    if 'tenant_id' in data:
        tenants = load_tenants()
        if data['tenant_id'] not in tenants:
            return jsonify({'error': 'Invalid tenant_id'}), 400
        user['tenant_id'] = data['tenant_id']
    
    if 'password' in data and data['password']:
        # Validate password policy
        is_valid, error_msg = validate_password_policy(data['password'])
        if not is_valid:
            return jsonify({'error': error_msg}), 400
        salt, password_hash = hash_password(data['password'])
        user['password_salt'] = salt
        user['password_hash'] = password_hash
        user['password_changed_at'] = datetime.now().isoformat()  # LW: reset expiry
    
    save_users(users_db)
    
    logging.info(f"Admin '{request.session['user']}' updated user '{username}'")
    log_audit(request.session['user'], 'user.updated', f"Updated user: {username}")
    
    return jsonify({'success': True})

@app.route('/api/users/<username>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def delete_user(username):
    """Delete a user (admin only)"""
    global users_db
    
    username = username.lower()
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    # Prevent deleting self
    if username == request.session['user']:
        return jsonify({'error': 'Cannot delete your own account'}), 400
    
    # Prevent deleting last admin
    user = users_db[username]
    if user['role'] == ROLE_ADMIN:
        admin_count = sum(1 for u in users_db.values() if u['role'] == ROLE_ADMIN)
        if admin_count <= 1:
            return jsonify({'error': 'Cannot delete last admin'}), 400
    
    # Mark admin initialized if deleting the default admin
    if user.get('is_default') or username == 'pegaprox':
        mark_admin_initialized()
    
    # NS: Fix - actually delete from database! Jan 2026
    try:
        db = get_db()
        db.delete_user(username)
        logging.info(f"Deleted user '{username}' from database")
    except Exception as e:
        logging.error(f"Failed to delete user from DB: {e}")
        return jsonify({'error': 'Failed to delete user'}), 500
    
    # Also remove from memory
    del users_db[username]
    
    # Invalidate any sessions for this user
    to_remove = [sid for sid, s in active_sessions.items() if s['user'] == username]
    for sid in to_remove:
        del active_sessions[sid]
    
    logging.info(f"Admin '{request.session['user']}' deleted user '{username}'")
    log_audit(request.session['user'], 'user.deleted', f"Deleted user: {username}")
    
    return jsonify({'success': True})


# ============================================
# Tenant Management API Routes
# Multi-tenancy - most requested feature on reddit
# on Reddit. MSPs use this to manage multiple customers separately.
# ============================================

@app.route('/api/tenants', methods=['GET'])
@require_auth()
def get_tenants():
    """Get tenants - admin sees all, users see only their tenant
    
    NS: Updated Dec 2025 - filter based on user role
    MK: Fixed session access, added fallback for edge cases
    """
    global tenants_db
    tenants_db = load_tenants()
    
    # get user info from session
    username = request.session.get('user', '')
    user_role = request.session.get('role', ROLE_VIEWER)
    
    # admin always sees all tenants - no filtering
    if user_role == ROLE_ADMIN:
        result = []
        for tid, t in tenants_db.items():
            result.append({
                'id': tid,
                'name': t.get('name', tid),
                'clusters': t.get('clusters', []),
                'created': t.get('created', ''),
                'user_count': sum(1 for u in load_users().values() if u.get('tenant_id') == tid)
            })
        return jsonify(result)
    
    # non-admin: load user to get tenant_id
    users = load_users()
    user = users.get(username, {})
    user_tenant = user.get('tenant_id', DEFAULT_TENANT_ID)
    
    result = []
    for tid, t in tenants_db.items():
        # user sees only their tenant + default tenant
        if tid != user_tenant and tid != DEFAULT_TENANT_ID:
            continue
        
        result.append({
            'id': tid,
            'name': t.get('name', tid),
            'clusters': t.get('clusters', []),
            'created': t.get('created', ''),
            'user_count': sum(1 for u in users.values() if u.get('tenant_id') == tid)
        })
    
    return jsonify(result)

@app.route('/api/tenants', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_tenant():
    """Create new tenant
    
    MK: Improved to handle duplicate names by adding suffix
    """
    global tenants_db
    
    data = request.json
    name = data.get('name', '').strip()
    clusters = data.get('clusters', [])
    
    if not name:
        return jsonify({'error': 'Name required'}), 400
    
    # generate ID from name
    import re
    base_tid = re.sub(r'[^a-z0-9]', '-', name.lower())
    base_tid = re.sub(r'-+', '-', base_tid).strip('-')
    
    if not base_tid:
        base_tid = 'tenant'
    
    tenants_db = load_tenants()
    
    # if ID exists, add numeric suffix
    tid = base_tid
    counter = 1
    while tid in tenants_db:
        tid = f"{base_tid}-{counter}"
        counter += 1
        if counter > 100:  # safety limit
            return jsonify({'error': 'Too many tenants with similar names'}), 409
    
    tenants_db[tid] = {
        'id': tid,
        'name': name,
        'clusters': clusters,
        'created': datetime.now().isoformat(),
    }
    
    save_tenants(tenants_db)
    log_audit(request.session['user'], 'tenant.created', f"Created tenant: {name} (id={tid})")
    
    return jsonify({'success': True, 'tenant': tenants_db[tid]})

@app.route('/api/tenants/<tenant_id>', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def update_tenant(tenant_id):
    """Update tenant"""
    global tenants_db
    
    tenants_db = load_tenants()
    
    if tenant_id not in tenants_db:
        return jsonify({'error': 'Tenant not found'}), 404
    
    data = request.json
    
    if 'name' in data:
        tenants_db[tenant_id]['name'] = data['name']
    if 'clusters' in data:
        tenants_db[tenant_id]['clusters'] = data['clusters']
    
    save_tenants(tenants_db)
    log_audit(request.session['user'], 'tenant.updated', f"Updated tenant: {tenant_id}")
    
    return jsonify({'success': True, 'tenant': tenants_db[tenant_id]})

@app.route('/api/tenants/<tenant_id>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def delete_tenant(tenant_id):
    """Delete tenant"""
    global tenants_db
    
    if tenant_id == DEFAULT_TENANT_ID:
        return jsonify({'error': 'Cannot delete default tenant'}), 400
    
    tenants_db = load_tenants()
    
    if tenant_id not in tenants_db:
        return jsonify({'error': 'Tenant not found'}), 404
    
    # check if users still assigned to this tenant
    users = load_users()
    users_in_tenant = [u for u, d in users.items() if d.get('tenant_id') == tenant_id]
    if users_in_tenant:
        return jsonify({'error': f'Tenant has {len(users_in_tenant)} users assigned. Reassign them first.'}), 400
    
    # Delete from database directly
    try:
        db = get_db()
        db.delete_tenant(tenant_id)
    except Exception as e:
        logging.error(f"Error deleting tenant from database: {e}")
        return jsonify({'error': 'Database error'}), 500
    
    # Update cache
    if tenant_id in tenants_db:
        del tenants_db[tenant_id]
    
    log_audit(request.session['user'], 'tenant.deleted', f"Deleted tenant: {tenant_id}")
    
    return jsonify({'success': True})


# ============================================
# Permission Management API Routes
# LW: For fine-grained access control
# ============================================

@app.route('/api/permissions', methods=['GET'])
@require_auth()
def get_all_permissions():
    """Get all available permissions"""
    result = []
    for perm, desc in PERMISSIONS.items():
        category = perm.split('.')[0]
        result.append({
            'permission': perm,
            'description': desc,
            'category': category
        })
    return jsonify(result)

@app.route('/api/permissions/roles', methods=['GET'])
@require_auth()
def get_role_permissions():
    """Get all roles - builtin + custom"""
    # builtin
    result = {
        'builtin': ROLE_PERMISSIONS,
        'custom': get_custom_roles()
    }
    return jsonify(result)


# ==================== CUSTOM ROLES API ====================
# custom role management

@app.route('/api/roles', methods=['GET'])
@require_auth()
def list_all_roles():
    """List all available roles (builtin + custom)"""
    custom = get_custom_roles()
    
    roles = []
    # builtins
    for role_id in BUILTIN_ROLES:
        roles.append({
            'id': role_id,
            'name': role_id.capitalize(),
            'builtin': True,
            'permissions': ROLE_PERMISSIONS.get(role_id, []),
            'scope': 'global'
        })
    
    # global custom
    for role_id, data in custom.get('global', {}).items():
        roles.append({
            'id': role_id,
            'name': data.get('name', role_id),
            'builtin': False,
            'permissions': data.get('permissions', []),
            'scope': 'global',
            'created_by': data.get('created_by')
        })
    
    # tenant-specific
    for tenant_id, tenant_roles in custom.get('tenants', {}).items():
        for role_id, data in tenant_roles.items():
            roles.append({
                'id': role_id,
                'name': data.get('name', role_id),
                'builtin': False,
                'permissions': data.get('permissions', []),
                'scope': 'tenant',
                'tenant_id': tenant_id,
                'created_by': data.get('created_by')
            })
    
    return jsonify(roles)


@app.route('/api/roles', methods=['POST'])
@require_auth(perms=['admin.roles'])
def create_custom_role():
    """Create a new custom role"""
    data = request.json or {}
    
    role_id = data.get('id', '').lower().strip()
    name = data.get('name', role_id)
    permissions = data.get('permissions', [])
    tenant_id = data.get('tenant_id')  # None = global role
    
    if not role_id:
        return jsonify({'error': 'Role ID required'}), 400
    
    # cant use builtin names
    if role_id in BUILTIN_ROLES:
        return jsonify({'error': 'Cannot use builtin role name'}), 400
    
    # validate role_id format
    if not role_id.replace('_', '').replace('-', '').isalnum():
        return jsonify({'error': 'Role ID must be alphanumeric'}), 400
    
    # validate perms
    for p in permissions:
        if p not in PERMISSIONS:
            return jsonify({'error': f'Invalid permission: {p}'}), 400
    
    custom = get_custom_roles()
    
    # Ensure tenants dict exists
    if 'tenants' not in custom:
        custom['tenants'] = {}
    if 'global' not in custom:
        custom['global'] = {}
    
    if tenant_id:
        # tenant-specific role
        if tenant_id not in custom['tenants']:
            custom['tenants'][tenant_id] = {}
        if role_id in custom['tenants'][tenant_id]:
            return jsonify({'error': 'Role already exists in this tenant'}), 400
        custom['tenants'][tenant_id][role_id] = {
            'name': name,
            'permissions': permissions,
            'created_by': request.session['user'],
            'created': datetime.now().isoformat()
        }
    else:
        # global role
        if role_id in custom['global']:
            return jsonify({'error': 'Global role already exists'}), 400
        custom['global'][role_id] = {
            'name': name,
            'permissions': permissions,
            'created_by': request.session['user'],
            'created': datetime.now().isoformat()
        }
    
    save_custom_roles(custom)
    invalidate_roles_cache()
    
    usr = request.session['user']
    scope = f"tenant:{tenant_id}" if tenant_id else "global"
    log_audit(usr, 'role.created', f"Created custom role: {role_id} ({scope})")
    
    return jsonify({'success': True, 'role_id': role_id})


@app.route('/api/roles/<role_id>', methods=['PUT'])
@require_auth(perms=['admin.roles'])
def update_custom_role(role_id):
    """Update a custom role"""
    if role_id in BUILTIN_ROLES:
        return jsonify({'error': 'Cannot modify builtin roles'}), 400
    
    data = request.json or {}
    name = data.get('name')
    permissions = data.get('permissions')
    tenant_id = data.get('tenant_id')  # which tenant's role to update
    
    custom = get_custom_roles()
    
    # find the role
    found = False
    if tenant_id:
        tenant_roles = custom.get('tenants', {}).get(tenant_id, {})
        if role_id in tenant_roles:
            if name: tenant_roles[role_id]['name'] = name
            if permissions is not None:
                # validate
                for p in permissions:
                    if p not in PERMISSIONS:
                        return jsonify({'error': f'Invalid permission: {p}'}), 400
                tenant_roles[role_id]['permissions'] = permissions
            tenant_roles[role_id]['modified'] = datetime.now().isoformat()
            found = True
    else:
        global_roles = custom.get('global', {})
        if role_id in global_roles:
            if name: global_roles[role_id]['name'] = name
            if permissions is not None:
                for p in permissions:
                    if p not in PERMISSIONS:
                        return jsonify({'error': f'Invalid permission: {p}'}), 400
                global_roles[role_id]['permissions'] = permissions
            global_roles[role_id]['modified'] = datetime.now().isoformat()
            found = True
    
    if not found:
        return jsonify({'error': 'Role not found'}), 404
    
    save_custom_roles(custom)
    invalidate_roles_cache()
    
    log_audit(request.session['user'], 'role.updated', f"Updated role: {role_id}")
    return jsonify({'success': True})


@app.route('/api/roles/<role_id>', methods=['DELETE'])
@require_auth(perms=['admin.roles'])
def delete_custom_role(role_id):
    """Delete a custom role"""
    if role_id in BUILTIN_ROLES:
        return jsonify({'error': 'Cannot delete builtin roles'}), 400
    
    tenant_id = request.args.get('tenant_id')
    
    custom = get_custom_roles()
    found = False
    
    if tenant_id:
        tenant_roles = custom.get('tenants', {}).get(tenant_id, {})
        if role_id in tenant_roles:
            del tenant_roles[role_id]
            found = True
    else:
        if role_id in custom.get('global', {}):
            del custom['global'][role_id]
            found = True
    
    if not found:
        return jsonify({'error': 'Role not found'}), 404
    
    save_custom_roles(custom)
    invalidate_roles_cache()
    
    log_audit(request.session['user'], 'role.deleted', f"Deleted role: {role_id}")
    return jsonify({'success': True})


# ==================== ROLE TEMPLATES API ====================
# predefined role configs for easy setup

@app.route('/api/roles/templates', methods=['GET'])
@require_auth()
def get_role_templates():
    """Get available role templates"""
    templates = []
    for tid, tpl in ROLE_TEMPLATES.items():
        templates.append({
            'id': tid,
            'name': tpl['name'],
            'description': tpl.get('description', ''),
            'permissions': tpl['permissions'],
            'permission_count': len(tpl['permissions'])
        })
    return jsonify(templates)


@app.route('/api/roles/templates/<template_id>/apply', methods=['POST'])
@require_auth(perms=['admin.roles'])
def apply_role_template(template_id):
    """Create a new role from a template"""
    if template_id not in ROLE_TEMPLATES:
        return jsonify({'error': 'Template not found'}), 404
    
    data = request.json or {}
    role_id = data.get('role_id', template_id)
    role_name = data.get('name', ROLE_TEMPLATES[template_id]['name'])
    tenant_id = data.get('tenant_id')  # None = global
    
    # validate role_id
    if role_id in BUILTIN_ROLES:
        return jsonify({'error': 'Cannot use builtin role name'}), 400
    
    custom = get_custom_roles()
    
    template = ROLE_TEMPLATES[template_id]
    role_data = {
        'name': role_name,
        'permissions': template['permissions'].copy(),
        'created_by': request.session['user'],
        'created': datetime.now().isoformat(),
        'from_template': template_id
    }
    
    if tenant_id:
        if 'tenants' not in custom:
            custom['tenants'] = {}
        if tenant_id not in custom['tenants']:
            custom['tenants'][tenant_id] = {}
        if role_id in custom['tenants'][tenant_id]:
            return jsonify({'error': 'Role already exists'}), 400
        custom['tenants'][tenant_id][role_id] = role_data
    else:
        if 'global' not in custom:
            custom['global'] = {}
        if role_id in custom['global']:
            return jsonify({'error': 'Role already exists'}), 400
        custom['global'][role_id] = role_data
    
    save_custom_roles(custom)
    invalidate_roles_cache()
    
    usr = request.session['user']
    scope = f"tenant:{tenant_id}" if tenant_id else "global"
    log_audit(usr, 'role.created_from_template', f"Created {role_id} from template {template_id} ({scope})")
    
    return jsonify({'success': True, 'role_id': role_id})


# ==================== VM ACCESS CONTROL API ====================
# per-VM permissions

@app.route('/api/clusters/<cluster_id>/vm-acls', methods=['GET'])
@require_auth(perms=['admin.users'])
def get_cluster_vm_acls(cluster_id):
    """Get VM ACLs for a cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    acls = get_vm_acls()
    cluster_acls = acls.get(cluster_id, {})
    
    # enrich with VM names if possible
    result = []
    for vmid, acl in cluster_acls.items():
        result.append({
            'vmid': int(vmid),
            'users': acl.get('users', []),
            'permissions': acl.get('permissions', []),
            'inherit_role': acl.get('inherit_role', True)
        })
    
    return jsonify(result)


@app.route('/api/clusters/<cluster_id>/vm-acls/<int:vmid>', methods=['GET'])
@require_auth(perms=['admin.users'])
def get_vm_acl(cluster_id, vmid):
    """Get ACL for a specific VM"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    acls = get_vm_acls()
    cluster_acls = acls.get(cluster_id, {})
    vm_acl = cluster_acls.get(str(vmid), {})
    
    return jsonify({
        'vmid': vmid,
        'users': vm_acl.get('users', []),
        'permissions': vm_acl.get('permissions', []),
        'inherit_role': vm_acl.get('inherit_role', True),
        'exists': bool(vm_acl)
    })


@app.route('/api/clusters/<cluster_id>/vm-acls/<int:vmid>', methods=['PUT'])
@require_auth(perms=['admin.users'])
def set_vm_acl(cluster_id, vmid):
    """Set ACL for a specific VM"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    data = request.json or {}
    users = data.get('users', [])
    permissions = data.get('permissions', [])
    inherit_role = data.get('inherit_role', True)
    
    # validate permissions
    for p in permissions:
        if p not in PERMISSIONS:
            return jsonify({'error': f'Invalid permission: {p}'}), 400
    
    acls = get_vm_acls()
    if cluster_id not in acls:
        acls[cluster_id] = {}
    
    acls[cluster_id][str(vmid)] = {
        'users': users,
        'permissions': permissions,
        'inherit_role': inherit_role,
        'modified': datetime.now().isoformat(),
        'modified_by': request.session['user']
    }
    
    save_vm_acls(acls)
    invalidate_vm_acls_cache()
    
    cluster_name = cluster_managers[cluster_id].config.name if cluster_id in cluster_managers else cluster_id
    log_audit(request.session['user'], 'vm.acl_updated', 
              f"VM {vmid} ACL updated: {len(users)} users, {len(permissions)} perms", 
              cluster=cluster_name)
    
    return jsonify({'success': True})


@app.route('/api/clusters/<cluster_id>/vm-acls/<int:vmid>', methods=['DELETE'])
@require_auth(perms=['admin.users'])
def delete_vm_acl(cluster_id, vmid):
    """Remove VM-specific ACL (use default permissions)"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    # NS: Fixed - was only deleting from dict, not from DB!
    # Now we delete directly from DB
    try:
        db = get_db()
        deleted = db.delete_vm_acl(cluster_id, vmid)
        
        if deleted:
            invalidate_vm_acls_cache()
            cluster_name = cluster_managers[cluster_id].config.name if cluster_id in cluster_managers else cluster_id
            log_audit(request.session['user'], 'vm.acl_deleted', f"VM {vmid} ACL removed", cluster=cluster_name)
        
        return jsonify({'success': True, 'deleted': deleted})
    except Exception as e:
        logging.error(f"Failed to delete VM ACL: {e}")
        return jsonify({'error': str(e)}), 500


# ==================== RESOURCE POOLS - MK Jan 2026 ====================

# Available pool permissions
POOL_PERMISSIONS = [
    'pool.view',        # View pool and members
    'vm.start',         # Start VMs in pool
    'vm.stop',          # Stop VMs in pool
    'vm.console',       # Access VM console
    'vm.config',        # Modify VM config
    'vm.snapshot',      # Create/delete snapshots
    'vm.backup',        # Create/restore backups
    'vm.migrate',       # Migrate VMs
    'vm.clone',         # Clone VMs
    'vm.delete',        # Delete VMs
    'pool.admin',       # Full admin access to pool
]


@app.route('/api/clusters/<cluster_id>/pools', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_cluster_pools(cluster_id):
    """Get all resource pools from Proxmox"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    pools = mgr.get_pools()
    
    # Add pool member details
    for pool in pools:
        try:
            details = mgr.get_pool_members(pool['poolid'])
            members = details.get('members', [])
            pool['members'] = members  # Include full members list for UI
            pool['member_count'] = len(members)
            pool['vms'] = len([m for m in members if m.get('type') in ('qemu', 'lxc')])
            pool['storage'] = len([m for m in members if m.get('type') == 'storage'])
        except:
            pool['members'] = []
            pool['member_count'] = 0
            pool['vms'] = 0
            pool['storage'] = 0
    
    # NS: Prevent caching to ensure fresh data after pool modifications
    response = jsonify(pools)
    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '0'
    return response


@app.route('/api/clusters/<cluster_id>/pools/<pool_id>', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_pool_details(cluster_id, pool_id):
    """Get pool details including members"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    pool_data = mgr.get_pool_members(pool_id)
    
    if not pool_data:
        return jsonify({'error': 'Pool not found'}), 404
    
    return jsonify(pool_data)


@app.route('/api/clusters/<cluster_id>/pools/<pool_id>/permissions', methods=['GET'])
@require_auth(perms=['admin.users'])
def get_pool_permissions_api(cluster_id, pool_id):
    """Get permissions for a pool"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    db = get_db()
    perms = db.get_pool_permissions(cluster_id, pool_id)
    
    return jsonify({
        'pool_id': pool_id,
        'permissions': perms,
        'available_permissions': POOL_PERMISSIONS
    })


@app.route('/api/clusters/<cluster_id>/pools/<pool_id>/permissions', methods=['POST'])
@require_auth(perms=['admin.users'])
def add_pool_permission_api(cluster_id, pool_id):
    """Add or update pool permission"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    data = request.json or {}
    subject_type = data.get('subject_type')  # 'user' or 'group'
    subject_id = data.get('subject_id')      # username or group name
    permissions = data.get('permissions', [])
    
    if not subject_type or not subject_id:
        return jsonify({'error': 'subject_type and subject_id required'}), 400
    
    if subject_type not in ('user', 'group'):
        return jsonify({'error': 'subject_type must be "user" or "group"'}), 400
    
    # Validate permissions
    invalid_perms = [p for p in permissions if p not in POOL_PERMISSIONS]
    if invalid_perms:
        return jsonify({'error': f'Invalid permissions: {invalid_perms}'}), 400
    
    db = get_db()
    success = db.save_pool_permission(cluster_id, pool_id, subject_type, subject_id, permissions)
    
    if success:
        cluster_name = cluster_managers[cluster_id].config.name if cluster_id in cluster_managers else cluster_id
        log_audit(request.session['user'], 'pool.permission_updated', 
                  f"Pool {pool_id}: {subject_type} '{subject_id}' permissions set to {permissions}", 
                  cluster=cluster_name)
        return jsonify({'success': True})
    else:
        return jsonify({'error': 'Failed to save permission'}), 500


@app.route('/api/clusters/<cluster_id>/pools/<pool_id>/permissions/<subject_type>/<subject_id>', methods=['DELETE'])
@require_auth(perms=['admin.users'])
def delete_pool_permission_api(cluster_id, pool_id, subject_type, subject_id):
    """Delete pool permission"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    db = get_db()
    deleted = db.delete_pool_permission(cluster_id, pool_id, subject_type, subject_id)
    
    if deleted:
        cluster_name = cluster_managers[cluster_id].config.name if cluster_id in cluster_managers else cluster_id
        log_audit(request.session['user'], 'pool.permission_deleted', 
                  f"Pool {pool_id}: {subject_type} '{subject_id}' permission removed", 
                  cluster=cluster_name)
    
    return jsonify({'success': True, 'deleted': deleted})


@app.route('/api/clusters/<cluster_id>/pool-permissions', methods=['GET'])
@require_auth(perms=['admin.users'])
def get_all_pool_permissions_api(cluster_id):
    """Get all pool permissions for a cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    db = get_db()
    perms = db.get_pool_permissions(cluster_id)
    
    # Group by pool
    by_pool = {}
    for p in perms:
        pool_id = p['pool_id']
        if pool_id not in by_pool:
            by_pool[pool_id] = []
        by_pool[pool_id].append(p)
    
    return jsonify({
        'permissions': by_pool,
        'available_permissions': POOL_PERMISSIONS
    })


@app.route('/api/clusters/<cluster_id>/pools/refresh-cache', methods=['POST'])
@require_auth(perms=['admin.users'])
def refresh_pool_cache_api(cluster_id):
    """Manually refresh the pool membership cache for a cluster
    
    MK: Useful when pools have been modified in Proxmox
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    # Invalidate and refresh
    invalidate_pool_cache(cluster_id)
    membership = get_pool_membership_cache(cluster_id)
    
    return jsonify({
        'success': True,
        'vms_in_pools': len(membership),
        'message': f'Cache refreshed - {len(membership)} VMs found in pools'
    })


# ============================================================================
# Pool Management API - NS Jan 2026
# Create, edit, delete pools and manage pool members directly from PegaProx
# ============================================================================

@app.route('/api/clusters/<cluster_id>/pools', methods=['POST'])
@require_auth(perms=['admin.users'])
def create_pool(cluster_id):
    """Create a new resource pool in Proxmox"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    data = request.get_json() or {}
    poolid = data.get('poolid', '').strip()
    comment = data.get('comment', '').strip()
    
    if not poolid:
        return jsonify({'error': 'Pool ID is required'}), 400
    
    # Validate pool ID (alphanumeric, dash, underscore only)
    import re
    if not re.match(r'^[a-zA-Z0-9_-]+$', poolid):
        return jsonify({'error': 'Pool ID can only contain letters, numbers, dashes and underscores'}), 400
    
    manager = cluster_managers.get(cluster_id)
    if not manager:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # Ensure connected
    if not manager.is_connected:
        if not manager.connect_to_proxmox():
            return jsonify({'error': 'Failed to connect to Proxmox cluster'}), 503
    
    try:
        # Create pool via Proxmox API
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/pools"
        
        api_data = {'poolid': poolid}
        if comment:
            api_data['comment'] = comment
        
        response = manager._api_post(url, data=api_data)
        
        if response.status_code not in [200, 201]:
            error_text = response.text
            if 'already exists' in error_text.lower():
                return jsonify({'error': f'Pool "{poolid}" already exists'}), 409
            return jsonify({'error': f'Proxmox API error: {error_text}'}), 500
        
        # Invalidate cache
        invalidate_pool_cache(cluster_id)
        
        audit_log(request.session.get('user'), 'pool.create', f'Created pool {poolid}', {'cluster': cluster_id, 'poolid': poolid})
        
        return jsonify({'success': True, 'poolid': poolid, 'message': f'Pool "{poolid}" created successfully'})
    except Exception as e:
        error_msg = str(e)
        if 'already exists' in error_msg.lower():
            return jsonify({'error': f'Pool "{poolid}" already exists'}), 409
        return jsonify({'error': f'Failed to create pool: {error_msg}'}), 500


@app.route('/api/clusters/<cluster_id>/pools/<pool_id>', methods=['PUT'])
@require_auth(perms=['admin.users'])
def update_pool(cluster_id, pool_id):
    """Update a pool's comment"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    data = request.get_json() or {}
    comment = data.get('comment', '')
    
    manager = cluster_managers.get(cluster_id)
    if not manager:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # Ensure connected
    if not manager.is_connected:
        if not manager.connect_to_proxmox():
            return jsonify({'error': 'Failed to connect to Proxmox cluster'}), 503
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/pools/{pool_id}"
        response = manager._api_put(url, data={'comment': comment})
        
        if response.status_code != 200:
            return jsonify({'error': f'Proxmox API error: {response.text}'}), 500
        
        audit_log(request.session.get('user'), 'pool.update', f'Updated pool {pool_id}', {'cluster': cluster_id, 'poolid': pool_id})
        
        return jsonify({'success': True, 'message': f'Pool "{pool_id}" updated successfully'})
    except Exception as e:
        return jsonify({'error': f'Failed to update pool: {str(e)}'}), 500


@app.route('/api/clusters/<cluster_id>/pools/<pool_id>', methods=['DELETE'])
@require_auth(perms=['admin.users'])
def delete_pool(cluster_id, pool_id):
    """Delete a resource pool"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager = cluster_managers.get(cluster_id)
    if not manager:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # Ensure connected
    if not manager.is_connected:
        if not manager.connect_to_proxmox():
            return jsonify({'error': 'Failed to connect to Proxmox cluster'}), 503
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/pools/{pool_id}"
        response = manager._api_delete(url)
        
        if response.status_code != 200:
            error_text = response.text
            if 'not empty' in error_text.lower() or 'contains' in error_text.lower():
                return jsonify({'error': 'Cannot delete pool - it still contains VMs or storage. Remove all members first.'}), 400
            return jsonify({'error': f'Proxmox API error: {error_text}'}), 500
        
        # Invalidate cache
        invalidate_pool_cache(cluster_id)
        
        # Also remove any PegaProx permissions for this pool
        conn = get_db()
        cursor = conn.cursor()
        cursor.execute('DELETE FROM pool_permissions WHERE cluster_id = ? AND pool_id = ?', (cluster_id, pool_id))
        conn.commit()
        conn.close()
        
        audit_log(request.session.get('user'), 'pool.delete', f'Deleted pool {pool_id}', {'cluster': cluster_id, 'poolid': pool_id})
        
        return jsonify({'success': True, 'message': f'Pool "{pool_id}" deleted successfully'})
    except Exception as e:
        error_msg = str(e)
        if 'not empty' in error_msg.lower() or 'contains' in error_msg.lower():
            return jsonify({'error': 'Cannot delete pool - it still contains VMs or storage. Remove all members first.'}), 400
        return jsonify({'error': f'Failed to delete pool: {error_msg}'}), 500


@app.route('/api/clusters/<cluster_id>/pools/<pool_id>/members', methods=['POST'])
@require_auth(perms=['admin.users'])
def add_pool_member(cluster_id, pool_id):
    """Add a VM/CT to a pool"""
    logging.info(f"add_pool_member called: cluster={cluster_id}, pool={pool_id}")
    
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    data = request.get_json() or {}
    vmid = data.get('vmid')
    vm_type = data.get('type', 'qemu')  # qemu or lxc
    
    logging.info(f"Request data: vmid={vmid}, type={vm_type}")
    
    if not vmid:
        return jsonify({'error': 'VMID is required'}), 400
    
    manager = cluster_managers.get(cluster_id)
    if not manager:
        logging.error(f"Cluster {cluster_id} not found in cluster_managers")
        return jsonify({'error': 'Cluster not found'}), 404
    
    logging.info(f"Manager found: is_connected={manager.is_connected}")
    
    # Ensure connected
    if not manager.is_connected:
        logging.info("Manager not connected, attempting to connect...")
        if not manager.connect_to_proxmox():
            logging.error("Failed to connect to Proxmox")
            return jsonify({'error': 'Failed to connect to Proxmox cluster'}), 503
    
    try:
        # Add VM to pool - Proxmox uses 'vms' parameter
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/pools/{pool_id}"
        
        logging.info(f"Adding VM {vmid} to pool {pool_id} on {host}")
        
        # Proxmox expects form data with string values
        response = manager._api_put(url, data={
            'vms': str(vmid)
        })
        
        logging.info(f"Proxmox response: {response.status_code} - {response.text[:200] if response.text else 'empty'}")
        
        if response.status_code != 200:
            error_text = response.text
            # Try to parse JSON error
            try:
                error_json = response.json()
                error_text = error_json.get('errors', {}).get('vms', error_text)
            except:
                pass
            return jsonify({'error': f'Proxmox API error: {error_text}'}), 500
        
        # Invalidate cache
        invalidate_pool_cache(cluster_id)
        
        audit_log(request.session.get('user'), 'pool.member.add', f'Added VM {vmid} to pool {pool_id}', 
                  {'cluster': cluster_id, 'poolid': pool_id, 'vmid': vmid})
        
        return jsonify({'success': True, 'message': f'VM {vmid} added to pool "{pool_id}"'})
    except Exception as e:
        return jsonify({'error': f'Failed to add VM to pool: {str(e)}'}), 500


@app.route('/api/clusters/<cluster_id>/pools/<pool_id>/members/<int:vmid>', methods=['DELETE'])
@require_auth(perms=['admin.users'])
def remove_pool_member(cluster_id, pool_id, vmid):
    """Remove a VM/CT from a pool"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager = cluster_managers.get(cluster_id)
    if not manager:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # Ensure connected
    if not manager.is_connected:
        if not manager.connect_to_proxmox():
            return jsonify({'error': 'Failed to connect to Proxmox cluster'}), 503
    
    try:
        # Remove VM from pool using DELETE parameter
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/pools/{pool_id}"
        response = manager._api_put(url, data={
            'vms': str(vmid),
            'delete': 1
        })
        
        if response.status_code != 200:
            return jsonify({'error': f'Proxmox API error: {response.text}'}), 500
        
        # Invalidate cache
        invalidate_pool_cache(cluster_id)
        
        audit_log(request.session.get('user'), 'pool.member.remove', f'Removed VM {vmid} from pool {pool_id}', 
                  {'cluster': cluster_id, 'poolid': pool_id, 'vmid': vmid})
        
        return jsonify({'success': True, 'message': f'VM {vmid} removed from pool "{pool_id}"'})
    except Exception as e:
        return jsonify({'error': f'Failed to remove VM from pool: {str(e)}'}), 500


@app.route('/api/clusters/<cluster_id>/vms-without-pool', methods=['GET'])
@require_auth()
def get_vms_without_pool(cluster_id):
    """Get all VMs that are not in any pool - useful for pool assignment UI"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager = cluster_managers.get(cluster_id)
    if not manager:
        return jsonify({'error': 'Cluster not found'}), 404
    
    try:
        # Get all VMs
        all_vms = manager.get_vm_resources()
        
        # Get pool membership
        membership = get_pool_membership_cache(cluster_id)
        
        # Filter VMs not in any pool
        vms_without_pool = []
        for vm in all_vms:
            vmid = vm.get('vmid')
            vm_type = vm.get('type', 'qemu')
            key = f"{vmid}:{vm_type}"
            
            if key not in membership:
                vms_without_pool.append({
                    'vmid': vmid,
                    'name': vm.get('name', f'VM {vmid}'),
                    'type': vm_type,
                    'status': vm.get('status', 'unknown'),
                    'node': vm.get('node', '')
                })
        
        return jsonify(vms_without_pool)
    except Exception as e:
        return jsonify({'error': str(e)}), 500


def check_pool_permission(cluster_id: str, vmid: int, vm_type: str, required_perm: str, user: str = None) -> bool:
    """Check if user has permission for a VM via pool permissions
    
    Returns True if user has the required permission through pool membership
    """
    if user is None:
        user = getattr(request, 'session', {}).get('user')
    
    if not user:
        return False
    
    # Admins always have access
    users = load_users()
    user_data = users.get(user, {})
    if user_data.get('role') == ROLE_ADMIN:
        return True
    
    # Get the pool this VM belongs to
    if cluster_id not in cluster_managers:
        return False
    
    mgr = cluster_managers[cluster_id]
    pool_id = mgr.get_vm_pool(vmid, vm_type)
    
    if not pool_id:
        return False  # VM not in any pool
    
    # Get user's groups
    user_groups = user_data.get('groups', [])
    
    # Get user's pool permissions
    db = get_db()
    user_pool_perms = db.get_user_pool_permissions(cluster_id, user, user_groups)
    
    # Check if user has required permission for this pool
    pool_perms = user_pool_perms.get(pool_id, [])
    
    # pool.admin grants all permissions
    if 'pool.admin' in pool_perms:
        return True
    
    return required_perm in pool_perms


@app.route('/api/users/<username>/vm-access', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_user_vm_access(username):
    """Get all VMs a user has explicit access to"""
    users = load_users()
    if username not in users:
        return jsonify({'error': 'User not found'}), 404
    
    acls = get_vm_acls()
    access = []
    
    for cluster_id, cluster_acls in acls.items():
        for vmid, acl in cluster_acls.items():
            if username in acl.get('users', []) or '*' in acl.get('users', []):
                access.append({
                    'cluster_id': cluster_id,
                    'vmid': int(vmid),
                    'permissions': acl.get('permissions', []),
                    'inherit_role': acl.get('inherit_role', True)
                })
    
    return jsonify(access)


# ==================== PER-TENANT USER PERMISSIONS ====================

@app.route('/api/users/<username>/permissions', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_user_perms(username):
    """Get effective permissions for a user"""
    users = load_users()
    
    if username not in users:
        return jsonify({'error': 'User not found'}), 404
    
    user = users[username]
    tenant_id = request.args.get('tenant_id', user.get('tenant_id', DEFAULT_TENANT_ID))
    
    effective = get_user_permissions(user, tenant_id)
    effective_role = get_user_effective_role(user, tenant_id)
    
    return jsonify({
        'username': username,
        'role': user.get('role'),
        'effective_role': effective_role,
        'tenant_id': tenant_id,
        'tenant_permissions': user.get('tenant_permissions', {}),
        'role_permissions': get_role_permissions_for_user(user, tenant_id),
        'extra_permissions': user.get('permissions', []),
        'denied_permissions': user.get('denied_permissions', []),
        'effective_permissions': effective
    })

@app.route('/api/users/<username>/permissions', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def set_user_perms(username):
    """Set user-specific permissions (global or per-tenant)"""
    global users_db
    
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    data = request.json or {}
    tenant_id = data.get('tenant_id')  # if set, update tenant-specific perms
    
    if tenant_id:
        # per-tenant permissions
        role = data.get('role')
        extra = data.get('extra', [])
        denied = data.get('denied', [])
        
        # validate
        for p in extra + denied:
            if p not in PERMISSIONS:
                return jsonify({'error': f'Invalid permission: {p}'}), 400
        
        if 'tenant_permissions' not in users_db[username]:
            users_db[username]['tenant_permissions'] = {}
        
        users_db[username]['tenant_permissions'][tenant_id] = {
            'role': role or users_db[username].get('role', ROLE_VIEWER),
            'extra': extra,
            'denied': denied
        }
        
        log_audit(request.session['user'], 'user.tenant_perms_changed', 
                  f"Changed tenant permissions for {username} in {tenant_id}")
    else:
        # global permissions (old behavior)
        extra = data.get('permissions', [])
        denied = data.get('denied_permissions', [])
        
        for p in extra + denied:
            if p not in PERMISSIONS:
                return jsonify({'error': f'Invalid permission: {p}'}), 400
        
        users_db[username]['permissions'] = extra
        users_db[username]['denied_permissions'] = denied
        
        log_audit(request.session['user'], 'user.permissions_changed', 
                  f"Changed permissions for: {username}")
    
    save_users(users_db)
    
    return jsonify({
        'success': True,
        'effective_permissions': get_user_permissions(users_db[username], tenant_id)
    })


@app.route('/api/users/<username>/tenant-permissions/<tenant_id>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def remove_user_tenant_perms(username, tenant_id):
    """Remove tenant-specific permissions for a user (revert to global)"""
    global users_db
    users_db = load_users()
    
    if username not in users_db:
        return jsonify({'error': 'User not found'}), 404
    
    tp = users_db[username].get('tenant_permissions', {})
    if tenant_id in tp:
        del tp[tenant_id]
        save_users(users_db)
        log_audit(request.session['user'], 'user.tenant_perms_removed', 
                  f"Removed tenant permissions for {username} in {tenant_id}")
    
    return jsonify({'success': True})

@app.route('/api/me/permissions', methods=['GET'])
@require_auth()
def get_my_permissions():
    """Get current user's permissions"""
    users = load_users()
    user = users.get(request.session['user'], {})
    tenant_id = request.args.get('tenant_id', user.get('tenant_id', DEFAULT_TENANT_ID))
    
    return jsonify({
        'role': user.get('role'),
        'effective_role': get_user_effective_role(user, tenant_id),
        'tenant_id': tenant_id,
        'tenant_permissions': user.get('tenant_permissions', {}),
        'permissions': get_user_permissions(user, tenant_id)
    })


# ============================================
# Server Settings API Routes
# MK: these were added in v1.2
# ============================================

def load_server_settings():
    """Load server settings from SQLite database
    
    SQLite migration
    """
    defaults = {
        'domain': '',
        'port': 5000,  # Web server port
        'ssl_enabled': False,
        'logo_url': '',
        'app_name': 'PegaProx',
        # HTTP redirect port - NS Jan 2026
        # Now that we have protocol detection on the main port, this is only needed
        # if you want HTTP:80 → HTTPS:5000 redirect
        # 0 = auto (80 if root, disabled otherwise), -1 = disabled, or specific port
        'http_redirect_port': -1,  # Disabled by default - protocol detection handles same-port redirect
        # Brute force protection settings
        'login_max_attempts': 5,
        'login_lockout_time': 300,  # 5 min
        'login_attempt_window': 600,  # 10 min
        # Password policy settings
        'password_min_length': 8,
        'password_require_uppercase': True,
        'password_require_lowercase': True,
        'password_require_numbers': True,
        'password_require_special': False,  # too annoying for most users
        # LW: Password expiry - Dec 2025
        'password_expiry_enabled': False,  # disabled by default
        'password_expiry_days': 90,  # days until password expires
        'password_expiry_warning_days': 14,  # warn this many days before
        'password_expiry_email_enabled': True,  # send email notifications
        'password_expiry_include_admins': False,  # MK: opt-in for admins, otherwise they could lock themselves out
        # Session settings
        'session_timeout': SESSION_TIMEOUT,  # Use constant (8h HIPAA default)
        # NS: SMTP Settings - Dec 2025
        'smtp_enabled': False,
        'smtp_host': '',
        'smtp_port': 587,
        'smtp_user': '',
        'smtp_password': '',  # stored encrypted ideally
        'smtp_from_email': '',
        'smtp_from_name': 'PegaProx Alerts',
        'smtp_tls': True,
        'smtp_ssl': False,
        # Alert notification settings
        'alert_email_recipients': [],  # list of email addresses
        'alert_cooldown': 300,  # Don't send same alert within 5 min
        # IP Whitelisting - Jan 2026
        'ip_whitelist_enabled': False,
        'ip_whitelist': '',  # Comma-separated IPs/CIDRs
        'ip_blacklist': '',  # Comma-separated IPs/CIDRs (always blocked)
    }
    
    try:
        db = get_db()
        saved = db.get_server_settings()
        if saved:
            # Merge with defaults (so new fields are always present)
            return {**defaults, **saved}
    except Exception as e:
        logging.error(f"Error loading server settings from database: {e}")
        # Try legacy fallback
        if os.path.exists(SERVER_SETTINGS_FILE):
            try:
                with open(SERVER_SETTINGS_FILE, 'r') as f:
                    saved = json.load(f)
                return {**defaults, **saved}
            except:
                pass
    
    return defaults


def save_server_settings(settings):
    """Save server settings to SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        db.save_server_settings(settings)
        return True
    except Exception as e:
        logging.error(f"Error saving server settings: {e}")
        return False

# Logo upload feature removed - NS: was never used in production


# =============================================================================
# EXTERNAL AUTHENTICATION - LDAP & OAUTH2/OIDC
# NS: Feb 2026 - Support for Active Directory, OpenLDAP, and generic OIDC
# =============================================================================

# Provider type constants
AUTH_PROVIDER_OIDC = 'oidc'
AUTH_PROVIDER_LDAP_AD = 'ldap_ad'
AUTH_PROVIDER_LDAP_OPENLDAP = 'ldap_openldap'


class LDAPAuthenticator:
    """LDAP/Active Directory authentication handler

    Supports both Active Directory and OpenLDAP with automatic detection
    of group membership retrieval method.

    Config schema:
    {
        'server': 'ldaps://ldap.example.com:636',
        'use_starttls': False,
        'bind_dn': 'CN=service,DC=example,DC=com',
        'bind_password': '...',
        'base_dn': 'DC=example,DC=com',
        'user_search_base': 'OU=Users,DC=example,DC=com',  # Optional, defaults to base_dn
        'user_search_filter': '(sAMAccountName={username})',  # AD default
        'user_attributes': {
            'username': 'sAMAccountName',
            'email': 'mail',
            'display_name': 'displayName',
            'groups': 'memberOf'  # AD only
        },
        'group_search_base': 'OU=Groups,DC=example,DC=com',  # For OpenLDAP
        'group_search_filter': '(member={user_dn})',  # For OpenLDAP
        'timeout': 10,
        'ca_cert_path': None  # Optional: custom CA certificate
    }
    """

    def __init__(self, config: dict, provider_type: str = AUTH_PROVIDER_LDAP_AD):
        self.config = config
        self.is_ad = provider_type == AUTH_PROVIDER_LDAP_AD

    def authenticate(self, username: str, password: str) -> dict:
        """Authenticate user against LDAP server

        Args:
            username: Username to authenticate
            password: Password to verify

        Returns:
            dict with keys:
                'success': bool
                'user_dn': str (if successful)
                'attributes': dict (email, display_name, etc.)
                'groups': list (group DNs or names)
                'error': str (if failed)
        """
        if not LDAP_AVAILABLE:
            return {'success': False, 'error': 'LDAP library not installed'}

        if not password:
            return {'success': False, 'error': 'Password required'}

        try:
            # First, bind with service account to search for user
            conn = self._get_connection()
            if not conn:
                return {'success': False, 'error': 'Could not connect to LDAP server'}

            # Search for user
            user_result = self._search_user(conn, username)
            if not user_result:
                conn.unbind_s()
                return {'success': False, 'error': 'User not found'}

            user_dn = user_result['dn']
            attributes = user_result['attributes']

            # Now try to bind as the user to verify password
            try:
                user_conn = self._get_connection(bind_dn=user_dn, bind_password=password)
                if user_conn:
                    user_conn.unbind_s()
                else:
                    conn.unbind_s()
                    return {'success': False, 'error': 'Invalid credentials'}
            except Exception as e:
                conn.unbind_s()
                logging.debug(f"LDAP user bind failed: {e}")
                return {'success': False, 'error': 'Invalid credentials'}

            # Get user groups
            groups = self._get_user_groups(conn, user_dn, attributes)

            conn.unbind_s()

            return {
                'success': True,
                'user_dn': user_dn,
                'external_id': user_dn,
                'attributes': attributes,
                'groups': groups
            }

        except Exception as e:
            logging.error(f"LDAP authentication error: {e}")
            return {'success': False, 'error': str(e)}

    def _get_connection(self, bind_dn: str = None, bind_password: str = None):
        """Create LDAP connection with proper TLS settings

        Args:
            bind_dn: DN to bind with (defaults to service account)
            bind_password: Password for bind

        Returns:
            ldap.LDAPObject or None
        """
        if not LDAP_AVAILABLE:
            return None

        try:
            server = self.config.get('server', '')
            timeout = self.config.get('timeout', 10)

            # Set global options
            ldap.set_option(ldap.OPT_NETWORK_TIMEOUT, timeout)
            ldap.set_option(ldap.OPT_TIMEOUT, timeout)

            # Handle CA certificate
            ca_cert = self.config.get('ca_cert_path')
            if ca_cert and os.path.exists(ca_cert):
                ldap.set_option(ldap.OPT_X_TLS_CACERTFILE, ca_cert)

            # Create connection
            conn = ldap.initialize(server)
            conn.set_option(ldap.OPT_PROTOCOL_VERSION, 3)
            conn.set_option(ldap.OPT_REFERRALS, 0)

            # Handle StartTLS for non-ldaps connections
            if self.config.get('use_starttls') and not server.startswith('ldaps://'):
                conn.start_tls_s()

            # Bind
            dn = bind_dn or self.config.get('bind_dn', '')
            pw = bind_password or self.config.get('bind_password', '')

            if dn and pw:
                conn.simple_bind_s(dn, pw)
            else:
                # Anonymous bind
                conn.simple_bind_s('', '')

            return conn

        except ldap.INVALID_CREDENTIALS:
            logging.debug("LDAP invalid credentials")
            return None
        except ldap.SERVER_DOWN:
            logging.error(f"LDAP server down: {self.config.get('server')}")
            return None
        except Exception as e:
            logging.error(f"LDAP connection error: {e}")
            return None

    def _search_user(self, conn, username: str) -> dict:
        """Search for user in LDAP

        Args:
            conn: LDAP connection
            username: Username to search for

        Returns:
            dict with 'dn' and 'attributes' or None
        """
        try:
            base_dn = self.config.get('user_search_base') or self.config.get('base_dn', '')
            search_filter = self.config.get('user_search_filter', '(sAMAccountName={username})')

            # Replace placeholder with actual username (escape special chars)
            search_filter = search_filter.replace('{username}', ldap.filter.escape_filter_chars(username))

            # Attributes to retrieve
            attr_map = self.config.get('user_attributes', {})
            attrs = list(attr_map.values()) if attr_map else ['*']

            results = conn.search_s(base_dn, ldap.SCOPE_SUBTREE, search_filter, attrs)

            for dn, entry in results:
                if dn:  # Skip referrals
                    attributes = {}
                    for key, ldap_attr in attr_map.items():
                        if ldap_attr in entry:
                            val = entry[ldap_attr]
                            if isinstance(val, list):
                                # Decode bytes if necessary
                                val = [v.decode('utf-8') if isinstance(v, bytes) else v for v in val]
                                attributes[key] = val[0] if len(val) == 1 else val
                            else:
                                attributes[key] = val.decode('utf-8') if isinstance(val, bytes) else val

                    # Store raw memberOf for AD
                    if 'memberOf' in entry:
                        attributes['_memberOf'] = [
                            m.decode('utf-8') if isinstance(m, bytes) else m
                            for m in entry['memberOf']
                        ]

                    return {'dn': dn, 'attributes': attributes}

            return None

        except Exception as e:
            logging.error(f"LDAP user search error: {e}")
            return None

    def _get_user_groups(self, conn, user_dn: str, attributes: dict) -> list:
        """Get user's group memberships

        AD: Uses memberOf attribute (recursive group membership)
        OpenLDAP: Searches for groups containing the user

        Args:
            conn: LDAP connection
            user_dn: User's distinguished name
            attributes: User attributes (may contain _memberOf for AD)

        Returns:
            List of group DNs or names
        """
        groups = []

        if self.is_ad:
            # Active Directory - use memberOf attribute
            if '_memberOf' in attributes:
                groups = attributes['_memberOf']
        else:
            # OpenLDAP - search for groups
            try:
                group_base = self.config.get('group_search_base') or self.config.get('base_dn', '')
                group_filter = self.config.get('group_search_filter', '(member={user_dn})')
                group_filter = group_filter.replace('{user_dn}', ldap.filter.escape_filter_chars(user_dn))

                results = conn.search_s(group_base, ldap.SCOPE_SUBTREE, group_filter, ['cn', 'dn'])

                for dn, entry in results:
                    if dn:
                        groups.append(dn)

            except Exception as e:
                logging.error(f"LDAP group search error: {e}")

        return groups

    def test_connection(self) -> dict:
        """Test LDAP connection and binding

        Returns:
            dict with 'success' and 'message' or 'error'
        """
        if not LDAP_AVAILABLE:
            return {'success': False, 'error': 'LDAP library not installed (pip install python-ldap)'}

        try:
            conn = self._get_connection()
            if conn:
                # Try a simple search to verify
                base_dn = self.config.get('base_dn', '')
                conn.search_s(base_dn, ldap.SCOPE_BASE, '(objectClass=*)', ['1.1'])
                conn.unbind_s()
                return {'success': True, 'message': 'Connection successful'}
            else:
                return {'success': False, 'error': 'Could not establish connection'}

        except ldap.INVALID_CREDENTIALS:
            return {'success': False, 'error': 'Invalid bind credentials'}
        except ldap.SERVER_DOWN:
            return {'success': False, 'error': 'Server unreachable'}
        except ldap.NO_SUCH_OBJECT:
            return {'success': False, 'error': 'Base DN does not exist'}
        except Exception as e:
            return {'success': False, 'error': str(e)}


class OIDCAuthenticator:
    """OAuth2/OIDC authentication handler

    Supports generic OIDC providers (Keycloak, Authentik, Okta, etc.)
    Uses authorization code flow with PKCE for security.

    Config schema:
    {
        'issuer': 'https://idp.example.com/realms/myrealm',
        'client_id': 'pegaprox',
        'client_secret': '...',
        'scopes': ['openid', 'profile', 'email', 'groups'],
        'claims': {
            'username': 'preferred_username',
            'email': 'email',
            'display_name': 'name',
            'groups': 'groups'
        },
        'verify_ssl': True
    }
    """

    def __init__(self, config: dict, provider_id: str):
        self.config = config
        self.provider_id = provider_id
        self._well_known = None
        self._jwks = None

    def _get_well_known(self) -> dict:
        """Fetch and cache OIDC well-known configuration"""
        if self._well_known:
            return self._well_known

        if not OIDC_AVAILABLE:
            return {}

        try:
            issuer = self.config.get('issuer', '').rstrip('/')
            well_known_url = f"{issuer}/.well-known/openid-configuration"

            verify_ssl = self.config.get('verify_ssl', True)
            response = requests.get(well_known_url, timeout=10, verify=verify_ssl)
            response.raise_for_status()

            self._well_known = response.json()
            return self._well_known

        except Exception as e:
            logging.error(f"Failed to fetch OIDC well-known config: {e}")
            return {}

    def get_authorization_url(self, redirect_uri: str, state: str = None) -> tuple:
        """Generate OAuth2 authorization URL with PKCE

        Args:
            redirect_uri: URL to redirect to after auth
            state: Optional state parameter for CSRF protection

        Returns:
            tuple of (authorization_url, state, code_verifier)
        """
        if not OIDC_AVAILABLE:
            return (None, None, None)

        well_known = self._get_well_known()
        auth_endpoint = well_known.get('authorization_endpoint')

        if not auth_endpoint:
            logging.error("No authorization_endpoint in OIDC configuration")
            return (None, None, None)

        # Generate PKCE code verifier and challenge
        code_verifier = base64.urlsafe_b64encode(os.urandom(32)).decode('utf-8').rstrip('=')
        code_challenge = create_s256_code_challenge(code_verifier)

        # Generate state if not provided
        if not state:
            state = base64.urlsafe_b64encode(os.urandom(16)).decode('utf-8').rstrip('=')

        # Build authorization URL
        params = {
            'client_id': self.config.get('client_id'),
            'redirect_uri': redirect_uri,
            'response_type': 'code',
            'scope': ' '.join(self.config.get('scopes', ['openid', 'profile', 'email'])),
            'state': state,
            'code_challenge': code_challenge,
            'code_challenge_method': 'S256'
        }

        auth_url = f"{auth_endpoint}?{urlencode(params)}"
        return (auth_url, state, code_verifier)

    def exchange_code(self, code: str, redirect_uri: str, code_verifier: str = None) -> dict:
        """Exchange authorization code for tokens

        Args:
            code: Authorization code from callback
            redirect_uri: Same redirect_uri used in authorization
            code_verifier: PKCE code verifier

        Returns:
            dict with access_token, id_token, etc. or error
        """
        if not OIDC_AVAILABLE:
            return {'error': 'OIDC library not installed'}

        well_known = self._get_well_known()
        token_endpoint = well_known.get('token_endpoint')

        if not token_endpoint:
            return {'error': 'No token_endpoint in OIDC configuration'}

        try:
            data = {
                'grant_type': 'authorization_code',
                'client_id': self.config.get('client_id'),
                'client_secret': self.config.get('client_secret'),
                'code': code,
                'redirect_uri': redirect_uri
            }

            if code_verifier:
                data['code_verifier'] = code_verifier

            verify_ssl = self.config.get('verify_ssl', True)
            response = requests.post(
                token_endpoint,
                data=data,
                timeout=10,
                verify=verify_ssl
            )

            if response.status_code != 200:
                logging.error(f"Token exchange failed: {response.text}")
                return {'error': f'Token exchange failed: {response.status_code}'}

            return response.json()

        except Exception as e:
            logging.error(f"Token exchange error: {e}")
            return {'error': str(e)}

    def get_user_info(self, access_token: str) -> dict:
        """Get user information from userinfo endpoint

        Args:
            access_token: Valid access token

        Returns:
            dict with user claims (sub, email, name, groups, etc.)
        """
        if not OIDC_AVAILABLE:
            return {}

        well_known = self._get_well_known()
        userinfo_endpoint = well_known.get('userinfo_endpoint')

        if not userinfo_endpoint:
            return {}

        try:
            verify_ssl = self.config.get('verify_ssl', True)
            response = requests.get(
                userinfo_endpoint,
                headers={'Authorization': f'Bearer {access_token}'},
                timeout=10,
                verify=verify_ssl
            )

            if response.status_code != 200:
                logging.error(f"Userinfo request failed: {response.text}")
                return {}

            return response.json()

        except Exception as e:
            logging.error(f"Userinfo error: {e}")
            return {}

    def validate_id_token(self, id_token: str) -> dict:
        """Validate and decode ID token

        Note: This does basic validation. For production, consider
        validating signature with provider's JWKS.

        Args:
            id_token: JWT ID token

        Returns:
            Decoded token claims or empty dict on error
        """
        if not OIDC_AVAILABLE:
            return {}

        try:
            # Decode without verification first to get claims
            # For production, should verify signature using JWKS
            claims = jwt.decode(
                id_token,
                options={"verify_signature": False}
            )

            # Basic validation
            issuer = self.config.get('issuer', '').rstrip('/')
            if claims.get('iss', '').rstrip('/') != issuer:
                logging.warning(f"ID token issuer mismatch: {claims.get('iss')} != {issuer}")

            if claims.get('aud') != self.config.get('client_id'):
                logging.warning(f"ID token audience mismatch")

            return claims

        except Exception as e:
            logging.error(f"ID token validation error: {e}")
            return {}

    def test_connection(self) -> dict:
        """Test OIDC provider configuration

        Returns:
            dict with 'success' and 'message' or 'error'
        """
        if not OIDC_AVAILABLE:
            return {'success': False, 'error': 'OIDC libraries not installed (pip install authlib httpx PyJWT)'}

        try:
            well_known = self._get_well_known()

            if not well_known:
                return {'success': False, 'error': 'Could not fetch well-known configuration'}

            required = ['authorization_endpoint', 'token_endpoint', 'issuer']
            missing = [k for k in required if k not in well_known]

            if missing:
                return {'success': False, 'error': f'Missing endpoints: {", ".join(missing)}'}

            return {
                'success': True,
                'message': 'OIDC configuration valid',
                'issuer': well_known.get('issuer'),
                'endpoints': {
                    'authorization': well_known.get('authorization_endpoint'),
                    'token': well_known.get('token_endpoint'),
                    'userinfo': well_known.get('userinfo_endpoint')
                }
            }

        except Exception as e:
            return {'success': False, 'error': str(e)}


def resolve_role_from_groups(provider_id: str, external_groups: list, tenant_id: str = None) -> dict:
    """Resolve PegaProx role from external group memberships

    Mappings are evaluated in priority order (lowest number first).
    First matching mapping wins.

    Args:
        provider_id: Auth provider ID
        external_groups: List of group names/DNs from external provider
        tenant_id: Optional tenant ID for tenant-specific mappings

    Returns:
        dict with 'role', 'tenant_id'
    """
    db = get_db()
    mappings = db.get_group_mappings(provider_id)

    # Mappings are already sorted by priority from database
    for mapping in mappings:
        external_group = mapping['external_group']

        # Check if any external group matches this mapping
        # Support both exact match and CN extraction for AD groups
        for group in external_groups:
            if group == external_group:
                return {
                    'role': mapping['role'],
                    'tenant_id': mapping.get('tenant_id', '_default')
                }
            # Also try matching just the CN part for AD groups
            # e.g., "CN=Admins,OU=Groups,DC=example,DC=com" matches "Admins"
            if group.startswith('CN='):
                cn = group.split(',')[0][3:]  # Extract CN value
                if cn == external_group:
                    return {
                        'role': mapping['role'],
                        'tenant_id': mapping.get('tenant_id', '_default')
                    }

    # No mapping found - use provider default
    provider = db.get_auth_provider(provider_id)
    if provider:
        return {
            'role': provider.get('default_role', 'viewer'),
            'tenant_id': tenant_id or '_default'
        }

    return {'role': 'viewer', 'tenant_id': '_default'}


def create_external_user(username: str, provider_id: str, external_id: str,
                        attributes: dict, groups: list, provider_type: str) -> bool:
    """Create a new user from external authentication

    Args:
        username: PegaProx username
        provider_id: Auth provider ID
        external_id: External unique ID (LDAP DN or OIDC sub)
        attributes: User attributes (email, display_name, etc.)
        groups: List of external groups
        provider_type: Provider type (oidc, ldap_ad, ldap_openldap)

    Returns:
        True on success
    """
    users_db = load_users()

    if username in users_db:
        return False  # User already exists

    # Resolve role from groups
    role_info = resolve_role_from_groups(provider_id, groups)

    # Create user with external auth source
    users_db[username] = {
        'password_salt': '',  # No local password
        'password_hash': '',
        'role': role_info['role'],
        'permissions': [],
        'tenant': role_info.get('tenant_id'),
        'created_at': datetime.now().isoformat(),
        'last_login': None,
        'auth_source': f"{provider_type}:{provider_id}",
        'enabled': True,
        'totp_secret_encrypted': '',
        'totp_pending_secret_encrypted': '',
        'totp_enabled': False,
        'force_password_change': False,
        'theme': '',
        'language': '',
        'ui_layout': 'modern'
    }

    save_users(users_db)

    # Link external identity
    db = get_db()
    db.link_external_identity(
        username=username,
        provider_id=provider_id,
        external_id=external_id,
        external_username=attributes.get('username'),
        external_email=attributes.get('email'),
        external_groups=groups
    )

    log_audit('system', 'user.auto_created',
              f"Auto-created user {username} from {provider_type} provider {provider_id}")

    return True


def sync_user_from_external(username: str, provider_id: str, auth_result: dict):
    """Update user from external provider on login

    Syncs group memberships and updates role if group mappings changed.

    Args:
        username: PegaProx username
        provider_id: Auth provider ID
        auth_result: Authentication result with groups, attributes
    """
    db = get_db()
    groups = auth_result.get('groups', [])
    external_id = auth_result.get('external_id') or auth_result.get('user_dn')

    # Update external identity groups
    if external_id:
        db.update_external_identity_groups(provider_id, external_id, groups)

    # Check if role should be updated based on new group memberships
    server_settings = load_server_settings()
    if server_settings.get('external_auth_sync_groups_on_login', True):
        role_info = resolve_role_from_groups(provider_id, groups)

        users_db = load_users()
        if username in users_db:
            current_role = users_db[username].get('role')
            new_role = role_info['role']

            if current_role != new_role:
                users_db[username]['role'] = new_role
                save_users(users_db)
                log_audit(username, 'user.role_synced',
                          f"Role updated from {current_role} to {new_role} based on external groups")


def authenticate_with_ldap(provider_id: str, username: str, password: str) -> dict:
    """Authenticate user against LDAP provider

    Args:
        provider_id: Auth provider ID
        username: Username
        password: Password

    Returns:
        Authentication result dict
    """
    db = get_db()
    provider = db.get_auth_provider(provider_id)

    if not provider or not provider.get('enabled'):
        return {'success': False, 'error': 'Provider not found or disabled'}

    provider_type = provider.get('type', AUTH_PROVIDER_LDAP_AD)
    config = provider.get('config', {})

    authenticator = LDAPAuthenticator(config, provider_type)
    return authenticator.authenticate(username, password)


def get_enabled_ldap_providers() -> list:
    """Get list of enabled LDAP providers

    Returns:
        List of provider dicts
    """
    db = get_db()
    all_providers = db.get_auth_providers(enabled_only=True)
    return [p for p in all_providers if p['type'] in (AUTH_PROVIDER_LDAP_AD, AUTH_PROVIDER_LDAP_OPENLDAP)]


def get_enabled_oidc_providers() -> list:
    """Get list of enabled OIDC providers

    Returns:
        List of provider dicts
    """
    db = get_db()
    all_providers = db.get_auth_providers(enabled_only=True)
    return [p for p in all_providers if p['type'] == AUTH_PROVIDER_OIDC]


# =====================================================
# UPDATE CHECKER
# Checks GitHub for new versions and allows auto-update
# =====================================================

GITHUB_VERSION_URL = "https://raw.githubusercontent.com/PegaProx/project-pegaprox/main/version.json"
GITHUB_REPO_URL = "https://github.com/PegaProx/project-pegaprox"

@app.route('/api/pegaprox/version', methods=['GET'])
@require_auth()
def get_pegaprox_version():
    """Get current PegaProx version"""
    return jsonify({
        'version': PEGAPROX_VERSION,
        'build': PEGAPROX_BUILD,
        'python_version': sys.version.split()[0],
        'gevent_available': GEVENT_AVAILABLE,
        'encryption_available': ENCRYPTION_AVAILABLE,
    })


# NS: Military Grade Encryption Status & Migration - Jan 2026
@app.route('/api/pegaprox/security/status', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_security_status():
    """Get encryption and security status"""
    db = get_db()
    
    # Count items that need migration
    users_needing_migration = 0
    clusters_needing_migration = 0
    
    try:
        users_db = load_users()
        for user in users_db.values():
            if needs_password_rehash(user.get('password_salt', ''), user.get('password_hash', '')):
                users_needing_migration += 1
    except:
        pass
    
    try:
        cursor = db.conn.cursor()
        cursor.execute('SELECT pass_encrypted, ssh_key_encrypted FROM clusters')
        for row in cursor.fetchall():
            if db._needs_reencrypt(row[0]):
                clusters_needing_migration += 1
            elif row[1] and db._needs_reencrypt(row[1]):
                clusters_needing_migration += 1
    except:
        pass
    
    # Get login rate limit settings
    login_settings = get_login_settings()
    
    return jsonify({
        'encryption': {
            'available': ENCRYPTION_AVAILABLE,
            'algorithm': 'AES-256-GCM' if ENCRYPTION_AVAILABLE else 'None',
            'key_size': '256-bit',
            'mode': 'GCM (Authenticated Encryption)',
        },
        'password_hashing': {
            'available': ARGON2_AVAILABLE,
            'algorithm': 'Argon2id' if ARGON2_AVAILABLE else 'PBKDF2-SHA256',
            'memory_cost': '64 MB' if ARGON2_AVAILABLE else 'N/A',
            'iterations': 3 if ARGON2_AVAILABLE else 600000,
        },
        'rate_limiting': {
            'login': {
                'enabled': True,
                'max_attempts': login_settings['max_attempts'],
                'lockout_time': login_settings['lockout_time'],
                'window': login_settings['attempt_window'],
            },
            'api': {
                'enabled': API_RATE_LIMIT > 0,
                'requests_per_window': API_RATE_LIMIT,
                'window_seconds': API_RATE_WINDOW,
                'active_clients': len(api_request_counts),
            }
        },
        'session_management': {
            'timeout_minutes': get_session_timeout() // 60,
            'active_sessions': len(active_sessions),
            'encrypted_storage': True,
            'secure_cookies': True,
        },
        'migration': {
            'users_pending': users_needing_migration,
            'clusters_pending': clusters_needing_migration,
            'total_pending': users_needing_migration + clusters_needing_migration,
            'auto_migration': True,
        },
        'features': {
            'aes_256_gcm': ENCRYPTION_AVAILABLE,
            'argon2id': ARGON2_AVAILABLE,
            'login_rate_limiting': True,
            'api_rate_limiting': API_RATE_LIMIT > 0,
            'secure_sessions': True,
            'csp_headers': True,
            'hsts': True,
        }
    })


@app.route('/api/pegaprox/security/migrate-all', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def migrate_all_encryption():
    """Force migration of all data to latest encryption
    
    NS: Migrates all passwords to Argon2id and all secrets to AES-256-GCM
    """
    if not ENCRYPTION_AVAILABLE:
        return jsonify({'error': 'Encryption not available'}), 400
    
    db = get_db()
    results = {
        'users_migrated': 0,
        'clusters_migrated': 0,
        'errors': []
    }
    
    # Migrate clusters
    try:
        cursor = db.conn.cursor()
        cursor.execute('SELECT id, pass_encrypted, ssh_key_encrypted FROM clusters')
        
        for row in cursor.fetchall():
            cluster_id = row[0]
            pass_encrypted = row[1]
            ssh_key_encrypted = row[2] or ''
            
            needs_update = False
            new_pass = pass_encrypted
            new_ssh_key = ssh_key_encrypted
            
            if db._needs_reencrypt(pass_encrypted):
                decrypted = db._decrypt(pass_encrypted)
                new_pass = db._encrypt(decrypted)
                needs_update = True
            
            if ssh_key_encrypted and db._needs_reencrypt(ssh_key_encrypted):
                decrypted = db._decrypt(ssh_key_encrypted)
                new_ssh_key = db._encrypt(decrypted)
                needs_update = True
            
            if needs_update:
                cursor.execute('''
                    UPDATE clusters SET pass_encrypted = ?, ssh_key_encrypted = ?, updated_at = ?
                    WHERE id = ?
                ''', (new_pass, new_ssh_key, datetime.now().isoformat(), cluster_id))
                results['clusters_migrated'] += 1
        
        db.conn.commit()
    except Exception as e:
        results['errors'].append(f"Cluster migration error: {e}")
    
    # Note: User password migration happens automatically on login
    # We can't migrate passwords without the original password
    results['users_note'] = 'User passwords will be migrated automatically on next login'
    
    user = request.session.get('user', 'unknown')
    log_audit(user, 'security.migration', f"Migrated {results['clusters_migrated']} clusters to AES-256-GCM")
    
    return jsonify({
        'success': True,
        'results': results,
        'message': f"Migrated {results['clusters_migrated']} clusters to Military Grade encryption"
    })


@app.route('/api/pegaprox/check-update', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def check_pegaprox_update():
    """Check for PegaProx updates from GitHub"""
    try:
        # Fetch version.json from GitHub
        response = requests.get(GITHUB_VERSION_URL, timeout=10)
        
        if response.status_code != 200:
            logging.warning(f"GitHub returned status {response.status_code} for update check")
            return jsonify({
                'error': f'GitHub returned status {response.status_code}',
                'current_version': PEGAPROX_VERSION,
                'current_build': PEGAPROX_BUILD,
                'update_available': False,
            }), 200  # Return 200 with error message so UI can still show current version
        
        try:
            remote_version = response.json()
        except Exception as json_err:
            logging.error(f"Failed to parse GitHub response: {json_err}")
            return jsonify({
                'error': 'Invalid response from GitHub',
                'current_version': PEGAPROX_VERSION,
                'current_build': PEGAPROX_BUILD,
                'update_available': False,
            }), 200
        
        current_version = PEGAPROX_VERSION.replace('Alpha ', '').replace('Beta ', '')
        latest_version = remote_version.get('version', '0.0')
        
        # Simple version comparison (works for semver-like versions)
        def parse_version(v):
            try:
                parts = str(v).replace('Alpha ', '').replace('Beta ', '').split('.')
                return tuple(int(p) for p in parts if p.isdigit())
            except:
                return (0, 0)
        
        current_tuple = parse_version(current_version)
        latest_tuple = parse_version(latest_version)
        
        update_available = latest_tuple > current_tuple
        
        return jsonify({
            'current_version': PEGAPROX_VERSION,
            'current_build': PEGAPROX_BUILD,
            'latest_version': remote_version.get('version'),
            'latest_build': remote_version.get('build'),
            'release_date': remote_version.get('release_date'),
            'changelog': remote_version.get('changelog', []),
            'download_url': remote_version.get('download_url', GITHUB_REPO_URL),
            'update_available': update_available,
            'min_python': remote_version.get('min_python', '3.8'),
            'breaking_changes': remote_version.get('breaking_changes', []),
        })
        
    except requests.exceptions.Timeout:
        logging.warning("Timeout checking for updates")
        return jsonify({
            'error': 'Timeout - GitHub not reachable',
            'current_version': PEGAPROX_VERSION,
            'current_build': PEGAPROX_BUILD,
            'update_available': False,
        }), 200
    except requests.exceptions.ConnectionError as e:
        logging.warning(f"Connection error checking updates: {e}")
        return jsonify({
            'error': 'Cannot connect to GitHub - check internet connection',
            'current_version': PEGAPROX_VERSION,
            'current_build': PEGAPROX_BUILD,
            'update_available': False,
        }), 200
    except requests.exceptions.RequestException as e:
        logging.warning(f"Request error checking updates: {e}")
        return jsonify({
            'error': f'Network error: {str(e)}',
            'current_version': PEGAPROX_VERSION,
            'current_build': PEGAPROX_BUILD,
            'update_available': False,
        }), 200
    except Exception as e:
        logging.error(f"Error checking updates: {e}")
        return jsonify({
            'error': str(e),
            'current_version': PEGAPROX_VERSION,
            'current_build': PEGAPROX_BUILD,
            'update_available': False,
        }), 200



@app.route('/api/pegaprox/update', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def perform_pegaprox_update():
    """Perform PegaProx update from GitHub
    
    NS: Full auto-update - Jan 2026
    Downloads ALL files (except protected paths), installs new packages, restarts server
    
    Protected paths (NEVER overwritten):
    - config/         (database, settings, encrypted data)
    - ssl/            (SSL certificates)
    - *.db            (local databases)
    - *.enc           (encrypted files)
    - *.pem           (SSL/TLS certificates)
    - *.key           (private keys)
    """
    try:
        data = request.json or {}
        force = data.get('force', False)
        
        # GitHub raw URL base
        REPO_BASE = "https://raw.githubusercontent.com/PegaProx/project-pegaprox/main"
        
        # Protected paths - NEVER overwrite these
        PROTECTED_PATTERNS = [
            'config/', 'ssl/', 'certs/',
            '.db', '.enc', '.pem', '.key', '.crt', '.p12'
        ]
        
        def is_protected(path):
            """Check if a path should be protected from updates"""
            path_lower = path.lower()
            for pattern in PROTECTED_PATTERNS:
                if pattern.endswith('/'):
                    # Directory pattern
                    if path_lower.startswith(pattern) or f'/{pattern}' in path_lower:
                        return True
                else:
                    # File extension pattern
                    if path_lower.endswith(pattern):
                        return True
            return False
        
        # Files to download (relative paths)
        # NS: Add new files here when they're added to the repo
        UPDATE_FILES = [
            ('pegaprox_multi_cluster.py', 'pegaprox_multi_cluster.py'),
            ('web/index.html', 'web/index.html'),
            ('requirements.txt', 'requirements.txt'),
            ('version.json', 'version.json'),
            ('deploy.sh', 'deploy.sh'),
            # Static files (optional, for offline mode)
            ('static/css/tailwind.min.css', 'static/css/tailwind.min.css'),
        ]
        
        # First check for updates
        try:
            response = requests.get(GITHUB_VERSION_URL, timeout=10)
        except requests.RequestException as e:
            return jsonify({
                'error': 'Cannot reach update server',
                'details': str(e),
                'hint': 'Check your internet connection or try again later'
            }), 503
        
        if response.status_code == 404:
            return jsonify({
                'error': 'Update server not found',
                'details': 'The GitHub repository may not be set up yet',
                'hint': 'Please check https://github.com/PegaProx/project-pegaprox'
            }), 404
        elif response.status_code != 200:
            return jsonify({
                'error': f'Failed to check for updates (HTTP {response.status_code})'
            }), 500
        
        try:
            remote_version = response.json()
        except:
            return jsonify({'error': 'Invalid version data from server'}), 500
        
        new_version = remote_version.get('version', '0.0')
        
        # Check if update is needed
        if not force:
            current = PEGAPROX_VERSION.replace('Alpha ', '').replace('Beta ', '')
            def parse_ver(v):
                try:
                    parts = str(v).replace('Alpha ', '').replace('Beta ', '').split('.')
                    return tuple(int(p) for p in parts if p.isdigit())
                except:
                    return (0, 0)
            
            if parse_ver(current) >= parse_ver(new_version):
                return jsonify({
                    'success': False,
                    'message': 'Already up to date',
                    'current_version': PEGAPROX_VERSION,
                    'latest_version': new_version
                })
        
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'pegaprox.update_started', f"Update to version {new_version} initiated")
        
        # Use file list from version.json if available, otherwise use defaults
        if 'update_files' in remote_version:
            UPDATE_FILES = [(f, f) for f in remote_version['update_files']]
        
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Create backup directory
        backup_dir = os.path.join(CONFIG_DIR, 'backups')
        os.makedirs(backup_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"pegaprox_backup_{PEGAPROX_VERSION.replace(' ', '_')}_{timestamp}"
        backup_path = os.path.join(backup_dir, backup_name)
        os.makedirs(backup_path, exist_ok=True)
        
        # Backup current files (only files we're going to update)
        backed_up_files = []
        for remote_path, local_path in UPDATE_FILES:
            # Skip protected files (they won't be updated anyway)
            if is_protected(local_path):
                continue
                
            full_path = os.path.join(current_dir, local_path)
            if os.path.exists(full_path):
                try:
                    backup_file_path = os.path.join(backup_path, local_path)
                    backup_dir_path = os.path.dirname(backup_file_path)
                    if backup_dir_path:  # Only makedirs if there's a directory part
                        os.makedirs(backup_dir_path, exist_ok=True)
                    shutil.copy2(full_path, backup_file_path)
                    backed_up_files.append((local_path, backup_file_path))
                    logging.info(f"Backed up: {local_path}")
                except Exception as e:
                    logging.warning(f"Could not backup {local_path}: {e}")
        
        logging.info(f"Backed up {len(backed_up_files)} files to {backup_path}")
        
        # MK: verify file hashes from version.json before installing
        file_hashes = remote_version.get('file_hashes', {})
        if not file_hashes:
            logging.warning("No file hashes in version.json - cant verify integrity!")
        else:
            logging.info(f"Hash verification enabled for {len(file_hashes)} files")
        
        # Download and replace files
        downloaded_files = []
        failed_files = []
        skipped_protected = []
        hash_failures = []
        
        for remote_path, local_path in UPDATE_FILES:
            # skip protected files (config, ssl, keys, etc.)
            if is_protected(local_path):
                skipped_protected.append(local_path)
                logging.info(f"Skipped (protected): {local_path}")
                continue
            
            url = f"{REPO_BASE}/{remote_path}"
            full_path = os.path.join(current_dir, local_path)
            
            try:
                logging.info(f"Downloading: {remote_path}")
                resp = requests.get(url, timeout=60)
                
                if resp.status_code == 200:
                    content = resp.text
                    
                    # MK: verify hash before writing - dont want to install tampered files
                    expected_hash = file_hashes.get(remote_path)
                    if expected_hash:
                        actual_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()
                        if actual_hash != expected_hash:
                            hash_failures.append((local_path, expected_hash[:16], actual_hash[:16]))
                            logging.error(f"Hash mismatch for {remote_path}!")
                            logging.error(f"  Expected: {expected_hash[:32]}...")
                            logging.error(f"  Got:      {actual_hash[:32]}...")
                            logging.error(f"  NOT INSTALLED - possible tampering!")
                            continue  # skip this file!
                        else:
                            logging.info(f"Hash verified: {local_path}")
                    
                    # Create directory if needed
                    dir_path = os.path.dirname(full_path)
                    if dir_path:
                        os.makedirs(dir_path, exist_ok=True)
                    
                    # Write to temp file first
                    temp_path = full_path + '.new'
                    with open(temp_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    
                    # Atomic replace
                    os.replace(temp_path, full_path)
                    downloaded_files.append(local_path)
                    logging.info(f"Updated: {local_path} ({len(content)} bytes)")
                    
                elif resp.status_code == 404:
                    # file doesn't exist in repo - skip (might be optional)
                    logging.debug(f"Skipped (not in repo): {remote_path}")
                else:
                    failed_files.append((local_path, f"HTTP {resp.status_code}"))
                    logging.warning(f"Failed to download {remote_path}: HTTP {resp.status_code}")
                    
            except Exception as e:
                failed_files.append((local_path, str(e)))
                logging.error(f"Error downloading {remote_path}: {e}")
        
        # LW: if any hash failed, abort the whole update
        if hash_failures:
            log_audit(user, 'pegaprox.update_aborted', f"Update aborted: {len(hash_failures)} files failed hash verification")
            return jsonify({
                'error': 'Update aborted due to integrity verification failure',
                'hash_failures': [{'file': f, 'expected': e, 'actual': a} for f, e, a in hash_failures],
                'security_warning': 'Downloaded files did not match expected hashes. This could indicate tampering.',
                'hint': 'If this persists, check GitHub repo integrity or report to maintainers.'
            }), 500
        
        # Install new Python packages
        pip_result = None
        requirements_path = os.path.join(current_dir, 'requirements.txt')
        if os.path.exists(requirements_path):
            try:
                logging.info("Installing Python packages from requirements.txt...")
                
                # MK: Multiple methods for pip install
                # 1. venv pip (preferred)
                # 2. sudo pip (if sudoers configured)
                # 3. direct pip if running as root
                # 4. pip with --user fallback
                
                venv_pip = os.path.join(current_dir, 'venv', 'bin', 'pip')
                venv_pip_win = os.path.join(current_dir, 'venv', 'Scripts', 'pip.exe')
                
                pip_result = None
                is_root = os.geteuid() == 0 if hasattr(os, 'geteuid') else False
                has_sudo = shutil.which('sudo') is not None
                
                # Method 1: Try venv pip first (preferred)
                if os.path.exists(venv_pip):
                    logging.info("Using venv pip...")
                    result = subprocess.run(
                        [venv_pip, 'install', '-r', requirements_path, '--quiet'],
                        capture_output=True, text=True, timeout=120
                    )
                    if result.returncode == 0:
                        pip_result = "success (venv)"
                        logging.info("Python packages installed successfully (venv)")
                    else:
                        logging.warning(f"venv pip failed: {result.stderr[:100]}")
                
                elif os.path.exists(venv_pip_win):
                    logging.info("Using venv pip (Windows)...")
                    result = subprocess.run(
                        [venv_pip_win, 'install', '-r', requirements_path, '--quiet'],
                        capture_output=True, text=True, timeout=120
                    )
                    if result.returncode == 0:
                        pip_result = "success (venv)"
                
                # Method 2: System pip
                if not pip_result:
                    system_pip = shutil.which('pip3') or shutil.which('pip')
                    if system_pip:
                        pip_args = [system_pip, 'install', '-r', requirements_path, '--quiet', '--break-system-packages']
                        
                        if is_root:
                            # Running as root - no sudo needed
                            logging.info("Running as root, using pip directly...")
                            result = subprocess.run(pip_args, capture_output=True, text=True, timeout=120)
                        elif has_sudo:
                            # Try sudo pip (works if sudoers is configured)
                            logging.info("Trying sudo pip install...")
                            result = subprocess.run(
                                ['sudo', '-n'] + pip_args,  # -n = non-interactive (no password prompt)
                                capture_output=True, text=True, timeout=120
                            )
                            if result.returncode != 0:
                                # Sudo failed, try --user
                                logging.info("sudo pip failed, trying --user...")
                                pip_args_user = [system_pip, 'install', '-r', requirements_path, '--user', '--quiet']
                                result = subprocess.run(pip_args_user, capture_output=True, text=True, timeout=120)
                        else:
                            # No sudo, try --user
                            logging.info("Trying pip install --user...")
                            pip_args_user = [system_pip, 'install', '-r', requirements_path, '--user', '--quiet']
                            result = subprocess.run(pip_args_user, capture_output=True, text=True, timeout=120)
                        
                        if result.returncode == 0:
                            pip_result = "success"
                            logging.info("Python packages installed successfully")
                        else:
                            pip_result = f"failed: {result.stderr[:100]}"
                            logging.warning(f"pip install failed: {result.stderr[:200]}")
                    else:
                        pip_result = "skipped (pip not found)"
                        logging.warning("pip not found, skipping package installation")
                    
            except subprocess.TimeoutExpired:
                pip_result = "timeout"
                logging.error("pip install timed out")
            except Exception as e:
                pip_result = f"error: {str(e)}"
                logging.error(f"pip install failed: {e}")
        
        log_audit(user, 'pegaprox.update_completed', f"Updated to version {new_version}, {len(downloaded_files)} files")
        
        # Schedule restart
        restart_delay = 3  # seconds
        
        def restart_server():
            time.sleep(restart_delay)
            logging.info("Restarting PegaProx server...")
            
            is_root = os.geteuid() == 0 if hasattr(os, 'geteuid') else False
            has_sudo = shutil.which('sudo') is not None
            
            # Try systemd first (if running as service)
            try:
                result = subprocess.run(['systemctl', 'is-active', 'pegaprox'], 
                                       capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    # Running as systemd service
                    if is_root:
                        subprocess.run(['systemctl', 'restart', 'pegaprox'], timeout=30)
                        return
                    elif has_sudo:
                        # Try sudo restart (works if sudoers is configured)
                        logging.info("Trying sudo systemctl restart...")
                        result = subprocess.run(
                            ['sudo', '-n', 'systemctl', 'restart', 'pegaprox'],
                            capture_output=True, text=True, timeout=30
                        )
                        if result.returncode == 0:
                            return
                        else:
                            logging.warning(f"sudo restart failed: {result.stderr}")
                    
                    # Fallback: just exit and let systemd restart us
                    logging.info("Exiting for systemd restart (Restart=always)...")
                    os._exit(0)
            except Exception as e:
                logging.warning(f"systemctl check failed: {e}")
            
            # Fallback: restart via Python
            try:
                os.execv(sys.executable, [sys.executable] + sys.argv)
            except Exception as e:
                logging.error(f"Failed to restart: {e}")
                os._exit(0)
        
        import threading
        threading.Thread(target=restart_server, daemon=True).start()
        
        return jsonify({
            'success': True,
            'message': f'Update to version {new_version} complete! Server restarting in {restart_delay} seconds...',
            'updated_version': new_version,
            'backup_path': backup_path,
            'files_updated': downloaded_files,
            'files_failed': failed_files,
            'files_protected': skipped_protected,
            'files_backed_up': len(backed_up_files),
            'pip_install': pip_result,
            'restarting': True,
            'restart_delay': restart_delay
        })
        
    except Exception as e:
        logging.error(f"Update error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/pegaprox/update/rollback', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def rollback_pegaprox_update():
    """Rollback to a previous PegaProx version from backup
    
    NS: Rollback functionality - Jan 2026
    """
    try:
        data = request.json or {}
        backup_name = data.get('backup')
        
        backup_dir = os.path.join(CONFIG_DIR, 'backups')
        
        if not backup_name:
            # List available backups
            backups = []
            if os.path.exists(backup_dir):
                for name in sorted(os.listdir(backup_dir), reverse=True):
                    backup_path = os.path.join(backup_dir, name)
                    if os.path.isdir(backup_path):
                        # Get backup info
                        files = os.listdir(backup_path)
                        backups.append({
                            'name': name,
                            'path': backup_path,
                            'files': files,
                            'created': datetime.fromtimestamp(os.path.getctime(backup_path)).isoformat()
                        })
            
            return jsonify({
                'backups': backups[:10],  # Last 10 backups
                'message': 'Select a backup to restore'
            })
        
        # Restore specific backup
        backup_path = os.path.join(backup_dir, backup_name)
        if not os.path.exists(backup_path):
            return jsonify({'error': 'Backup not found'}), 404
        
        current_dir = os.path.dirname(os.path.abspath(__file__))
        current_backend = os.path.abspath(__file__)
        current_frontend = os.path.join(current_dir, 'index.html')
        
        restored = []
        
        # Restore backend
        backup_backend = None
        for f in os.listdir(backup_path):
            if f.endswith('.py'):
                backup_backend = os.path.join(backup_path, f)
                break
        
        if backup_backend and os.path.exists(backup_backend):
            shutil.copy2(backup_backend, current_backend)
            restored.append('backend')
            logging.info(f"Restored backend from {backup_backend}")
        
        # Restore frontend
        backup_frontend = os.path.join(backup_path, 'index.html')
        if os.path.exists(backup_frontend):
            shutil.copy2(backup_frontend, current_frontend)
            restored.append('frontend')
            logging.info(f"Restored frontend from {backup_frontend}")
        
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'pegaprox.rollback', f"Rolled back to backup: {backup_name}")
        
        # Schedule restart
        def restart_server():
            time.sleep(3)
            is_root = os.geteuid() == 0 if hasattr(os, 'geteuid') else False
            has_sudo = shutil.which('sudo') is not None
            
            try:
                result = subprocess.run(['systemctl', 'is-active', 'pegaprox'], 
                                       capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    if is_root:
                        subprocess.run(['systemctl', 'restart', 'pegaprox'], timeout=30)
                        return
                    elif has_sudo:
                        result = subprocess.run(
                            ['sudo', '-n', 'systemctl', 'restart', 'pegaprox'],
                            capture_output=True, text=True, timeout=30
                        )
                        if result.returncode == 0:
                            return
                    # Fallback: exit for systemd restart
                    logging.info("Exiting for systemd restart...")
                    os._exit(0)
            except:
                pass
            try:
                os.execv(sys.executable, [sys.executable] + sys.argv)
            except:
                os._exit(0)
        
        import threading
        threading.Thread(target=restart_server, daemon=True).start()
        
        return jsonify({
            'success': True,
            'message': f'Rolled back to {backup_name}. Server restarting...',
            'restored': restored,
            'restarting': True
        })
        
    except Exception as e:
        logging.error(f"Rollback error: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/pegaprox/changelog', methods=['GET'])
@require_auth()
def get_pegaprox_changelog():
    """Get PegaProx changelog from GitHub"""
    try:
        response = requests.get(GITHUB_VERSION_URL, timeout=10)
        if response.status_code == 200:
            data = response.json()
            return jsonify({
                'changelog': data.get('changelog', []),
                'version': data.get('version'),
                'release_date': data.get('release_date'),
            })
        return jsonify({'error': 'Failed to fetch changelog'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# Serve static files (JS, CSS, fonts for offline mode)
STATIC_DIR = 'static'
Path(STATIC_DIR).mkdir(exist_ok=True)
Path(os.path.join(STATIC_DIR, 'js')).mkdir(exist_ok=True)
Path(os.path.join(STATIC_DIR, 'css')).mkdir(exist_ok=True)

@app.route('/static/<path:filename>')
def serve_static(filename):
    """Serve static files (JS, CSS, logo, etc.) for offline operation
    NS: Returns 404 with matching content-type to avoid MIME errors in browser
    """
    # Determine MIME type based on extension
    mime_types = {
        '.js': 'application/javascript',
        '.mjs': 'application/javascript',
        '.css': 'text/css',
        '.woff2': 'font/woff2',
        '.woff': 'font/woff',
        '.ttf': 'font/ttf',
        '.png': 'image/png',
        '.jpg': 'image/jpeg',
        '.svg': 'image/svg+xml',
        '.ico': 'image/x-icon',
        '.json': 'application/json',
        '.map': 'application/json'
    }
    ext = os.path.splitext(filename)[1].lower()
    mimetype = mime_types.get(ext, 'application/octet-stream')
    
    # Check if file exists
    filepath = os.path.join(STATIC_DIR, filename)
    if not os.path.isfile(filepath):
        # Return 404 with correct MIME type - prevents "not executable" errors
        return Response('', status=404, mimetype=mimetype)
    
    return send_from_directory(STATIC_DIR, filename, mimetype=mimetype)

# SECURITY: Block access to config directory
@app.route('/config/<path:filename>')
def block_config_access(filename):
    """Block any attempt to access config files via HTTP"""
    logging.warning(f"Blocked attempt to access config file: {filename} from {request.remote_addr}")
    return jsonify({'error': 'Access denied'}), 403

@app.route('/config')
def block_config_dir():
    """Block any attempt to access config directory"""
    logging.warning(f"Blocked attempt to list config directory from {request.remote_addr}")
    return jsonify({'error': 'Access denied'}), 403

# Serve images (logos, sponsors, etc.)
IMAGES_DIR = 'images'
Path(IMAGES_DIR).mkdir(exist_ok=True)

@app.route('/favicon.ico')
def serve_favicon():
    """serve favicon from images or static folder"""
    # try images first, then static
    for folder in [IMAGES_DIR, STATIC_DIR]:
        favicon_path = os.path.join(folder, 'favicon.ico')
        if os.path.exists(favicon_path):
            return send_from_directory(folder, 'favicon.ico', mimetype='image/x-icon')
    # return empty response if no favicon (prevents 404 spam in logs)
    return '', 204

@app.route('/images/<path:filename>')
def serve_images(filename):
    """Serve image files (pegaprox logo, sponsor logos, etc.)"""
    return send_from_directory(IMAGES_DIR, filename)


# ============================================
# EXTERNAL AUTHENTICATION PROVIDER MANAGEMENT
# NS: Feb 2026 - LDAP and OAuth2/OIDC provider configuration
# ============================================

@app.route('/api/auth/providers/public', methods=['GET'])
def get_public_auth_providers():
    """Get list of enabled auth providers for login page (public endpoint)

    Returns only provider ID, name, and type - no sensitive config.
    """
    try:
        db = get_db()
        providers = db.get_auth_providers(enabled_only=True)

        # Return only public info
        public_providers = [{
            'id': p['id'],
            'name': p['name'],
            'type': p['type']
        } for p in providers]

        return jsonify({
            'providers': public_providers,
            'local_enabled': True  # TODO: make configurable
        })
    except Exception as e:
        logging.error(f"Error getting public providers: {e}")
        return jsonify({'providers': [], 'local_enabled': True})


@app.route('/api/auth/providers', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_auth_providers():
    """Get all auth providers (admin only)

    Returns full provider info with masked secrets.
    """
    try:
        db = get_db()
        providers = db.get_auth_providers()

        # Mask sensitive fields
        for p in providers:
            config = p.get('config', {})
            if 'client_secret' in config:
                config['client_secret'] = '********' if config['client_secret'] else ''
            if 'bind_password' in config:
                config['bind_password'] = '********' if config['bind_password'] else ''
            p['config'] = config

        return jsonify({'providers': providers})

    except Exception as e:
        logging.error(f"Error getting auth providers: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/auth/providers/<provider_id>', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_auth_provider(provider_id):
    """Get single auth provider by ID (admin only)"""
    try:
        db = get_db()
        provider = db.get_auth_provider(provider_id)

        if not provider:
            return jsonify({'error': 'Provider not found'}), 404

        # Mask sensitive fields
        config = provider.get('config', {})
        if 'client_secret' in config:
            config['client_secret'] = '********' if config['client_secret'] else ''
        if 'bind_password' in config:
            config['bind_password'] = '********' if config['bind_password'] else ''
        provider['config'] = config

        return jsonify(provider)

    except Exception as e:
        logging.error(f"Error getting auth provider: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/auth/providers', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_auth_provider():
    """Create new auth provider (admin only)"""
    try:
        data = request.get_json()

        if not data:
            return jsonify({'error': 'No data provided'}), 400

        # Validate required fields
        if not data.get('name'):
            return jsonify({'error': 'Provider name is required'}), 400

        if not data.get('type') or data['type'] not in (AUTH_PROVIDER_OIDC, AUTH_PROVIDER_LDAP_AD, AUTH_PROVIDER_LDAP_OPENLDAP):
            return jsonify({'error': 'Invalid provider type'}), 400

        # Generate provider ID
        provider_id = data.get('id') or str(uuid.uuid4())[:8]

        # Build provider object
        provider = {
            'id': provider_id,
            'name': data['name'],
            'type': data['type'],
            'enabled': data.get('enabled', True),
            'priority': data.get('priority', 100),
            'default_role': data.get('default_role', 'viewer'),
            'auto_create_users': data.get('auto_create_users', False),
            'config': data.get('config', {})
        }

        db = get_db()
        db.save_auth_provider(provider)

        log_audit(session.get('user', 'admin'), 'auth_provider.created',
                  f"Created auth provider: {data['name']} ({data['type']})")

        return jsonify({'success': True, 'id': provider_id})

    except Exception as e:
        logging.error(f"Error creating auth provider: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/auth/providers/<provider_id>', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def update_auth_provider(provider_id):
    """Update existing auth provider (admin only)"""
    try:
        data = request.get_json()

        if not data:
            return jsonify({'error': 'No data provided'}), 400

        db = get_db()
        existing = db.get_auth_provider(provider_id)

        if not existing:
            return jsonify({'error': 'Provider not found'}), 404

        # Preserve existing secrets if masked values sent
        config = data.get('config', existing.get('config', {}))
        existing_config = existing.get('config', {})

        if config.get('client_secret') == '********':
            config['client_secret'] = existing_config.get('client_secret', '')
        if config.get('bind_password') == '********':
            config['bind_password'] = existing_config.get('bind_password', '')

        # Build updated provider
        provider = {
            'id': provider_id,
            'name': data.get('name', existing['name']),
            'type': data.get('type', existing['type']),
            'enabled': data.get('enabled', existing['enabled']),
            'priority': data.get('priority', existing['priority']),
            'default_role': data.get('default_role', existing['default_role']),
            'auto_create_users': data.get('auto_create_users', existing['auto_create_users']),
            'config': config
        }

        db.save_auth_provider(provider)

        log_audit(session.get('user', 'admin'), 'auth_provider.updated',
                  f"Updated auth provider: {provider['name']}")

        return jsonify({'success': True})

    except Exception as e:
        logging.error(f"Error updating auth provider: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/auth/providers/<provider_id>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def delete_auth_provider(provider_id):
    """Delete auth provider (admin only)"""
    try:
        db = get_db()
        provider = db.get_auth_provider(provider_id)

        if not provider:
            return jsonify({'error': 'Provider not found'}), 404

        db.delete_auth_provider(provider_id)

        log_audit(session.get('user', 'admin'), 'auth_provider.deleted',
                  f"Deleted auth provider: {provider['name']}")

        return jsonify({'success': True})

    except Exception as e:
        logging.error(f"Error deleting auth provider: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/auth/providers/<provider_id>/test', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def test_auth_provider(provider_id):
    """Test auth provider connection (admin only)"""
    try:
        db = get_db()
        provider = db.get_auth_provider(provider_id)

        if not provider:
            return jsonify({'error': 'Provider not found'}), 404

        provider_type = provider.get('type')
        config = provider.get('config', {})

        if provider_type == AUTH_PROVIDER_OIDC:
            authenticator = OIDCAuthenticator(config, provider_id)
        elif provider_type in (AUTH_PROVIDER_LDAP_AD, AUTH_PROVIDER_LDAP_OPENLDAP):
            authenticator = LDAPAuthenticator(config, provider_type)
        else:
            return jsonify({'error': 'Unknown provider type'}), 400

        result = authenticator.test_connection()

        return jsonify(result)

    except Exception as e:
        logging.error(f"Error testing auth provider: {e}")
        return jsonify({'success': False, 'error': str(e)})


# Group Mapping Endpoints

@app.route('/api/auth/providers/<provider_id>/mappings', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_group_mappings(provider_id):
    """Get group-to-role mappings for a provider (admin only)"""
    try:
        db = get_db()
        provider = db.get_auth_provider(provider_id)

        if not provider:
            return jsonify({'error': 'Provider not found'}), 404

        mappings = db.get_group_mappings(provider_id)

        return jsonify({'mappings': mappings})

    except Exception as e:
        logging.error(f"Error getting group mappings: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/auth/providers/<provider_id>/mappings', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_group_mapping(provider_id):
    """Add group-to-role mapping (admin only)"""
    try:
        data = request.get_json()

        if not data:
            return jsonify({'error': 'No data provided'}), 400

        if not data.get('external_group'):
            return jsonify({'error': 'External group is required'}), 400

        if not data.get('role'):
            return jsonify({'error': 'Role is required'}), 400

        db = get_db()
        provider = db.get_auth_provider(provider_id)

        if not provider:
            return jsonify({'error': 'Provider not found'}), 404

        mapping = {
            'provider_id': provider_id,
            'external_group': data['external_group'],
            'role': data['role'],
            'tenant_id': data.get('tenant_id', '_default'),
            'priority': data.get('priority', 100)
        }

        mapping_id = db.save_group_mapping(mapping)

        log_audit(session.get('user', 'admin'), 'group_mapping.created',
                  f"Created group mapping: {data['external_group']} -> {data['role']}")

        return jsonify({'success': True, 'id': mapping_id})

    except Exception as e:
        logging.error(f"Error creating group mapping: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/auth/providers/<provider_id>/mappings/<int:mapping_id>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def delete_group_mapping(provider_id, mapping_id):
    """Delete group-to-role mapping (admin only)"""
    try:
        db = get_db()
        success = db.delete_group_mapping(mapping_id)

        if not success:
            return jsonify({'error': 'Mapping not found'}), 404

        log_audit(session.get('user', 'admin'), 'group_mapping.deleted',
                  f"Deleted group mapping ID: {mapping_id}")

        return jsonify({'success': True})

    except Exception as e:
        logging.error(f"Error deleting group mapping: {e}")
        return jsonify({'error': str(e)}), 500


# User External Identity Endpoints

@app.route('/api/users/<username>/identities', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_user_identities(username):
    """Get external identities linked to a user (admin only)"""
    try:
        db = get_db()
        identities = db.get_user_external_identities(username)

        return jsonify({'identities': identities})

    except Exception as e:
        logging.error(f"Error getting user identities: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/users/<username>/identities/<int:identity_id>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def unlink_user_identity(username, identity_id):
    """Unlink external identity from user (admin only)"""
    try:
        db = get_db()
        success = db.unlink_external_identity(identity_id)

        if not success:
            return jsonify({'error': 'Identity not found'}), 404

        log_audit(session.get('user', 'admin'), 'external_identity.unlinked',
                  f"Unlinked external identity {identity_id} from user {username}")

        return jsonify({'success': True})

    except Exception as e:
        logging.error(f"Error unlinking identity: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/settings/server', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_server_settings():
    """Get server settings (admin only)"""
    settings = load_server_settings()
    # Add info about existing cert/key files
    settings['ssl_cert_exists'] = os.path.exists(SSL_CERT_FILE)
    settings['ssl_key_exists'] = os.path.exists(SSL_KEY_FILE)
    # Don't return actual cert/key content or SMTP password
    # Mask SMTP password if set
    if settings.get('smtp_password'):
        settings['smtp_password'] = '********'
    return jsonify(settings)


@app.route('/api/password-policy', methods=['GET'])
def get_password_policy():
    """Get password policy settings (public - needed for password change forms)
    
    NS: Jan 2026 - Returns only password-related settings, no auth required
    """
    settings = load_server_settings()
    return jsonify({
        'min_length': settings.get('password_min_length', 8),
        'require_uppercase': settings.get('password_require_uppercase', True),
        'require_lowercase': settings.get('password_require_lowercase', True),
        'require_numbers': settings.get('password_require_numbers', True),
        'require_special': settings.get('password_require_special', False),
        'expiry_days': settings.get('password_expiry_days', 0)
    })

@app.route('/api/settings/server', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def update_server_settings():
    """Update server settings (admin only)
    
    NS: Fixed Dec 2025 - now accepts both JSON and form-data
    """
    try:
        settings = load_server_settings()
        restart_required = False
        
        # check if JSON or form-data
        if request.is_json:
            data = request.get_json() or {}
            
            # server config
            if 'domain' in data:
                # MK: Auto-strip port from domain if present (user might accidentally include it)
                domain_value = data['domain'].strip()
                if domain_value and ':' in domain_value and not domain_value.startswith('['):
                    domain_value = domain_value.rsplit(':', 1)[0]
                settings['domain'] = domain_value
            if 'port' in data:
                new_port = int(data['port'])
                if settings.get('port') != new_port:
                    restart_required = True
                settings['port'] = new_port
            if 'http_redirect_port' in data:
                new_http_port = int(data['http_redirect_port'])
                if settings.get('http_redirect_port') != new_http_port:
                    restart_required = True
                settings['http_redirect_port'] = new_http_port
            if 'ssl_enabled' in data:
                new_ssl = bool(data['ssl_enabled'])
                if settings.get('ssl_enabled') != new_ssl:
                    restart_required = True
                settings['ssl_enabled'] = new_ssl
            
            # security/bruteforce settings
            if 'login_max_attempts' in data:
                settings['login_max_attempts'] = max(1, min(50, int(data['login_max_attempts'])))
            if 'login_lockout_time' in data:
                settings['login_lockout_time'] = max(30, min(86400, int(data['login_lockout_time'])))
            if 'login_attempt_window' in data:
                settings['login_attempt_window'] = max(60, min(3600, int(data['login_attempt_window'])))
            
            # password policy
            if 'password_min_length' in data:
                settings['password_min_length'] = max(4, min(64, int(data['password_min_length'])))
            if 'password_require_uppercase' in data:
                settings['password_require_uppercase'] = bool(data['password_require_uppercase'])
            if 'password_require_lowercase' in data:
                settings['password_require_lowercase'] = bool(data['password_require_lowercase'])
            if 'password_require_numbers' in data:
                settings['password_require_numbers'] = bool(data['password_require_numbers'])
            if 'password_require_special' in data:
                settings['password_require_special'] = bool(data['password_require_special'])
            
            # LW: password expiry settings - Dec 2025
            if 'password_expiry_enabled' in data:
                old_val = settings.get('password_expiry_enabled')
                settings['password_expiry_enabled'] = bool(data['password_expiry_enabled'])
                if old_val != settings['password_expiry_enabled']:
                    log_audit(request.session.get('user', 'admin'), 'settings.password_expiry', 
                              f"Password expiry {'enabled' if settings['password_expiry_enabled'] else 'disabled'}")
            if 'password_expiry_days' in data:
                settings['password_expiry_days'] = max(7, min(365, int(data['password_expiry_days'])))
            if 'password_expiry_warning_days' in data:
                settings['password_expiry_warning_days'] = max(1, min(30, int(data['password_expiry_warning_days'])))
            if 'password_expiry_email_enabled' in data:
                settings['password_expiry_email_enabled'] = bool(data['password_expiry_email_enabled'])
            if 'password_expiry_include_admins' in data:
                old_val = settings.get('password_expiry_include_admins')
                settings['password_expiry_include_admins'] = bool(data['password_expiry_include_admins'])
                if old_val != settings['password_expiry_include_admins']:
                    log_audit(request.session.get('user', 'admin'), 'settings.password_expiry', 
                              f"Admin password expiry {'enabled' if settings['password_expiry_include_admins'] else 'disabled'}")
            
            # session settings
            if 'session_timeout' in data:
                settings['session_timeout'] = max(300, min(604800, int(data['session_timeout'])))
            
            # NS: SMTP Settings - Jan 2026
            if 'smtp_enabled' in data:
                settings['smtp_enabled'] = bool(data['smtp_enabled'])
                logging.info(f"[Settings] Setting smtp_enabled = {settings['smtp_enabled']}")
            if 'smtp_host' in data:
                settings['smtp_host'] = str(data['smtp_host']).strip()
                logging.info(f"[Settings] Setting smtp_host = {settings['smtp_host']}")
            if 'smtp_port' in data:
                settings['smtp_port'] = max(1, min(65535, int(data['smtp_port'] or 587)))
                logging.info(f"[Settings] Setting smtp_port = {settings['smtp_port']}")
            if 'smtp_user' in data:
                settings['smtp_user'] = str(data['smtp_user'] or '').strip()
            if 'smtp_password' in data:
                # Only update if not empty (don't overwrite with empty string)
                pwd = str(data['smtp_password'] or '')
                if pwd and pwd != '********':  # Don't save masked password
                    settings['smtp_password'] = pwd
                    logging.info("[Settings] SMTP password updated")
            if 'smtp_from_email' in data:
                settings['smtp_from_email'] = str(data['smtp_from_email'] or '').strip()
                logging.info(f"[Settings] Setting smtp_from_email = {settings['smtp_from_email']}")
            if 'smtp_from_name' in data:
                settings['smtp_from_name'] = str(data['smtp_from_name'] or '').strip()
            if 'smtp_tls' in data:
                settings['smtp_tls'] = bool(data['smtp_tls'])
            if 'smtp_ssl' in data:
                settings['smtp_ssl'] = bool(data['smtp_ssl'])
            
            # NS: Alert settings
            if 'alert_email_recipients' in data:
                recipients = data['alert_email_recipients']
                if isinstance(recipients, str):
                    # Parse comma-separated string
                    recipients = [r.strip() for r in recipients.split(',') if r.strip()]
                settings['alert_email_recipients'] = recipients
            if 'alert_cooldown' in data:
                settings['alert_cooldown'] = max(60, min(86400, int(data['alert_cooldown'])))
            
            # NS: Default theme for new users - Jan 2026
            if 'default_theme' in data:
                allowed_themes = [
                    'proxmoxDark', 'proxmoxLight', 'midnight', 'forest', 'rose', 'ocean',
                    'highContrast', 'dracula', 'nord', 'monokai', 'matrix', 'sunset',
                    'cyberpunk', 'github', 'solarizedDark', 'gruvbox',
                    'corporateDark', 'corporateLight', 'enterpriseBlue'  # NS: Corporate themes
                ]
                if data['default_theme'] in allowed_themes:
                    settings['default_theme'] = data['default_theme']
            
        else:
            # form-data (for file uploads)
            domain = request.form.get('domain', '')
            port = request.form.get('port', '5000')
            http_redirect_port = request.form.get('http_redirect_port', '0')
            ssl_enabled = request.form.get('ssl_enabled', 'false').lower() == 'true'
            default_theme = request.form.get('default_theme', 'proxmoxDark')
            
            if settings.get('port') != int(port):
                restart_required = True
            if settings.get('http_redirect_port') != int(http_redirect_port):
                restart_required = True
            if settings.get('ssl_enabled') != ssl_enabled:
                restart_required = True
            
            settings['domain'] = domain
            settings['port'] = int(port)
            settings['http_redirect_port'] = int(http_redirect_port)
            settings['ssl_enabled'] = ssl_enabled
            
            # NS: Default theme for new users - Jan 2026
            allowed_themes = [
                'proxmoxDark', 'proxmoxLight', 'midnight', 'forest', 'rose', 'ocean',
                'highContrast', 'dracula', 'nord', 'monokai', 'matrix', 'sunset',
                'cyberpunk', 'github', 'solarizedDark', 'gruvbox',
                'corporateDark', 'corporateLight', 'enterpriseBlue'  # NS: Corporate themes
            ]
            if default_theme in allowed_themes:
                settings['default_theme'] = default_theme
            
            # Handle certificate upload
            if 'ssl_cert' in request.files:
                cert_file = request.files['ssl_cert']
                if cert_file.filename:
                    cert_content = cert_file.read()
                    if b'-----BEGIN CERTIFICATE-----' in cert_content or b'-----BEGIN' in cert_content:
                        with open(SSL_CERT_FILE, 'wb') as f:
                            f.write(cert_content)
                        os.chmod(SSL_CERT_FILE, 0o600)
                        restart_required = True
                    else:
                        return jsonify({'error': 'Invalid certificate format'}), 400
            
            # Handle key upload
            if 'ssl_key' in request.files:
                key_file = request.files['ssl_key']
                if key_file.filename:
                    key_content = key_file.read()
                    if b'-----BEGIN' in key_content and b'KEY-----' in key_content:
                        with open(SSL_KEY_FILE, 'wb') as f:
                            f.write(key_content)
                        os.chmod(SSL_KEY_FILE, 0o600)
                        restart_required = True
                    else:
                        return jsonify({'error': 'Invalid key format'}), 400
        
        # save
        logging.info(f"[Settings] Saving settings. SMTP enabled={settings.get('smtp_enabled')}, host={settings.get('smtp_host')}")
        if save_server_settings(settings):
            logging.info("[Settings] Settings saved successfully")
            usr = getattr(request, 'session', {}).get('user', 'system')
            log_audit(usr, 'settings.server_updated', f"Settings updated (restart_required={restart_required})")
            
            return jsonify({
                'success': True,
                'restart_required': restart_required,
                'message': 'Settings saved'
            })
        else:
            logging.error("[Settings] Failed to save settings")
            return jsonify({'error': 'Failed to save settings'}), 500
            
    except Exception as e:
        logging.error(f"Error updating server settings: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/settings/server/restart', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def restart_server():
    """Restart the PegaProx server (admin only)"""
    try:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'settings.server_restart', 'Server restart initiated')
        
        # Send response before restarting
        response = jsonify({'success': True, 'message': 'Server restart initiated'})
        
        # Schedule restart in a separate thread
        def do_restart():
            time.sleep(1)  # Give time for response to be sent
            logging.info("Server restart initiated by admin")
            
            is_root = os.geteuid() == 0 if hasattr(os, 'geteuid') else False
            has_sudo = shutil.which('sudo') is not None
            
            try:
                result = subprocess.run(['systemctl', 'is-active', 'pegaprox'],
                                       capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    if is_root:
                        subprocess.run(['systemctl', 'restart', 'pegaprox'], 
                                      capture_output=True, timeout=30)
                        return
                    elif has_sudo:
                        result = subprocess.run(
                            ['sudo', '-n', 'systemctl', 'restart', 'pegaprox'],
                            capture_output=True, text=True, timeout=30
                        )
                        if result.returncode == 0:
                            return
            except Exception:
                pass
            
            # Fallback: exit and let systemd restart
            logging.info("Exiting for systemd restart...")
            os._exit(0)
        
        restart_thread = threading.Thread(target=do_restart)
        restart_thread.daemon = True
        restart_thread.start()
        
        return response
        
    except Exception as e:
        logging.error(f"Error restarting server: {e}")
        return jsonify({'error': str(e)}), 500

# ============================================
# Config Backup/Restore API Routes
# NS: Jan 2026 - encrypted backups finally
# MK: AES-256-GCM with PBKDF2 key derivation
# ============================================

@app.route('/api/config/backup', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def backup_config():
    """Export full PegaProx configuration as encrypted backup (admin only)
    
    SECURITY: Requires user password confirmation and backup encryption password.
    MK: Double password = prevents stolen sessions from exporting data
    
    Body:
    - user_password: Current user's password for confirmation
    - backup_password: Password to encrypt the backup file (min 8 chars)
    - include_secrets: Include encrypted passwords/keys (default: false)
    - include_users: Include user accounts (default: true)
    - include_audit: Include audit log (default: false)
    """
    try:
        logging.info("[Backup] Starting config backup...")
        data = request.json or {}
        
        # 1. Verify user password first
        # NS: prevents session hijacking from exporting everything
        user_password = data.get('user_password', '')
        if not user_password:
            logging.warning("[Backup] No user password provided")
            return jsonify({'error': 'User password required for security verification'}), 400
        
        username = getattr(request, 'session', {}).get('user')
        logging.info(f"[Backup] User from session: {username}")
        if not username:
            logging.warning("[Backup] No user in session")
            return jsonify({'error': 'Not authenticated'}), 401
        
        users = load_users()
        
        # LW: these type checks saved us hours of debugging
        logging.debug(f"[Backup] Users type: {type(users)}, count: {len(users) if isinstance(users, dict) else 'N/A'}")
        
        if not isinstance(users, dict):
            logging.error(f"[Backup] Users is not a dict: {type(users)}")
            return jsonify({'error': 'User database error'}), 500
        
        user = users.get(username)
        
        logging.debug(f"[Backup] User data type: {type(user)}")
        
        if not user:
            logging.warning(f"[Backup] User {username} not found in database")
            return jsonify({'error': 'User not found'}), 404
        
        # MK: happened once after a botched migration, better safe than sorry
        if isinstance(user, str):
            logging.error(f"[Backup] User data is string, not dict")
            return jsonify({'error': 'User data format error - please re-login'}), 500
        
        # Verify password
        password_salt = user.get('password_salt', '') if isinstance(user, dict) else ''
        password_hash = user.get('password_hash', '') if isinstance(user, dict) else ''
        
        if not verify_password(user_password, password_salt, password_hash):
            log_audit(username, 'config.backup_failed', 'Password verification failed')
            logging.warning(f"[Backup] Password verification failed for {username}")
            return jsonify({'error': 'Incorrect password'}), 401
        
        logging.debug(f"[Backup] Password verified for {username}")
        
        # 2. Validate backup password
        backup_password = data.get('backup_password', '')
        if not backup_password or len(backup_password) < 8:
            logging.warning("[Backup] Backup password too short")
            return jsonify({'error': 'Backup password must be at least 8 characters'}), 400
        
        include_secrets = data.get('include_secrets', False)
        include_users = data.get('include_users', True)
        include_audit = data.get('include_audit', False)
        
        database = get_db()
        
        backup_data = {
            'version': PEGAPROX_VERSION,
            'build': PEGAPROX_BUILD,
            'export_date': datetime.now().isoformat(),
            'exported_by': username,
            'encrypted': True,  # Mark as encrypted backup
        }
        
        # Server settings
        backup_data['server_settings'] = load_server_settings()
        # Remove sensitive data if not requested
        if not include_secrets:
            if 'smtp_password' in backup_data['server_settings']:
                backup_data['server_settings']['smtp_password'] = ''
        
        # Clusters
        clusters = database.get_all_clusters()
        if not include_secrets:
            # Remove passwords and keys - clusters is a dict: {'id': {data}}
            for cluster_id, cluster_data in clusters.items():
                if isinstance(cluster_data, dict):
                    cluster_data.pop('password_encrypted', None)
                    cluster_data.pop('password', None)
                    cluster_data.pop('pass', None)
                    cluster_data.pop('ssh_key_encrypted', None)
                    cluster_data.pop('ssh_key', None)
                    cluster_data.pop('api_token_encrypted', None)
                    cluster_data.pop('api_token', None)
        backup_data['clusters'] = clusters
        
        # Users (optional)
        if include_users:
            users_data = database.get_all_users()
            if not include_secrets:
                # users_data is a dict: {'username': {data}}
                for username, user_data in users_data.items():
                    if isinstance(user_data, dict):
                        user_data.pop('password_hash', None)
                        user_data.pop('password_salt', None)
                        user_data.pop('totp_secret', None)
                        user_data.pop('totp_secret_encrypted', None)
            backup_data['users'] = users_data
        
        # Tenants
        backup_data['tenants'] = database.get_all_tenants()
        
        # VM ACLs
        backup_data['vm_acls'] = database.get_all_vm_acls()
        
        # Affinity Rules
        backup_data['affinity_rules'] = database.get_affinity_rules()
        
        # Cluster Groups
        try:
            cursor = database.conn.cursor()
            cursor.execute('SELECT * FROM cluster_groups')
            backup_data['cluster_groups'] = [dict(row) for row in cursor.fetchall()]
        except:
            backup_data['cluster_groups'] = []
        
        # Custom Scripts
        try:
            cursor = database.conn.cursor()
            cursor.execute('SELECT * FROM custom_scripts WHERE deleted_at IS NULL')
            scripts = [dict(row) for row in cursor.fetchall()]
            # Don't include output in backup
            for script in scripts:
                script.pop('last_output', None)
            backup_data['custom_scripts'] = scripts
        except:
            backup_data['custom_scripts'] = []
        
        # Audit Log (optional, can be large)
        if include_audit:
            backup_data['audit_log'] = database.get_audit_log(limit=10000)
        
        logging.debug(f"[Backup] Encrypting backup data...")
        # 3. Encrypt the backup with AES-256-GCM
        encrypted_backup = _encrypt_backup(json.dumps(backup_data, default=str), backup_password)
        logging.debug(f"[Backup] Encryption complete, size: {len(encrypted_backup)} bytes")
        
        # Log the backup action
        log_audit(username, 'config.backup', f"Configuration exported (secrets={'included' if include_secrets else 'excluded'}, encrypted=True)")
        
        # Return as downloadable encrypted file
        response = make_response(encrypted_backup)
        response.headers['Content-Type'] = 'application/octet-stream'
        response.headers['Content-Disposition'] = f'attachment; filename=pegaprox-backup-{datetime.now().strftime("%Y%m%d-%H%M%S")}.pegabackup'
        
        logging.debug(f"[Backup] Sending response with {len(encrypted_backup)} bytes")
        return response
        
    except Exception as e:
        logging.error(f"Config backup failed: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

def _encrypt_backup(data: str, password: str) -> bytes:
    """Encrypt backup data with password using AES-256-GCM
    
    Uses PBKDF2 to derive key from password.
    Format: salt (16 bytes) + nonce (12 bytes) + ciphertext
    
    MK: Same format as our cluster password encryption
    """
    import hashlib
    from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
    from cryptography.hazmat.primitives import hashes
    from cryptography.hazmat.backends import default_backend
    
    # Generate random salt
    salt = os.urandom(16)
    
    # Derive key from password using PBKDF2
    # MK: 100k iterations is OWASP minimum, good enough for backups
    kdf = PBKDF2HMAC(
        algorithm=hashes.SHA256(),
        length=32,  # 256 bits
        salt=salt,
        iterations=100000,  # OWASP recommended minimum
        backend=default_backend()
    )
    key = kdf.derive(password.encode('utf-8'))
    
    # Encrypt with AES-256-GCM
    aesgcm = AESGCM(key)
    nonce = os.urandom(12)  # NS: 12 bytes is standard for GCM
    ciphertext = aesgcm.encrypt(nonce, data.encode('utf-8'), None)
    
    # Combine: salt + nonce + ciphertext
    return salt + nonce + ciphertext

def _decrypt_backup(encrypted_data: bytes, password: str) -> str:
    """Decrypt backup data with password
    
    Returns decrypted JSON string or raises exception on failure.
    NS: Wrong password will throw InvalidTag exception
    """
    import hashlib
    from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
    from cryptography.hazmat.primitives import hashes
    from cryptography.hazmat.backends import default_backend
    
    logging.debug(f"[Decrypt] Input data size: {len(encrypted_data)} bytes")
    
    if len(encrypted_data) < 28:  # salt (16) + nonce (12)
        logging.error(f"[Decrypt] Data too short: {len(encrypted_data)} bytes (need at least 28)")
        raise ValueError("Invalid backup file format - file too short")
    
    # Extract components - format is: salt + nonce + ciphertext
    # MK: same format as our cluster password encryption
    salt = encrypted_data[:16]
    nonce = encrypted_data[16:28]
    ciphertext = encrypted_data[28:]
    
    logging.debug(f"[Decrypt] Salt: {len(salt)} bytes, Nonce: {len(nonce)} bytes, Ciphertext: {len(ciphertext)} bytes")
    
    # Derive key from password using PBKDF2
    kdf = PBKDF2HMAC(
        algorithm=hashes.SHA256(),
        length=32,
        salt=salt,
        iterations=100000,
        backend=default_backend()
    )
    key = kdf.derive(password.encode('utf-8'))
    
    # Decrypt with AES-256-GCM
    aesgcm = AESGCM(key)
    try:
        plaintext = aesgcm.decrypt(nonce, ciphertext, None)
        logging.debug(f"[Decrypt] Decryption successful, plaintext size: {len(plaintext)} bytes")
        return plaintext.decode('utf-8')
    except Exception as e:
        # NS: InvalidTag means wrong password, dont log the actual error (security)
        logging.error(f"[Decrypt] Decryption failed")
        raise ValueError("Incorrect backup password or corrupted file")

@app.route('/api/config/restore', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def restore_config():
    """Import PegaProx configuration from encrypted backup (admin only)
    
    SECURITY: Requires user password confirmation and backup decryption password.
    NS: merge mode is default because overwrite is scary
    
    Accepts multipart form with:
    - user_password: Current user's password for confirmation
    - backup_password: Password to decrypt the backup file
    - backup_file: The encrypted .pegabackup file
    - mode: 'merge' (default) or 'overwrite'
    - restore_users: Restore user accounts (default: false for safety)
    - dry_run: Validate only, don't apply (default: false)
    """
    try:
        logging.info("[Restore] Starting config restore...")
        # NS: these help when debugging upload issues
        logging.debug(f"[Restore] Content-Type: {request.content_type}")
        logging.debug(f"[Restore] Form keys: {list(request.form.keys())}")
        logging.debug(f"[Restore] Files keys: {list(request.files.keys())}")
        
        # Get form data
        user_password = request.form.get('user_password', '')
        backup_password = request.form.get('backup_password', '')
        mode = request.form.get('mode', 'merge')
        restore_users_str = request.form.get('restore_users', 'false')
        dry_run_str = request.form.get('dry_run', 'false')
        
        # LW: form data comes as strings, need to convert
        restore_users = str(restore_users_str).lower() in ('true', '1', 'yes')
        dry_run = str(dry_run_str).lower() in ('true', '1', 'yes')
        
        logging.info(f"[Restore] Mode: {mode}, dry_run: {dry_run}, restore_users: {restore_users}")
        
        # 1. Verify user password first
        if not user_password:
            logging.warning("[Restore] No user password provided")
            return jsonify({'error': 'User password required for security verification'}), 400
        
        username = getattr(request, 'session', {}).get('user')
        logging.debug(f"[Restore] User from session: {username}")
        if not username:
            return jsonify({'error': 'Not authenticated'}), 401
        
        users = load_users()
        
        # NS: copy-paste from backup_config, same validation
        logging.debug(f"[Restore] Users type: {type(users)}, count: {len(users) if isinstance(users, dict) else 'N/A'}")
        
        if not isinstance(users, dict):
            logging.error(f"[Restore] Users is not a dict: {type(users)}")
            return jsonify({'error': 'User database error'}), 500
        
        user = users.get(username)
        
        logging.debug(f"[Restore] User data type: {type(user)}")
        if user:
            logging.debug(f"[Restore] User keys: {user.keys() if isinstance(user, dict) else 'NOT A DICT'}")
        
        if not user:
            return jsonify({'error': 'User not found'}), 404
        
        # same legacy check as backup
        if isinstance(user, str):
            logging.error(f"[Restore] User data is string, not dict: {user[:50]}...")
            return jsonify({'error': 'User data format error - please re-login'}), 500
        
        # Verify password
        password_salt = user.get('password_salt', '') if isinstance(user, dict) else ''
        password_hash = user.get('password_hash', '') if isinstance(user, dict) else ''
        
        if not verify_password(user_password, password_salt, password_hash):
            log_audit(username, 'config.restore_failed', 'Password verification failed')
            logging.warning(f"[Restore] Password verification failed for {username}")
            return jsonify({'error': 'Incorrect password'}), 401
        
        logging.debug(f"[Restore] Password verified for {username}")
        
        # 2. Validate backup password
        if not backup_password:
            return jsonify({'error': 'Backup password required to decrypt file'}), 400
        
        # 3. Get backup file
        if 'backup_file' not in request.files:
            logging.warning("[Restore] No backup_file in request.files")
            return jsonify({'error': 'No backup file provided'}), 400
        
        backup_file = request.files['backup_file']
        if not backup_file.filename:
            return jsonify({'error': 'No backup file selected'}), 400
        
        logging.debug(f"[Restore] Processing file: {backup_file.filename}")
        
        # Read and decrypt
        encrypted_data = backup_file.read()
        logging.debug(f"[Restore] Read {len(encrypted_data)} bytes from file")
        
        try:
            decrypted_json = _decrypt_backup(encrypted_data, backup_password)
            data = json.loads(decrypted_json)
        except ValueError as e:
            log_audit(username, 'config.restore_failed', f'Decryption failed: {str(e)}')
            return jsonify({'error': str(e)}), 400
        except json.JSONDecodeError:
            return jsonify({'error': 'Invalid backup file format'}), 400
        
        # Validate backup format
        if 'version' not in data or 'export_date' not in data:
            return jsonify({'error': 'Invalid backup format - missing required fields'}), 400
        
        database = get_db()
        results = {
            'mode': mode,
            'dry_run': dry_run,
            'backup_version': data.get('version'),
            'backup_date': data.get('export_date'),
            'backup_by': data.get('exported_by'),
            'restored': {},
            'skipped': {},
            'errors': []
        }
        
        # Server Settings
        if 'server_settings' in data:
            try:
                if not dry_run:
                    current = load_server_settings()
                    if mode == 'merge':
                        # Only update non-empty values
                        for key, value in data['server_settings'].items():
                            if value not in [None, '', []]:
                                current[key] = value
                        save_server_settings(current)
                    else:
                        save_server_settings(data['server_settings'])
                results['restored']['server_settings'] = True
            except Exception as e:
                results['errors'].append(f"Server settings: {str(e)}")
        
        # Clusters
        if 'clusters' in data:
            cluster_count = 0
            clusters_data = data['clusters']
            
            # NS: log types because old backups might have different formats
            logging.debug(f"[Restore] Clusters type: {type(clusters_data)}")
            if isinstance(clusters_data, list) and len(clusters_data) > 0:
                logging.debug(f"[Restore] First cluster type: {type(clusters_data[0])}")
            
            # Handle both list of dicts and dict of dicts formats
            # MK: we changed the export format once, need to support both
            if isinstance(clusters_data, dict):
                # Format: {'cluster_id': {cluster_data}, ...}
                clusters_list = [{'id': k, **v} if isinstance(v, dict) else {'id': k} for k, v in clusters_data.items()]
            elif isinstance(clusters_data, list):
                clusters_list = clusters_data
            else:
                clusters_list = []
                results['errors'].append(f"Clusters: Invalid format (expected list or dict, got {type(clusters_data).__name__})")
            
            for cluster in clusters_list:
                try:
                    # Skip if not a dict
                    if not isinstance(cluster, dict):
                        logging.warning(f"[Restore] Skipping non-dict cluster: {type(cluster)}")
                        continue
                    
                    cluster_id = cluster.get('id')
                    if not cluster_id:
                        logging.warning(f"[Restore] Skipping cluster without id")
                        continue
                    
                    existing = database.get_cluster(cluster_id)
                    
                    if existing and mode == 'merge':
                        # Keep existing passwords if not in backup
                        if not cluster.get('password_encrypted') and existing.get('password_encrypted'):
                            cluster['password_encrypted'] = existing['password_encrypted']
                        if not cluster.get('ssh_key_encrypted') and existing.get('ssh_key_encrypted'):
                            cluster['ssh_key_encrypted'] = existing['ssh_key_encrypted']
                    
                    if not dry_run:
                        database.save_cluster(cluster_id, cluster)
                    cluster_count += 1
                except Exception as e:
                    cluster_id_str = cluster.get('id', 'unknown') if isinstance(cluster, dict) else str(cluster)[:20]
                    results['errors'].append(f"Cluster {cluster_id_str}: {str(e)}")
            results['restored']['clusters'] = cluster_count
        
        # Users (only if explicitly requested)
        if restore_users and 'users' in data:
            user_count = 0
            users_data = data['users']
            
            # Handle both list and dict formats
            if isinstance(users_data, dict):
                # Format: {'username': {user_data}, ...}
                users_list = [{'username': k, **v} if isinstance(v, dict) else {'username': k} for k, v in users_data.items()]
            elif isinstance(users_data, list):
                users_list = users_data
            else:
                users_list = []
                results['errors'].append(f"Users: Invalid format")
            
            for u in users_list:
                try:
                    if not isinstance(u, dict):
                        continue
                    uname = u.get('username')
                    if not uname or uname == 'admin' or uname == 'pegaprox':  # Never overwrite admin
                        continue
                    
                    if not dry_run:
                        database.save_user(uname, u)
                    user_count += 1
                except Exception as e:
                    uname_str = u.get('username', 'unknown') if isinstance(u, dict) else str(u)[:20]
                    results['errors'].append(f"User {uname_str}: {str(e)}")
            results['restored']['users'] = user_count
        else:
            results['skipped']['users'] = 'Not requested (safety)'
        
        # Tenants
        if 'tenants' in data:
            tenant_count = 0
            tenants_data = data['tenants']
            
            # Handle both list and dict formats
            if isinstance(tenants_data, dict):
                tenants_list = [{'id': k, **v} if isinstance(v, dict) else {'id': k} for k, v in tenants_data.items()]
            elif isinstance(tenants_data, list):
                tenants_list = tenants_data
            else:
                tenants_list = []
            
            for tenant in tenants_list:
                try:
                    if not isinstance(tenant, dict):
                        continue
                    if not dry_run:
                        database.save_tenant(tenant.get('id'), tenant)
                    tenant_count += 1
                except Exception as e:
                    results['errors'].append(f"Tenant: {str(e)}")
            results['restored']['tenants'] = tenant_count
        
        # VM ACLs
        if 'vm_acls' in data:
            try:
                if not dry_run:
                    if mode == 'overwrite':
                        # Clear existing
                        database.conn.cursor().execute('DELETE FROM vm_acls')
                    database.save_all_vm_acls(data['vm_acls'])
                results['restored']['vm_acls'] = len(data['vm_acls'])
            except Exception as e:
                results['errors'].append(f"VM ACLs: {str(e)}")
        
        # Affinity Rules
        if 'affinity_rules' in data:
            rule_count = 0
            for cluster_id, rules in data['affinity_rules'].items():
                for rule in rules:
                    try:
                        if not dry_run:
                            database.save_affinity_rule(cluster_id, rule)
                        rule_count += 1
                    except Exception as e:
                        results['errors'].append(f"Affinity rule: {str(e)}")
            results['restored']['affinity_rules'] = rule_count
        
        # Cluster Groups
        if 'cluster_groups' in data:
            group_count = 0
            cursor = database.conn.cursor()
            for group in data['cluster_groups']:
                try:
                    if not dry_run:
                        cursor.execute('''
                            INSERT OR REPLACE INTO cluster_groups (id, name, tenant_id, description, created_at)
                            VALUES (?, ?, ?, ?, ?)
                        ''', (group.get('id'), group.get('name'), group.get('tenant_id'), 
                              group.get('description'), group.get('created_at', datetime.now().isoformat())))
                    group_count += 1
                except Exception as e:
                    results['errors'].append(f"Cluster group: {str(e)}")
            if not dry_run:
                database.conn.commit()
            results['restored']['cluster_groups'] = group_count
        
        # Log the restore action
        if not dry_run:
            log_audit(username, 'config.restore', f"Configuration restored from backup ({mode} mode)")
        else:
            log_audit(username, 'config.restore_dryrun', f"Configuration restore dry-run ({mode} mode)")
        
        return jsonify(results)
        
    except Exception as e:
        logging.error(f"Config restore failed: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

# ============================================
# IP Whitelisting API Routes
# LW: Jan 2026 - enterprise feature request
# ============================================

# IP Whitelist storage (loaded from settings)
_ip_whitelist_enabled = False
_ip_whitelist = set()
_ip_blacklist = set()  # MK: blacklist always wins over whitelist

def load_ip_whitelist():
    """Load IP whitelist from server settings"""
    global _ip_whitelist_enabled, _ip_whitelist, _ip_blacklist
    
    try:
        settings = load_server_settings()
        _ip_whitelist_enabled = settings.get('ip_whitelist_enabled', False)
        
        # MK: 'or' handles None values from old configs
        whitelist_str = settings.get('ip_whitelist') or ''
        _ip_whitelist = set(ip.strip() for ip in whitelist_str.split(',') if ip.strip())
        
        blacklist_str = settings.get('ip_blacklist') or ''
        _ip_blacklist = set(ip.strip() for ip in blacklist_str.split(',') if ip.strip())
    except Exception as e:
        logging.warning(f"Could not load IP whitelist: {e}")
        _ip_whitelist_enabled = False
        _ip_whitelist = set()
        _ip_blacklist = set()

def check_ip_allowed(client_ip: str) -> tuple:
    """Check if client IP is allowed
    
    Returns: (allowed: bool, reason: str)
    """
    if not _ip_whitelist_enabled:
        return True, 'Whitelist disabled'
    
    if not client_ip:
        return False, 'No IP detected'
    
    # Check blacklist first (always blocks)
    # NS: blacklist is checked before whitelist, security first
    if _ip_blacklist:
        for blocked in _ip_blacklist:
            if _ip_matches(client_ip, blocked):
                return False, f'IP blacklisted: {blocked}'
    
    # If whitelist is empty, allow all (only blacklist applies)
    if not _ip_whitelist:
        return True, 'No whitelist configured'
    
    # Check whitelist
    for allowed in _ip_whitelist:
        if _ip_matches(client_ip, allowed):
            return True, f'IP allowed: {allowed}'
    
    return False, 'IP not in whitelist'

def _ip_matches(client_ip: str, pattern: str) -> bool:
    """Check if IP matches pattern (supports CIDR and wildcards)
    
    NS: supports 192.168.1.100, 192.168.1.0/24, 192.168.1.*
    MK: chatgpt helped with the ipaddress module stuff
    """
    try:
        # Exact match
        if client_ip == pattern:
            return True
        
        # Wildcard match (e.g., 192.168.1.*)
        if '*' in pattern:
            import fnmatch
            return fnmatch.fnmatch(client_ip, pattern)
        
        # CIDR match (e.g., 192.168.1.0/24)
        if '/' in pattern:
            import ipaddress
            network = ipaddress.ip_network(pattern, strict=False)
            return ipaddress.ip_address(client_ip) in network
        
        return False
    except Exception:
        return False

# Load whitelist on startup
try:
    load_ip_whitelist()
except:
    pass  # Settings might not exist yet

@app.before_request
def check_ip_whitelist():
    """Check IP whitelist before processing request"""
    # Skip for static files
    if request.path.startswith('/static'):
        return None
    
    # Skip if whitelist not enabled
    if not _ip_whitelist_enabled:
        return None
    
    client_ip = get_client_ip()
    allowed, reason = check_ip_allowed(client_ip)
    
    if not allowed:
        logging.warning(f"IP blocked: {client_ip} - {reason}")
        return jsonify({
            'error': 'Access denied',
            'message': 'Your IP address is not allowed to access this service',
            'ip': client_ip
        }), 403

@app.route('/api/security/ip-whitelist', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_ip_whitelist():
    """Get IP whitelist configuration (admin only)"""
    try:
        settings = load_server_settings()
        
        whitelist_str = settings.get('ip_whitelist') or ''
        blacklist_str = settings.get('ip_blacklist') or ''
        
        return jsonify({
            'enabled': settings.get('ip_whitelist_enabled', False),
            'whitelist': [ip.strip() for ip in whitelist_str.split(',') if ip.strip()],
            'blacklist': [ip.strip() for ip in blacklist_str.split(',') if ip.strip()],
            'your_ip': get_client_ip(),
            'formats_supported': ['Single IP (192.168.1.100)', 'CIDR (192.168.1.0/24)', 'Wildcard (192.168.1.*)']
        })
    except Exception as e:
        logging.error(f"Error getting IP whitelist: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/security/ip-whitelist', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def update_ip_whitelist():
    """Update IP whitelist configuration (admin only)
    
    Body:
    - enabled: bool
    - whitelist: list of IPs/CIDRs
    - blacklist: list of IPs/CIDRs
    """
    try:
        data = request.json or {}
        settings = load_server_settings()
        current_ip = get_client_ip()
        
        # Validate that admin's current IP would still be allowed
        if data.get('enabled', False):
            new_whitelist = set(data.get('whitelist', []))
            new_blacklist = set(data.get('blacklist', []))
            
            # Check if current IP would be blocked
            if new_whitelist:
                allowed = False
                for pattern in new_whitelist:
                    if _ip_matches(current_ip, pattern):
                        allowed = True
                        break
                
                if not allowed:
                    return jsonify({
                        'error': 'Your current IP would be blocked',
                        'message': f'Add {current_ip} to the whitelist before enabling',
                        'your_ip': current_ip
                    }), 400
            
            # Check blacklist doesn't include current IP
            for pattern in new_blacklist:
                if _ip_matches(current_ip, pattern):
                    return jsonify({
                        'error': 'Your current IP is in the blacklist',
                        'message': f'Remove {current_ip} from the blacklist',
                        'your_ip': current_ip
                    }), 400
        
        # Update settings
        if 'enabled' in data:
            settings['ip_whitelist_enabled'] = bool(data['enabled'])
        
        if 'whitelist' in data:
            settings['ip_whitelist'] = ','.join(data['whitelist'])
        
        if 'blacklist' in data:
            settings['ip_blacklist'] = ','.join(data['blacklist'])
        
        save_server_settings(settings)
        
        # Reload whitelist
        load_ip_whitelist()
        
        # Audit log
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'security.ip_whitelist_updated', 
                 f"IP whitelist {'enabled' if settings.get('ip_whitelist_enabled') else 'disabled'}, "
                 f"{len(_ip_whitelist)} IPs whitelisted, {len(_ip_blacklist)} IPs blacklisted")
        
        return jsonify({
            'success': True,
            'enabled': settings.get('ip_whitelist_enabled', False),
            'whitelist_count': len(_ip_whitelist),
            'blacklist_count': len(_ip_blacklist)
        })
        
    except Exception as e:
        logging.error(f"IP whitelist update failed: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/security/ip-whitelist/test', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def test_ip_whitelist():
    """Test if an IP would be allowed (admin only)
    
    Body:
    - ip: IP address to test
    """
    try:
        data = request.json or {}
        test_ip = data.get('ip', get_client_ip())
        
        allowed, reason = check_ip_allowed(test_ip)
        
        return jsonify({
            'ip': test_ip,
            'allowed': allowed,
            'reason': reason,
            'whitelist_enabled': _ip_whitelist_enabled
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ============================================
# Audit Log API Route
# ============================================

@app.route('/api/audit', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_audit_log_api():
    """Get audit log entries (admin only)"""
    # Optional filters
    user_filter = request.args.get('user')
    action_filter = request.args.get('action')
    limit = int(request.args.get('limit', 500))
    verify = request.args.get('verify', '').lower() == 'true'
    
    # Get from database with optional integrity verification
    database = get_db()
    entries = database.get_audit_log(
        limit=limit,
        user=user_filter,
        action=action_filter,
        verify_integrity=verify
    )
    
    return jsonify(entries)


# MK: Cluster-specific audit endpoint with vmid filter
@app.route('/api/clusters/<cluster_id>/audit', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_cluster_audit_log_api(cluster_id):
    """Get audit log entries for a specific cluster, optionally filtered by vmid"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    # Get cluster name for filtering
    cluster_name = None
    if cluster_id in cluster_managers:
        cluster_name = cluster_managers[cluster_id].config.name
    
    # Check if we're in multi-cluster mode
    multi_cluster = len(cluster_managers) > 1
    
    # Optional filters
    vmid = request.args.get('vmid')
    limit = int(request.args.get('limit', 100))
    
    # Get from database
    database = get_db()
    entries = database.get_audit_log(limit=limit * 10)  # Get more to filter
    
    # Filter by cluster and vmid
    filtered = []
    for entry in entries:
        entry_cluster = entry.get('cluster', '')
        details = entry.get('details', '')
        
        # Cluster filter
        if cluster_name:
            detected_cluster = None
            
            # First check the cluster field
            if entry_cluster:
                detected_cluster = entry_cluster
            else:
                # Try to detect cluster from details text
                import re
                # Look for [SomeCluster] pattern at end
                bracket_match = re.search(r'\[([^\]]+)\]\s*$', details)
                if bracket_match:
                    detected_cluster = bracket_match.group(1)
                else:
                    # Look for "for cluster X" or "cluster X" pattern
                    cluster_match = re.search(r'(?:for )?cluster\s+(\S+)', details, re.IGNORECASE)
                    if cluster_match:
                        detected_cluster = cluster_match.group(1)
            
            # If we detected a cluster, it must match
            if detected_cluster:
                if detected_cluster != cluster_name:
                    continue
            else:
                # No cluster info at all - skip in multi-cluster mode
                if multi_cluster:
                    continue
        
        # Check vmid filter
        if vmid:
            vmid_str = str(vmid)
            vmid_found = False
            
            # Check for patterns in details
            patterns = [
                f"VM {vmid_str} ", f"VM {vmid_str}-", f"VM {vmid_str})",
                f"CT {vmid_str} ", f"CT {vmid_str}-", f"CT {vmid_str})",
                f"QEMU {vmid_str} ", f"QEMU {vmid_str}-", f"QEMU {vmid_str})",
                f"LXC {vmid_str} ", f"LXC {vmid_str}-",
                f"/{vmid_str} ", f"/{vmid_str})",
                f"qemu/{vmid_str}", f"lxc/{vmid_str}",
            ]
            
            for pattern in patterns:
                if pattern in details:
                    vmid_found = True
                    break
            
            # Also check if details ends with the vmid pattern
            if not vmid_found:
                for ending in [f"VM {vmid_str}", f"CT {vmid_str}", f"QEMU {vmid_str}", f"LXC {vmid_str}"]:
                    if details.endswith(ending):
                        vmid_found = True
                        break
            
            if not vmid_found:
                continue
        
        filtered.append(entry)
        if len(filtered) >= limit:
            break
    
    return jsonify(filtered)


@app.route('/api/audit/integrity', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def verify_audit_integrity():
    """Verify integrity of audit log using HMAC signatures (admin only)
    
    Returns statistics about log integrity:
    - total_entries: Total number of entries
    - verified: Entries with valid HMAC signature
    - unsigned: Old entries without signature (pre-upgrade)
    - potentially_tampered: Entries with invalid signature (WARNING!)
    - integrity_percentage: Percentage of verified entries
    """
    database = get_db()
    result = database.verify_audit_log_integrity()
    
    # Log this check itself
    usr = getattr(request, 'session', {}).get('user', 'system')
    log_audit(usr, 'audit.integrity_check', f"Audit integrity check: {result['verified']}/{result['total_entries']} verified, {result['potentially_tampered']} potentially tampered")
    
    return jsonify(result)

@app.route('/api/security/key-info', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_encryption_key_info():
    """Get information about the current encryption key (admin only)
    
    Returns:
    - exists: Whether key file exists
    - created: When key was created
    - algorithm: Encryption algorithm (AES-256-GCM)
    - backups: List of backup key files from previous rotations
    """
    database = get_db()
    return jsonify(database.get_key_info())

@app.route('/api/security/key-rotate', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def rotate_encryption_key():
    """Rotate the encryption key (admin only)
    
    IMPORTANT: This re-encrypts all sensitive data with a new key.
    Required for HIPAA/ISO 27001 compliance (periodic key rotation).
    
    The old key is backed up before rotation.
    """
    usr = getattr(request, 'session', {}).get('user', 'system')
    
    # Confirm action
    data = request.json or {}
    if not data.get('confirm'):
        return jsonify({
            'error': 'Key rotation requires confirmation',
            'message': 'Send {"confirm": true} to proceed. This will re-encrypt all data.'
        }), 400
    
    log_audit(usr, 'security.key_rotation_started', 'Encryption key rotation initiated')
    
    database = get_db()
    result = database.rotate_encryption_key()
    
    if result.get('success'):
        log_audit(usr, 'security.key_rotation_completed', 
                 f"Key rotation completed: {result.get('users_rotated', 0)} users, "
                 f"{result.get('clusters_rotated', 0)} clusters rotated")
        return jsonify(result)
    else:
        log_audit(usr, 'security.key_rotation_failed', f"Key rotation failed: {result.get('error', 'Unknown error')}")
        return jsonify(result), 500

@app.route('/api/security/compliance', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_compliance_status():
    """Get security compliance status (admin only)
    
    Returns overview of security features for HIPAA/ISO 27001 compliance.
    """
    try:
        settings = load_server_settings()
        database = get_db()
        key_info = database.get_key_info()
        
        # Calculate compliance score
        # MK: each check is worth the same, simple but effective
        checks = {
            'encryption_enabled': ENCRYPTION_AVAILABLE and key_info.get('exists', False),
            'https_enabled': os.path.exists(SSL_CERT_FILE) and os.path.exists(SSL_KEY_FILE),
            'password_policy_enabled': settings.get('password_min_length', 8) >= 8,
            'session_timeout_compliant': settings.get('session_timeout', SESSION_TIMEOUT) <= 28800,  # 8h max for HIPAA
            '2fa_available': TOTP_AVAILABLE,
            'audit_logging_enabled': True,  # Always enabled
            'rate_limiting_enabled': API_RATE_LIMIT > 0,
            'brute_force_protection': True,  # Always enabled
        }
        
        score = sum(1 for v in checks.values() if v) / len(checks) * 100
        
        return jsonify({
            'compliance_score': round(score, 1),
            'checks': checks,
            'encryption': {
                'algorithm': 'AES-256-GCM',
                'key_exists': key_info.get('exists', False),
                'key_created': key_info.get('created'),
                'last_rotation': key_info.get('last_modified'),
                'backups_count': len(key_info.get('backups', []))
            },
            'session': {
                'timeout_seconds': settings.get('session_timeout', SESSION_TIMEOUT),
                'timeout_hours': settings.get('session_timeout', SESSION_TIMEOUT) / 3600
            },
            'password_policy': {
                'min_length': settings.get('password_min_length', 8),
                'require_uppercase': settings.get('password_require_uppercase', True),
                'require_lowercase': settings.get('password_require_lowercase', True),
                'require_numbers': settings.get('password_require_numbers', True),
                'require_special': settings.get('password_require_special', False),
                'expiry_enabled': settings.get('password_expiry_enabled', False),
                'expiry_days': settings.get('password_expiry_days', 90)
            },
            'recommendations': [
                r for r in [
                    None if checks['https_enabled'] else 'Enable HTTPS with valid certificates',
                    None if checks['session_timeout_compliant'] else 'Reduce session timeout to 8 hours or less',
                    None if settings.get('password_require_special') else 'Consider requiring special characters in passwords',
                    None if settings.get('password_expiry_enabled') else 'Consider enabling password expiry',
                    None if key_info.get('backups') else 'Perform initial key rotation to create backup',
                    None if not _check_default_password_in_use() else 'CRITICAL: Default admin password is still in use! Change it immediately.',
                ] if r is not None
            ],
            'default_password_warning': _check_default_password_in_use()
        })
    except Exception as e:
        logging.error(f"Error getting compliance status: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/security/cors', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_cors_origins():
    """Get configured CORS origins (admin only)"""
    env_origins = [o.strip() for o in _cors_origins_env.split(',') if o.strip()] if _cors_origins_env else []
    
    return jsonify({
        'mode': 'same-origin' if not env_origins and not _auto_allowed_origins else 'configured',
        'environment_origins': env_origins,
        'auto_allowed_origins': list(_auto_allowed_origins),
        'all_allowed': get_allowed_origins() or [],
        'help': {
            'same-origin': 'Only requests from the same host are allowed (most secure)',
            'configured': 'Specific origins are allowed',
            'env_variable': 'PEGAPROX_ALLOWED_ORIGINS',
            'example': 'export PEGAPROX_ALLOWED_ORIGINS="https://pegaprox.example.com"'
        }
    })

@app.route('/api/security/cors', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def add_cors_origin():
    """Manually add a CORS origin (admin only)
    
    Note: This is temporary (until server restart). For permanent origins,
    use the PEGAPROX_ALLOWED_ORIGINS environment variable.
    """
    data = request.json or {}
    origin = data.get('origin', '').strip()
    
    if not origin:
        return jsonify({'error': 'Origin required'}), 400
    
    if not origin.startswith(('http://', 'https://')):
        return jsonify({'error': 'Origin must start with http:// or https://'}), 400
    
    if origin == '*':
        return jsonify({'error': 'Wildcard (*) not allowed for security reasons'}), 400
    
    add_allowed_origin(origin)
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    log_audit(usr, 'security.cors_origin_added', f"Added CORS origin: {origin}")
    
    return jsonify({
        'success': True,
        'message': f'Origin {origin} added',
        'note': 'This is temporary until server restart. Set PEGAPROX_ALLOWED_ORIGINS env var for permanent configuration.'
    })

# API Routes
@app.route('/')
def index():
    """Serve the web interface"""
    return send_from_directory(WEB_DIR, 'index.html')

@app.route('/api/status', methods=['GET'])
def get_status():
    """Get PegaProx system status - includes version info"""
    return jsonify({
        'version': PEGAPROX_VERSION,
        'build': PEGAPROX_BUILD,
        'encryption': {
            'available': ENCRYPTION_AVAILABLE,
            'enabled': ENCRYPTION_AVAILABLE and os.path.exists(KEY_FILE),
            'config_encrypted': os.path.exists(CONFIG_FILE_ENCRYPTED)
        },
        'clusters_count': len(cluster_managers),
        'totp_available': TOTP_AVAILABLE,
        'gevent_available': GEVENT_AVAILABLE,
        'ssh': get_ssh_connection_stats()  # NS: Jan 2026 - SSH connection pool stats
    })

@app.route('/api/clusters', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_clusters():
    """Get all configured clusters (filtered by tenant)
    
    NS: Clusters are now sorted by sort_order, then by name for consistent ordering
    """
    # get user's allowed clusters
    users = load_users()
    user = users.get(request.session['user'], {})
    allowed = get_user_clusters(user)
    
    # Get cluster metadata from database (display_name, group_id, sort_order)
    db = get_db()
    cluster_meta = {}
    try:
        meta_rows = db.query('SELECT id, display_name, group_id, sort_order FROM clusters')
        for row in meta_rows:
            cluster_meta[row['id']] = {
                'display_name': row['display_name'],
                'group_id': row['group_id'],
                'sort_order': row['sort_order'] if row['sort_order'] is not None else 0
            }
    except:
        pass
    
    clusters = []
    for cluster_id, mgr in cluster_managers.items():
        # filter by tenant
        if allowed is not None and cluster_id not in allowed:
            continue
        
        meta = cluster_meta.get(cluster_id, {})
        display_name = meta.get('display_name') or ''
            
        clusters.append({
            'id': cluster_id,
            'name': mgr.config.name,
            'display_name': display_name,
            'group_id': meta.get('group_id'),
            'sort_order': meta.get('sort_order', 0),
            'host': mgr.config.host,
            'status': 'running' if mgr.running else 'stopped',
            'connected': mgr.is_connected,
            'connection_error': mgr.connection_error,
            'migration_threshold': mgr.config.migration_threshold,
            'check_interval': mgr.config.check_interval,
            'auto_migrate': mgr.config.auto_migrate,
            'balance_containers': getattr(mgr.config, 'balance_containers', False),
            'balance_local_disks': getattr(mgr.config, 'balance_local_disks', False),
            'dry_run': mgr.config.dry_run,
            'enabled': mgr.config.enabled,
            'ha_enabled': mgr.config.ha_enabled,
            'fallback_hosts': mgr.config.fallback_hosts,
            'excluded_nodes': getattr(mgr.config, 'excluded_nodes', []),  # LW: Nodes excluded from balancing
            'current_host': getattr(mgr, 'current_host', None),
            'last_run': mgr.last_run.isoformat() if mgr.last_run else None
        })
    
    # MK: Sort clusters by sort_order first, then by name for consistent ordering
    clusters.sort(key=lambda c: (c.get('sort_order', 0), c.get('name', '').lower()))
    
    return jsonify(clusters)


def get_connected_manager(cluster_id):
    """Get a cluster manager, return (manager, None) if connected, (None, error_response) if not"""
    if cluster_id not in cluster_managers:
        return None, (jsonify({'error': 'Cluster not found'}), 404)
    
    manager = cluster_managers[cluster_id]
    
    if not manager.is_connected:
        return None, (jsonify({
            'error': 'Cluster not connected',
            'offline': True,
            'connection_error': manager.connection_error
        }), 503)
    
    return manager, None


def check_cluster_access(cluster_id):
    """Check if current user can access a cluster based on tenant
    Returns (True, None) if allowed, (False, error_response) if not
    
    LW: Simple but effective - this gets called on every cluster operation
    """
    users = load_users()
    user = users.get(request.session['user'], {})
    allowed = get_user_clusters(user)
    
    if allowed is not None and cluster_id not in allowed:
        return False, (jsonify({'error': 'Access denied to this cluster'}), 403)
    
    return True, None


@app.route('/api/clusters', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def add_cluster():
    """Add a new cluster"""
    data = request.json
    
    # Validate required fields
    required = ['name', 'host', 'user', 'pass']
    for field in required:
        if field not in data:
            return jsonify({'error': f'Missing required field: {field}'}), 400
    
    # Generate unique ID
    cluster_id = str(uuid.uuid4())[:8]
    
    # Create config
    config = PegaProxConfig(data)
    
    # Create and start manager
    manager = PegaProxManager(cluster_id, config)
    
    # Test connection
    if not manager.connect_to_proxmox():
        return jsonify({'error': 'Failed to connect to Proxmox cluster'}), 400
    
    manager.start()
    cluster_managers[cluster_id] = manager
    
    # Save configuration
    save_config()
    
    # Audit log
    log_audit(request.session['user'], 'cluster.added', f"Added cluster: {data.get('name')} ({data.get('host')})")
    
    return jsonify({'id': cluster_id, 'message': 'Cluster added successfully'}), 201


@app.route('/api/clusters/<cluster_id>/nodes', methods=['GET'])
@require_auth(perms=['node.view'])
def get_cluster_nodes(cluster_id):
    """Get list of nodes in a cluster
    
    NS: Made more resilient - returns cached/last known nodes if connection fails
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    
    # Try to get live data
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/nodes"
        r = manager._create_session().get(url, timeout=10)
        
        if r.status_code == 200:
            nodes = r.json().get('data', [])
            # Cache the nodes data
            manager._cached_nodes = nodes
            return jsonify(nodes)
    except Exception as e:
        logging.debug(f"Failed to get nodes for {cluster_id}: {e}")
    
    # If live data failed, return cached data with offline status
    if hasattr(manager, '_cached_nodes') and manager._cached_nodes:
        cached = manager._cached_nodes
        # Mark all as potentially stale
        for node in cached:
            if 'connection_status' not in node:
                node['connection_status'] = 'stale'
        return jsonify(cached)
    
    # If HA is tracking nodes, return those
    if manager.ha_node_status:
        nodes = []
        for name, data in manager.ha_node_status.items():
            nodes.append({
                'node': name,
                'status': data.get('status', 'unknown'),
                'connection_status': 'from_ha_cache'
            })
        return jsonify(nodes)
    
    # Last resort - return empty but with error info
    return jsonify({
        'error': 'Connection temporarily unavailable',
        'nodes': [],
        'offline': not manager.is_connected
    }), 503


@app.route('/api/clusters/<cluster_id>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.delete'])
def delete_cluster(cluster_id):
    """Delete a cluster"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    cluster_name = mgr.config.name
    mgr.stop()
    del cluster_managers[cluster_id]
    
    # MK: Delete cluster and all related data from database
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        # Delete cluster
        db.delete_cluster(cluster_id)
        
        # Clean up related tables
        cursor.execute('DELETE FROM vm_acls WHERE cluster_id = ?', (cluster_id,))
        cursor.execute('DELETE FROM affinity_rules WHERE cluster_id = ?', (cluster_id,))
        cursor.execute('DELETE FROM cluster_alerts WHERE cluster_id = ?', (cluster_id,))
        db.conn.commit()
        
        logging.info(f"Deleted cluster {cluster_id} and related data from database")
    except Exception as e:
        logging.error(f"Failed to delete cluster from database: {e}")
    
    log_audit(request.session['user'], 'cluster.deleted', f"Deleted cluster: {cluster_name}")
    
    return jsonify({'message': 'Cluster deleted successfully'})


@app.route('/api/clusters/reorder', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.config'])
def reorder_clusters():
    """Update cluster sort order for sidebar display
    
    NS: Allows admins to reorder clusters via drag-and-drop in UI
    Request body: { "order": ["cluster_id_1", "cluster_id_2", ...] }
    """
    data = request.get_json()
    order = data.get('order', [])
    
    if not order:
        return jsonify({'error': 'No order provided'}), 400
    
    db = get_db()
    cursor = db.conn.cursor()
    
    try:
        for idx, cluster_id in enumerate(order):
            cursor.execute(
                'UPDATE clusters SET sort_order = ? WHERE id = ?',
                (idx, cluster_id)
            )
        db.conn.commit()
        
        log_audit(request.session['user'], 'cluster.reordered', f"Reordered {len(order)} clusters")
        
        return jsonify({'message': 'Cluster order updated', 'order': order})
    except Exception as e:
        logging.error(f"Failed to reorder clusters: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/sort-order', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.config'])
def update_cluster_sort_order(cluster_id):
    """Update a single cluster's sort order
    
    Request body: { "sort_order": 5 }
    """
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    data = request.get_json()
    sort_order = data.get('sort_order', 0)
    
    db = get_db()
    cursor = db.conn.cursor()
    
    try:
        cursor.execute(
            'UPDATE clusters SET sort_order = ? WHERE id = ?',
            (sort_order, cluster_id)
        )
        db.conn.commit()
        
        return jsonify({'message': 'Sort order updated', 'sort_order': sort_order})
    except Exception as e:
        logging.error(f"Failed to update sort order: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/metrics', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_cluster_metrics(cluster_id):
    """Get cluster node metrics
    
    NS: Made more resilient - returns cached/HA data if connection fails
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    # Try to get live metrics
    if mgr.is_connected:
        try:
            metrics = mgr.get_node_status()
            if metrics:
                # Cache the metrics
                mgr._cached_metrics = metrics
                return jsonify(metrics)
        except Exception as e:
            logging.debug(f"Error getting metrics for {cluster_id}: {e}")
    
    # If live data failed, try cached data
    if hasattr(mgr, '_cached_metrics') and mgr._cached_metrics:
        return jsonify(mgr._cached_metrics)
    
    # If HA is tracking nodes, build metrics from HA data
    if mgr.ha_node_status:
        ha_metrics = {}
        for name, data in mgr.ha_node_status.items():
            ha_metrics[name] = {
                'status': data.get('status', 'unknown'),
                'cpu': 0,
                'memory': {'used': 0, 'total': 0},
                'disk': {'used': 0, 'total': 0},
                'from_ha_cache': True
            }
        return jsonify(ha_metrics)
    
    # Return error with empty metrics - frontend will keep old data
    return jsonify({'error': 'Connection temporarily unavailable', 'offline': True}), 503

@app.route('/api/clusters/<cluster_id>/resources', methods=['GET'])
@require_auth()
def get_cluster_resources(cluster_id):
    """Get cluster VM resources - filtered by VM ACLs
    
    NS: Dec 2025 - Now filters based on VM-specific ACLs
    Admin sees all VMs, others see only VMs they have access to
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    if not mgr.is_connected:
        return jsonify({'error': 'Cluster not connected', 'offline': True}), 503
    
    # get all resources
    all_resources = mgr.get_vm_resources()
    
    # check if user is admin - admin sees everything
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    
    if user.get('role') == ROLE_ADMIN:
        return jsonify(all_resources)
    
    # LW: Filter VMs based on ACLs - only show VMs user can access
    acls = get_vm_acls()
    cluster_acls = acls.get(cluster_id, {})
    
    # if no ACLs defined for this cluster, check if user has general vm.view permission
    if not cluster_acls:
        if has_permission(user, 'vm.view'):
            return jsonify(all_resources)
        else:
            return jsonify([])  # no vm.view permission and no ACLs
    
    # filter resources - show VMs user has ACL access to OR general vm.view permission
    filtered = []
    has_general_view = has_permission(user, 'vm.view')
    
    for vm in all_resources:
        vmid = str(vm.get('vmid', ''))
        vm_acl = cluster_acls.get(vmid, {})
        
        if vm_acl:
            # VM has specific ACL - check if user is in whitelist
            allowed_users = vm_acl.get('users', [])
            if user['username'] in allowed_users or '*' in allowed_users:
                filtered.append(vm)
        elif has_general_view:
            # No specific ACL but user has general view permission
            filtered.append(vm)
    
    return jsonify(filtered)

@app.route('/api/clusters/<cluster_id>', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.config'])
def update_cluster_config(cluster_id):
    """Update cluster configuration"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    data = request.json
    mgr = cluster_managers[cluster_id]
    
    # update config
    updated = []
    for key, value in data.items():
        if hasattr(mgr.config, key):
            old = getattr(mgr.config, key)
            setattr(mgr.config, key, value)
            updated.append(key)
    
    save_config()
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    log_audit(usr, 'cluster.config_changed', f"Cluster {mgr.config.name} config updated: {', '.join(updated)}")
    
    return jsonify({'message': 'Configuration updated successfully', 'updated_fields': updated})

@app.route('/api/clusters/<cluster_id>/config', methods=['PATCH'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.config'])
def update_cluster_config_live(cluster_id):
    """Update cluster configuration without restart"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    data = request.json
    mgr = cluster_managers[cluster_id]
    
    for key, value in data.items():
        if hasattr(mgr.config, key):
            setattr(mgr.config, key, value)
    
    save_config()
    
    return jsonify({'message': 'Configuration updated successfully', 'updated_fields': list(data.keys())})


@app.route('/api/clusters/<cluster_id>/excluded-nodes', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_excluded_nodes(cluster_id):
    """Get list of nodes excluded from balancing
    
    NS: Feature request - allow excluding specific nodes from VM balancing
    Similar to ProxLB's exclude hosts feature
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    excluded = getattr(mgr.config, 'excluded_nodes', []) or []
    
    return jsonify({
        'excluded_nodes': excluded,
        'cluster_id': cluster_id
    })


@app.route('/api/clusters/<cluster_id>/excluded-nodes', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.config'])
def set_excluded_nodes(cluster_id):
    """Set list of nodes excluded from balancing
    
    NS: Feature request - allow excluding specific nodes from VM balancing
    Request body: { "excluded_nodes": ["node1", "node2"] }
    
    Excluded nodes will:
    - NOT be targets for automatic VM balancing
    - NOT be targets for balancing-related live migrations
    - NOT be included in balancing score calculations
    
    Note: Manual migrations TO excluded nodes are still allowed
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    data = request.get_json() or {}
    excluded_nodes = data.get('excluded_nodes', [])
    
    # Validate it's a list of strings
    if not isinstance(excluded_nodes, list):
        return jsonify({'error': 'excluded_nodes must be a list'}), 400
    
    excluded_nodes = [str(n) for n in excluded_nodes]  # Ensure strings
    
    mgr = cluster_managers[cluster_id]
    mgr.config.excluded_nodes = excluded_nodes
    
    # Save to database
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute(
            'UPDATE clusters SET excluded_nodes = ? WHERE id = ?',
            (json.dumps(excluded_nodes), cluster_id)
        )
        db.conn.commit()
    except Exception as e:
        logging.error(f"Failed to save excluded_nodes: {e}")
        return jsonify({'error': f'Database error: {str(e)}'}), 500
    
    log_audit(request.session['user'], 'cluster.excluded_nodes_changed', 
              f"Cluster {mgr.config.name}: excluded nodes set to {excluded_nodes}")
    
    return jsonify({
        'success': True,
        'excluded_nodes': excluded_nodes,
        'message': f'{len(excluded_nodes)} node(s) excluded from balancing'
    })


@app.route('/api/clusters/<cluster_id>/excluded-nodes/<node>', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.config'])
def add_excluded_node(cluster_id, node):
    """Add a single node to the exclusion list"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    excluded = getattr(mgr.config, 'excluded_nodes', []) or []
    
    if node not in excluded:
        excluded.append(node)
        mgr.config.excluded_nodes = excluded
        
        # Save to database
        try:
            db = get_db()
            cursor = db.conn.cursor()
            cursor.execute(
                'UPDATE clusters SET excluded_nodes = ? WHERE id = ?',
                (json.dumps(excluded), cluster_id)
            )
            db.conn.commit()
        except Exception as e:
            logging.error(f"Failed to save excluded_nodes: {e}")
            return jsonify({'error': f'Database error: {str(e)}'}), 500
        
        log_audit(request.session['user'], 'cluster.node_excluded', 
                  f"Node {node} excluded from balancing in cluster {mgr.config.name}")
    
    return jsonify({
        'success': True,
        'excluded_nodes': excluded,
        'message': f'Node {node} excluded from balancing'
    })


@app.route('/api/clusters/<cluster_id>/excluded-nodes/<node>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.config'])
def remove_excluded_node(cluster_id, node):
    """Remove a node from the exclusion list"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    excluded = getattr(mgr.config, 'excluded_nodes', []) or []
    
    if node in excluded:
        excluded.remove(node)
        mgr.config.excluded_nodes = excluded
        
        # Save to database
        try:
            db = get_db()
            cursor = db.conn.cursor()
            cursor.execute(
                'UPDATE clusters SET excluded_nodes = ? WHERE id = ?',
                (json.dumps(excluded), cluster_id)
            )
            db.conn.commit()
        except Exception as e:
            logging.error(f"Failed to save excluded_nodes: {e}")
            return jsonify({'error': f'Database error: {str(e)}'}), 500
        
        log_audit(request.session['user'], 'cluster.node_included', 
                  f"Node {node} re-included in balancing for cluster {mgr.config.name}")
    
    return jsonify({
        'success': True,
        'excluded_nodes': excluded,
        'message': f'Node {node} re-included in balancing'
    })


# ============================================
# Excluded VMs from Balancing API
# MK: VMs that should not be auto-migrated
# ============================================

@app.route('/api/clusters/<cluster_id>/excluded-vms', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_excluded_vms(cluster_id):
    """Get list of VMs excluded from load balancing"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        # MK: Ensure table exists (migration for existing databases)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS balancing_excluded_vms (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                cluster_id TEXT NOT NULL,
                vmid INTEGER NOT NULL,
                reason TEXT,
                created_by TEXT,
                created_at TEXT,
                UNIQUE(cluster_id, vmid)
            )
        ''')
        
        cursor.execute(
            'SELECT vmid, reason, created_by, created_at FROM balancing_excluded_vms WHERE cluster_id = ?',
            (cluster_id,)
        )
        excluded = []
        for row in cursor.fetchall():
            excluded.append({
                'vmid': row['vmid'],
                'reason': row['reason'],
                'created_by': row['created_by'],
                'created_at': row['created_at']
            })
        
        # Get VM names for display
        vms = mgr.get_vm_resources() if mgr.is_connected else []
        vm_names = {vm['vmid']: vm.get('name', f"VM {vm['vmid']}") for vm in vms}
        
        for ex in excluded:
            ex['name'] = vm_names.get(ex['vmid'], f"VM {ex['vmid']}")
        
        return jsonify({
            'excluded_vms': excluded,
            'cluster_id': cluster_id
        })
    except Exception as e:
        logging.error(f"Error getting excluded VMs: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/excluded-vms/<int:vmid>', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.config'])
def add_excluded_vm(cluster_id, vmid):
    """Add a VM to the exclusion list for load balancing"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    reason = data.get('reason', 'Manually excluded')
    user = request.session.get('user', 'system')
    
    if mgr.set_vm_balancing_excluded(vmid, True, reason, user):
        log_audit(user, 'cluster.vm_excluded', 
                  f"VM {vmid} excluded from balancing for cluster {mgr.config.name} (reason: {reason})")
        return jsonify({
            'success': True,
            'vmid': vmid,
            'message': f'VM {vmid} excluded from balancing'
        })
    else:
        return jsonify({'error': 'Failed to exclude VM'}), 500


@app.route('/api/clusters/<cluster_id>/excluded-vms/<int:vmid>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.config'])
def remove_excluded_vm(cluster_id, vmid):
    """Remove a VM from the exclusion list"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    user = request.session.get('user', 'system')
    
    if mgr.set_vm_balancing_excluded(vmid, False, user=user):
        log_audit(user, 'cluster.vm_included', 
                  f"VM {vmid} re-included in balancing for cluster {mgr.config.name}")
        return jsonify({
            'success': True,
            'vmid': vmid,
            'message': f'VM {vmid} re-included in balancing'
        })
    else:
        return jsonify({'error': 'Failed to include VM'}), 500


@app.route('/api/clusters/<cluster_id>/fallback-hosts', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_fallback_hosts(cluster_id):
    """Get list of fallback hosts for HA"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    fallback = getattr(mgr.config, 'fallback_hosts', []) or []
    
    return jsonify({
        'fallback_hosts': fallback,
        'cluster_id': cluster_id
    })


@app.route('/api/clusters/<cluster_id>/fallback-hosts', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN], perms=['cluster.config'])
def set_fallback_hosts(cluster_id):
    """Set list of fallback hosts for HA
    
    Request body: { "fallback_hosts": ["192.168.1.2", "192.168.1.3"] }
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    data = request.get_json() or {}
    fallback_hosts = data.get('fallback_hosts', [])
    
    if not isinstance(fallback_hosts, list):
        return jsonify({'error': 'fallback_hosts must be a list'}), 400
    
    fallback_hosts = [str(h) for h in fallback_hosts if h]
    
    mgr = cluster_managers[cluster_id]
    mgr.config.fallback_hosts = fallback_hosts
    
    # Save to database
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute(
            'UPDATE clusters SET fallback_hosts = ? WHERE id = ?',
            (json.dumps(fallback_hosts), cluster_id)
        )
        db.conn.commit()
    except Exception as e:
        logging.error(f"Failed to save fallback_hosts: {e}")
        return jsonify({'error': f'Database error: {str(e)}'}), 500
    
    log_audit(request.session['user'], 'cluster.fallback_hosts_changed', 
              f"Cluster {mgr.config.name}: fallback hosts set to {fallback_hosts}")
    
    return jsonify({
        'success': True,
        'fallback_hosts': fallback_hosts,
        'message': f'{len(fallback_hosts)} fallback host(s) configured'
    })


@app.route('/api/clusters/<cluster_id>/migrations', methods=['GET'])
@require_auth(perms=['vm.view'])
def get_migration_log(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    return jsonify(cluster_managers[cluster_id].last_migration_log)


@app.route('/api/clusters/<cluster_id>/tasks', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_cluster_tasks(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    if not mgr.is_connected:
        return jsonify([])
    
    limit = request.args.get('limit', 50, type=int)
    return jsonify(mgr.get_tasks(limit=limit))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/tasks/<path:upid>', methods=['DELETE'])
@require_auth(perms=['vm.stop'])  # cancelling task is like stopping
def cancel_task(cluster_id, node, upid):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    try:
        result = mgr.stop_task(node, upid)
        if result:
            # Log the action
            log_audit(
                request.session.get('user', 'system'),
                'task.cancelled',
                f'Task {upid} on {node}',
                request.remote_addr,
                cluster=mgr.config.name
            )
            return jsonify({'success': True, 'message': 'Task cancelled'})
        else:
            return jsonify({'error': 'Failed to cancel task'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500



# High Availability (HA) API Routes
@app.route('/api/clusters/<cluster_id>/ha', methods=['GET'])
@require_auth(perms=['ha.view'])
def get_ha_status(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    return jsonify(cluster_managers[cluster_id].get_ha_status())


@app.route('/api/clusters/<cluster_id>/ha/status', methods=['GET'])
@require_auth(perms=['ha.view'])
def get_ha_status_detailed(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    return jsonify(cluster_managers[cluster_id].get_ha_status())


@app.route('/api/clusters/<cluster_id>/ha/enable', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN], perms=['ha.config'])
def enable_ha(cluster_id):
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    mgr.start_ha_monitor()
    mgr.config.ha_enabled = True
    save_config()
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    log_audit(usr, 'ha.enabled', f"HA enabled for cluster {mgr.config.name}", cluster=mgr.config.name)
    
    return jsonify({
        'message': 'High Availability aktiviert',
        'status': mgr.get_ha_status()
    })


@app.route('/api/clusters/<cluster_id>/ha/disable', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN], perms=['ha.config'])
def disable_ha(cluster_id):
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    mgr.stop_ha_monitor()
    mgr.config.ha_enabled = False
    save_config()
    
    user = getattr(request, 'session', {}).get('user', 'system')
    log_audit(user, 'ha.disabled', f"High Availability disabled for cluster {mgr.config.name}", cluster=mgr.config.name)
    
    return jsonify({
        'message': 'High Availability disabled',
        'status': mgr.get_ha_status()
    })


@app.route('/api/clusters/<cluster_id>/ha/config', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def update_ha_config(cluster_id):
    """Update HA configuration including split-brain prevention settings"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    # Update HA config
    if 'quorum_enabled' in data:
        manager.ha_config['quorum_enabled'] = data['quorum_enabled']
    if 'quorum_hosts' in data:
        manager.ha_config['quorum_hosts'] = data['quorum_hosts']
    if 'quorum_gateway' in data:
        manager.ha_config['quorum_gateway'] = data['quorum_gateway']
    if 'quorum_required_votes' in data:
        manager.ha_config['quorum_required_votes'] = data['quorum_required_votes']
    if 'self_fence_enabled' in data:
        manager.ha_config['self_fence_enabled'] = data['self_fence_enabled']
    if 'watchdog_enabled' in data:
        manager.ha_config['watchdog_enabled'] = data['watchdog_enabled']
    if 'verify_network' in data:
        manager.ha_config['verify_network_before_recovery'] = data['verify_network']
    if 'recovery_delay' in data:
        manager.ha_config['recovery_delay'] = data['recovery_delay']
    if 'failure_threshold' in data:
        manager.ha_failure_threshold = data['failure_threshold']
    
    # 2-Node Cluster Mode - NS Jan 2026
    if 'two_node_mode' in data:
        manager.ha_config['two_node_mode'] = data['two_node_mode']
    if 'force_quorum_on_failure' in data:
        manager.ha_config['force_quorum_on_failure'] = data['force_quorum_on_failure']
    
    # Storage-based Split-Brain Protection - NS Jan 2026
    if 'storage_heartbeat_enabled' in data:
        manager.ha_config['storage_heartbeat_enabled'] = data['storage_heartbeat_enabled']
    
    if 'storage_heartbeat_path' in data:
        manager.ha_config['storage_heartbeat_path'] = data['storage_heartbeat_path']
        
        # Auto-enable storage heartbeat when path is provided
        if data['storage_heartbeat_path']:
            manager.ha_config['storage_heartbeat_enabled'] = True
            manager.ha_config['dual_network_mode'] = True
            
            # Auto-install node agents when storage path is configured
            def install_agents():
                try:
                    manager.logger.info("[HA] ═══════════════════════════════════════════════════════")
                    manager.logger.info("[HA] AUTO-INSTALLING NODE AGENTS FOR STORAGE HEARTBEAT")
                    manager.logger.info(f"[HA] Storage path: {data['storage_heartbeat_path']}")
                    manager.logger.info("[HA] ═══════════════════════════════════════════════════════")
                    results = manager._ha_install_agents_on_all_nodes()
                    success_count = sum(1 for v in results.values() if v)
                    manager.logger.info(f"[HA] ✓ Agent installation complete: {success_count}/{len(results)} nodes")
                except Exception as e:
                    manager.logger.error(f"[HA] ✗ Agent installation failed: {e}")
            
            threading.Thread(target=install_agents, daemon=True).start()
    
    if 'storage_heartbeat_timeout' in data:
        manager.ha_config['storage_heartbeat_timeout'] = data['storage_heartbeat_timeout']
    if 'poison_pill_enabled' in data:
        manager.ha_config['poison_pill_enabled'] = data['poison_pill_enabled']
    if 'strict_fencing' in data:
        manager.ha_config['strict_fencing'] = data['strict_fencing']
    
    # Enable/disable HA if specified
    if 'enabled' in data:
        if data['enabled'] and not manager.ha_enabled:
            manager.start_ha_monitor()
        elif not data['enabled'] and manager.ha_enabled:
            manager.stop_ha_monitor()
    
    # Save to config
    # Store HA settings in cluster config for persistence
    if not hasattr(manager.config, 'ha_settings'):
        manager.config.ha_settings = {}
    
    manager.config.ha_settings = {
        'quorum_enabled': manager.ha_config.get('quorum_enabled', True),
        'quorum_hosts': manager.ha_config.get('quorum_hosts', []),
        'quorum_gateway': manager.ha_config.get('quorum_gateway', ''),
        'quorum_required_votes': manager.ha_config.get('quorum_required_votes', 2),
        'self_fence_enabled': manager.ha_config.get('self_fence_enabled', True),
        'watchdog_enabled': manager.ha_config.get('watchdog_enabled', False),
        'verify_network': manager.ha_config.get('verify_network_before_recovery', True),
        'recovery_delay': manager.ha_config.get('recovery_delay', 30),
        'failure_threshold': manager.ha_failure_threshold,
        # 2-Node Cluster Mode
        'two_node_mode': manager.ha_config.get('two_node_mode', False),
        'force_quorum_on_failure': manager.ha_config.get('force_quorum_on_failure', False),
        # Storage-based Split-Brain Protection - NS Jan 2026
        'storage_heartbeat_enabled': manager.ha_config.get('storage_heartbeat_enabled', False),
        'storage_heartbeat_path': manager.ha_config.get('storage_heartbeat_path', ''),
        'storage_heartbeat_timeout': manager.ha_config.get('storage_heartbeat_timeout', 30),
        'poison_pill_enabled': manager.ha_config.get('poison_pill_enabled', True),
        'strict_fencing': manager.ha_config.get('strict_fencing', False),
    }
    
    save_config()
    
    user = getattr(request, 'session', {}).get('user', 'system')
    log_audit(user, 'ha.config_updated', f"HA configuration updated for cluster {manager.config.name}", cluster=manager.config.name)
    
    return jsonify({
        'message': 'HA-Konfiguration gespeichert',
        'status': manager.get_ha_status()
    })


def _save_ha_config_to_db(cluster_id: str, manager):
    """Helper to persist ha_config changes to database
    
    NS: Called after self-fence install/uninstall so status survives restart
    """
    try:
        db = get_db()
        cluster = db.get_cluster(cluster_id)
        if cluster:
            # Update ha_settings with current ha_config
            ha_settings = cluster.get('ha_settings', {})
            ha_settings['self_fence_installed'] = manager.ha_config.get('self_fence_installed', False)
            ha_settings['self_fence_nodes'] = manager.ha_config.get('self_fence_nodes', [])
            ha_settings['node_agent_installed'] = manager.ha_config.get('node_agent_installed', {})
            cluster['ha_settings'] = ha_settings
            db.save_cluster(cluster_id, cluster)
            logging.info(f"[HA] Persisted ha_config to database for {cluster_id}")
    except Exception as e:
        logging.error(f"[HA] Failed to persist ha_config: {e}")


@app.route('/api/clusters/<cluster_id>/ha/install-self-fence', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def install_self_fence_agent(cluster_id):
    """Install self-fence agent on all cluster nodes"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    
    # Run installation in background
    def do_install():
        try:
            manager.logger.info("[HA] ═══════════════════════════════════════════════════════")
            manager.logger.info("[HA] INSTALLING SELF-FENCE AGENTS ON ALL NODES")
            manager.logger.info("[HA] ═══════════════════════════════════════════════════════")
            results = manager._ha_install_self_fence_on_all_nodes()
            success_count = sum(1 for v in results.values() if v)
            manager.logger.info(f"[HA] ✓ Self-fence installation complete: {success_count}/{len(results)} nodes")
            
            # Store installation status
            manager.ha_config['self_fence_installed'] = success_count > 0
            manager.ha_config['self_fence_nodes'] = [k for k, v in results.items() if v]
            
            # NS: Persist to database so it survives restart
            _save_ha_config_to_db(cluster_id, manager)
        except Exception as e:
            manager.logger.error(f"[HA] ✗ Self-fence installation failed: {e}")
    
    threading.Thread(target=do_install, daemon=True).start()
    
    user = getattr(request, 'session', {}).get('user', 'system')
    log_audit(user, 'ha.self_fence_install', f"Self-fence agent installation started for cluster {manager.config.name}", cluster=manager.config.name)
    
    return jsonify({
        'message': 'Self-fence agent installation started',
        'status': 'installing'
    })


@app.route('/api/clusters/<cluster_id>/ha/uninstall-self-fence', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def uninstall_self_fence_agent(cluster_id):
    """Uninstall self-fence agent from all cluster nodes"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    
    # Run uninstallation in background
    def do_uninstall():
        try:
            manager.logger.info("[HA] ═══════════════════════════════════════════════════════")
            manager.logger.info("[HA] UNINSTALLING SELF-FENCE AGENTS FROM ALL NODES")
            manager.logger.info("[HA] ═══════════════════════════════════════════════════════")
            results = manager._ha_uninstall_self_fence_on_all_nodes()
            success_count = sum(1 for v in results.values() if v)
            manager.logger.info(f"[HA] ✓ Self-fence uninstallation complete: {success_count}/{len(results)} nodes")
            
            # Update status
            manager.ha_config['self_fence_installed'] = False
            manager.ha_config['self_fence_nodes'] = []
            
            # NS: Persist to database
            _save_ha_config_to_db(cluster_id, manager)
        except Exception as e:
            manager.logger.error(f"[HA] ✗ Self-fence uninstallation failed: {e}")
    
    threading.Thread(target=do_uninstall, daemon=True).start()
    
    user = getattr(request, 'session', {}).get('user', 'system')
    log_audit(user, 'ha.self_fence_uninstall', f"Self-fence agent uninstallation started for cluster {manager.config.name}", cluster=manager.config.name)
    
    return jsonify({
        'message': 'Self-fence agent uninstallation started',
        'status': 'uninstalling'
    })


@app.route('/api/clusters/<cluster_id>/ha', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def set_ha_status(cluster_id):
    """Enable or disable HA for a cluster (legacy endpoint)"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    enable = data.get('enable', True)
    
    if enable:
        manager.start_ha_monitor()
        manager.config.ha_enabled = True
        save_config()
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'ha.enabled', f"High Availability enabled for cluster {manager.config.name}", cluster=manager.config.name)
        return jsonify({
            'message': 'High Availability aktiviert',
            'status': manager.get_ha_status()
        })
    else:
        manager.stop_ha_monitor()
        manager.config.ha_enabled = False
        save_config()
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'ha.disabled', f"High Availability disabled for cluster {manager.config.name}", cluster=manager.config.name)
        return jsonify({
            'message': 'High Availability disabled',
            'status': manager.get_ha_status()
        })


# Proxmox Native HA API Routes
@app.route('/api/clusters/<cluster_id>/proxmox-ha/resources', methods=['GET'])
@require_auth(perms=['ha.view'])
def get_proxmox_ha_resources(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    return jsonify(cluster_managers[cluster_id].get_proxmox_ha_resources())


@app.route('/api/clusters/<cluster_id>/proxmox-ha/groups', methods=['GET'])
@require_auth(perms=['ha.view'])
def get_proxmox_ha_groups(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    return jsonify(cluster_managers[cluster_id].get_proxmox_ha_groups())


# MK: Create HA Group
@app.route('/api/clusters/<cluster_id>/proxmox-ha/groups', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN], perms=['ha.config'])
def create_proxmox_ha_group(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    data = request.json or {}
    group_name = data.get('group')
    nodes = data.get('nodes')
    
    if not group_name or not nodes:
        return jsonify({'error': 'group and nodes required'}), 400
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/ha/groups"
        
        payload = {
            'group': group_name,
            'nodes': nodes
        }
        if data.get('restricted'):
            payload['restricted'] = 1
        if data.get('nofailback'):
            payload['nofailback'] = 1
        if data.get('comment'):
            payload['comment'] = data['comment']
        
        resp = manager._api_post(url, data=payload)
        
        if resp.status_code == 200:
            usr = getattr(request, 'session', {}).get('user', 'system')
            log_audit(usr, 'ha.group_created', f"HA group '{group_name}' created", cluster=manager.config.name)
            return jsonify({'success': True})
        else:
            return jsonify({'error': resp.text}), 400
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# MK: Delete HA Group
@app.route('/api/clusters/<cluster_id>/proxmox-ha/groups/<group_name>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN], perms=['ha.config'])
def delete_proxmox_ha_group(cluster_id, group_name):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/ha/groups/{group_name}"
        
        resp = manager._api_delete(url)
        
        if resp.status_code == 200:
            usr = getattr(request, 'session', {}).get('user', 'system')
            log_audit(usr, 'ha.group_deleted', f"HA group '{group_name}' deleted", cluster=manager.config.name)
            return jsonify({'success': True})
        else:
            return jsonify({'error': resp.text}), 400
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/proxmox-ha/resources', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN], perms=['ha.config'])
def add_to_proxmox_ha(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    
    logging.debug(f"[HA] Add resource request: {data}")
    
    # MK: Support both sid format (vm:100) and separate vmid/type
    sid = data.get('sid', '').strip()
    if sid and ':' in sid:
        parts = sid.split(':')
        vm_type = parts[0]  # vm or ct
        vmid = parts[1]
    else:
        vmid = data.get('vmid')
        vm_type = data.get('type', 'vm')
    
    group = data.get('group')
    max_restart = data.get('max_restart', 1)
    max_relocate = data.get('max_relocate', 1)
    state = data.get('state', 'started')
    comment = data.get('comment', '')
    
    if not vmid:
        logging.warning(f"[HA] Add resource failed: no vmid/sid in request data: {data}")
        return jsonify({'error': 'vmid or sid required (format: vm:100 or ct:101)'}), 400
    
    result = mgr.add_vm_to_proxmox_ha(vmid, vm_type, group, max_restart, max_relocate, state, comment)
    
    if result['success']:
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'ha.vm_added', f"{vm_type.upper()} {vmid} added to HA" + (f" (group: {group})" if group else ""), cluster=mgr.config.name)
        return jsonify(result)
    else:
        return jsonify(result), 400


@app.route('/api/clusters/<cluster_id>/proxmox-ha/resources/<vm_type>:<int:vmid>', methods=['DELETE'])
@require_auth(perms=['ha.config'])
def remove_from_proxmox_ha(cluster_id, vm_type, vmid):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    result = mgr.remove_vm_from_proxmox_ha(vmid, vm_type)
    
    if result['success']:
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'ha.vm_removed', f"{vm_type.upper()} {vmid} removed from HA", cluster=mgr.config.name)
        return jsonify(result)
    else:
        return jsonify(result), 400


# MK: Alternative DELETE endpoint that accepts full sid string like "vm:100"
@app.route('/api/clusters/<cluster_id>/proxmox-ha/resources/<sid>', methods=['DELETE'])
@require_auth(perms=['ha.config'])
def remove_from_proxmox_ha_by_sid(cluster_id, sid):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    # Parse sid (vm:100 or ct:101)
    if ':' in sid:
        vm_type, vmid = sid.split(':', 1)
        try:
            vmid = int(vmid)
        except ValueError:
            return jsonify({'error': f'Invalid VMID in sid: {sid}'}), 400
    else:
        return jsonify({'error': f'Invalid sid format: {sid}. Expected vm:VMID or ct:VMID'}), 400
    
    result = mgr.remove_vm_from_proxmox_ha(vmid, vm_type)
    
    if result['success']:
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'ha.vm_removed', f"{vm_type.upper()} {vmid} removed from HA", cluster=mgr.config.name)
        return jsonify(result)
    else:
        return jsonify(result), 400


# =====================================================
# DATACENTER / CLUSTER CONFIGURATION API
# =====================================================

@app.route('/api/clusters/<cluster_id>/datacenter/status', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_datacenter_status(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        
        # get cluster status
        status_url = f"https://{host}:8006/api2/json/cluster/status"
        status_resp = manager._create_session().get(status_url, timeout=5)
        
        # get resources
        resources_url = f"https://{host}:8006/api2/json/cluster/resources"
        resources_resp = manager._create_session().get(resources_url, timeout=5)
        
        status_data = status_resp.json().get('data', []) if status_resp.status_code == 200 else []
        resources_data = resources_resp.json().get('data', []) if resources_resp.status_code == 200 else []
        
        # calc summary
        nodes_online = sum(1 for s in status_data if s.get('type') == 'node' and s.get('online', 0) == 1)
        nodes_offline = sum(1 for s in status_data if s.get('type') == 'node' and s.get('online', 0) == 0)
        cluster_info = next((s for s in status_data if s.get('type') == 'cluster'), {})
        
        vms_running = sum(1 for r in resources_data if r.get('type') == 'qemu' and r.get('status') == 'running')
        vms_stopped = sum(1 for r in resources_data if r.get('type') == 'qemu' and r.get('status') == 'stopped')
        cts_running = sum(1 for r in resources_data if r.get('type') == 'lxc' and r.get('status') == 'running')
        cts_stopped = sum(1 for r in resources_data if r.get('type') == 'lxc' and r.get('status') == 'stopped')
        
        # Calculate total resources
        total_cpu = sum(r.get('maxcpu', 0) for r in resources_data if r.get('type') == 'node')
        used_cpu = sum(r.get('cpu', 0) * r.get('maxcpu', 0) for r in resources_data if r.get('type') == 'node')
        total_mem = sum(r.get('maxmem', 0) for r in resources_data if r.get('type') == 'node')
        used_mem = sum(r.get('mem', 0) for r in resources_data if r.get('type') == 'node')
        total_disk = sum(r.get('maxdisk', 0) for r in resources_data if r.get('type') == 'storage')
        used_disk = sum(r.get('disk', 0) for r in resources_data if r.get('type') == 'storage')
        
        return jsonify({
            'cluster': {
                'name': cluster_info.get('name', 'Unknown'),
                'quorate': cluster_info.get('quorate', 0) == 1,
                'version': cluster_info.get('version', 0)
            },
            'nodes': {
                'online': nodes_online,
                'offline': nodes_offline,
                'total': nodes_online + nodes_offline
            },
            'guests': {
                'vms': {'running': vms_running, 'stopped': vms_stopped},
                'containers': {'running': cts_running, 'stopped': cts_stopped}
            },
            'resources': {
                'cpu': {'total': total_cpu, 'used': used_cpu, 'percent': round(used_cpu / total_cpu * 100, 1) if total_cpu > 0 else 0},
                'memory': {'total': total_mem, 'used': used_mem, 'percent': round(used_mem / total_mem * 100, 1) if total_mem > 0 else 0},
                'storage': {'total': total_disk, 'used': used_disk, 'percent': round(used_disk / total_disk * 100, 1) if total_disk > 0 else 0}
            }
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/vms', methods=['GET'])
@require_auth(perms=['vm.view'])
def get_cluster_vms_list(cluster_id):
    """Get all VMs and containers in a cluster
    
    NS: Added Dec 2025 for VM ACL management
    Returns simple list with vmid, name, node, type
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        resources_url = f"https://{host}:8006/api2/json/cluster/resources"
        resp = manager._create_session().get(resources_url, timeout=10)
        
        if resp.status_code != 200:
            return jsonify({'error': 'Failed to fetch resources'}), 500
        
        resources = resp.json().get('data', [])
        
        # filter VMs and containers
        vms = []
        for r in resources:
            if r.get('type') in ['qemu', 'lxc']:
                vms.append({
                    'vmid': r.get('vmid'),
                    'name': r.get('name', ''),
                    'node': r.get('node'),
                    'type': r.get('type'),
                    'status': r.get('status', 'unknown')
                })
        
        # sort by vmid
        vms.sort(key=lambda x: x['vmid'])
        
        return jsonify({'vms': vms})
    except Exception as e:
        logging.error(f"Error fetching VMs for cluster {cluster_id}: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/cluster-info', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_cluster_info(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/config/nodes"
        r = manager._create_session().get(url, timeout=5)
        
        if r.status_code == 200:
            return jsonify(r.json().get('data', []))
        return jsonify([])
    except:
        return jsonify([])


@app.route('/api/clusters/<cluster_id>/datacenter/join-info', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_join_info(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        
        # try to get join info
        url = f"https://{host}:8006/api2/json/cluster/config/join"
        r = manager._create_session().get(url, timeout=5)
        
        if r.status_code == 200:
            data = r.json().get('data', {})
            if 'preferred_node' not in data:
                data['preferred_node'] = host
            return jsonify(data)
        
        # fallback
        result = {
            'cluster_name': None,
            'fingerprint': None,
            'preferred_node': host,
            'nodelist': []
        }
        
        status_url = f"https://{host}:8006/api2/json/cluster/status"
        status_resp = manager._create_session().get(status_url, timeout=5)
        
        if status_resp.status_code == 200:
            status_data = status_resp.json().get('data', [])
            cluster_info = next((s for s in status_data if s.get('type') == 'cluster'), {})
            nodes = [s for s in status_data if s.get('type') == 'node']
            
            result['cluster_name'] = cluster_info.get('name', 'Unknown')
            result['nodelist'] = [{'name': n.get('name'), 'ip': n.get('ip'), 'online': n.get('online', 0)} for n in nodes]
        
        # get nodes config
        nodes_url = f"https://{host}:8006/api2/json/cluster/config/nodes"
        nodes_resp = manager._create_session().get(nodes_url, timeout=5)
        
        if nodes_resp.status_code == 200:
            nodes_data = nodes_resp.json().get('data', [])
            for node in nodes_data:
                # Update nodelist with ring0_addr
                for n in result['nodelist']:
                    if n['name'] == node.get('name'):
                        n['ring0_addr'] = node.get('ring0_addr')
                        n['pve_addr'] = node.get('pve_addr')
        
        # Try to get fingerprint via SSL certificate
        try:
            import ssl
            import socket
            import hashlib
            
            context = ssl.create_default_context()
            context.check_hostname = False
            context.verify_mode = ssl.CERT_NONE
            
            with socket.create_connection((host, 8006), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=host) as ssock:
                    cert_der = ssock.getpeercert(binary_form=True)
                    fingerprint = hashlib.sha256(cert_der).hexdigest()
                    # Format as colon-separated uppercase
                    result['fingerprint'] = ':'.join(fingerprint[i:i+2].upper() for i in range(0, len(fingerprint), 2))
        except Exception as e:
            logging.debug(f"Could not get SSL fingerprint: {e}")
            result['fingerprint'] = f'Run "pvecm status" on {host} to get fingerprint'
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/options', methods=['GET'])
@require_auth(perms=["cluster.view"])
def get_datacenter_options(cluster_id):
    """Get datacenter options"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/options"
        response = manager._create_session().get(url, timeout=5)
        
        if response.status_code == 200:
            return jsonify(response.json().get('data', {}))
        return jsonify({})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/options', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def set_datacenter_options(cluster_id):
    """Update datacenter options"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/options"
        data = request.json or {}
        
        response = manager._create_session().put(url, data=data, timeout=10)
        
        if response.status_code == 200:
            return jsonify({'success': True, 'message': 'Options updated'})
        return jsonify({'error': response.text}), response.status_code
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# Storage API
@app.route('/api/clusters/<cluster_id>/datacenter/storage', methods=['GET'])
@require_auth(perms=['storage.view'])
def get_storage_list(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/storage"
        r = manager._create_session().get(url, timeout=5)
        
        if r.status_code == 200:
            return jsonify(r.json().get('data', []))
        return jsonify([])
    except:
        return jsonify([])


@app.route('/api/clusters/<cluster_id>/datastores', methods=['GET'])
@require_auth(perms=['storage.view'])
def get_datastores(cluster_id):
    """Get all datastores with usage info"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        
        # get storage configs
        storage_url = f"https://{host}:8006/api2/json/storage"
        storage_resp = manager._create_session().get(storage_url, timeout=5)
        storage_configs = {}
        if storage_resp.status_code == 200:
            for s in storage_resp.json().get('data', []):
                storage_configs[s['storage']] = s
        
        # get nodes
        nodes_url = f"https://{host}:8006/api2/json/nodes"
        nodes_resp = manager._create_session().get(nodes_url, timeout=5)
        nodes = []
        if nodes_resp.status_code == 200:
            nodes = [n['node'] for n in nodes_resp.json().get('data', [])]
        
        shared_storages = {}
        local_storages = {}
        
        for node in nodes:
            node_storage_url = f"https://{host}:8006/api2/json/nodes/{node}/storage"
            node_storage_response = manager._create_session().get(node_storage_url, timeout=5)
            
            if node_storage_response.status_code == 200:
                for storage in node_storage_response.json().get('data', []):
                    storage_name = storage.get('storage')
                    config = storage_configs.get(storage_name, {})
                    is_shared = config.get('shared', 0) == 1
                    
                    storage_info = {
                        'storage': storage_name,
                        'type': storage.get('type', config.get('type', 'unknown')),
                        'content': storage.get('content', config.get('content', '')),
                        'total': storage.get('total', 0),
                        'used': storage.get('used', 0),
                        'avail': storage.get('avail', 0),
                        'used_fraction': storage.get('used_fraction', 0),
                        'active': storage.get('active', 1),
                        'enabled': storage.get('enabled', 1),
                        'shared': is_shared,
                        'path': config.get('path', ''),
                        'nodes': config.get('nodes', ''),
                    }
                    
                    if is_shared:
                        if storage_name not in shared_storages:
                            shared_storages[storage_name] = storage_info
                    else:
                        if node not in local_storages:
                            local_storages[node] = []
                        local_storages[node].append(storage_info)
        
        return jsonify({
            'shared': list(shared_storages.values()),
            'local': local_storages,
            'nodes': nodes
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datastores/<storage_name>/content', methods=['GET'])
@require_auth(perms=['storage.view'])
def get_datastore_content(cluster_id, storage_name):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        node = request.args.get('node')
        
        # If no node specified, find first node that has this storage
        if not node:
            nodes_url = f"https://{host}:8006/api2/json/nodes"
            nodes_response = manager._create_session().get(nodes_url, timeout=5)
            if nodes_response.status_code == 200:
                for n in nodes_response.json().get('data', []):
                    node = n['node']
                    break
        
        if not node:
            return jsonify({'error': 'No node available'}), 400
        
        # Get storage content
        content_url = f"https://{host}:8006/api2/json/nodes/{node}/storage/{storage_name}/content"
        response = manager._create_session().get(content_url, timeout=5)
        
        if response.status_code == 200:
            content = response.json().get('data', [])
            # Enhance with additional info
            for item in content:
                item['storage'] = storage_name
                item['node'] = node
                # Calculate size in human readable format
                size = item.get('size', 0)
                if size > 1024**3:
                    item['size_human'] = f"{size / 1024**3:.2f} GB"
                elif size > 1024**2:
                    item['size_human'] = f"{size / 1024**2:.2f} MB"
                else:
                    item['size_human'] = f"{size / 1024:.2f} KB"
                
                # check item is in use by a VM
                item['in_use'] = False
                if item.get('vmid'):
                    item['in_use'] = True
                    
            return jsonify(content)
        return jsonify([])
    except Exception as e:
        logging.error(f"Error getting datastore content: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datastores/<storage_name>/content/<path:volid>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def delete_datastore_content(cluster_id, storage_name, volid):
    """Delete content from a datastore (ISO, backup, etc.)"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        node = request.args.get('node')
        
        if not node:
            # Find a node that has this storage
            nodes_url = f"https://{host}:8006/api2/json/nodes"
            nodes_response = manager._create_session().get(nodes_url, timeout=5)
            if nodes_response.status_code == 200:
                for n in nodes_response.json().get('data', []):
                    node = n['node']
                    break
        
        if not node:
            return jsonify({'error': 'No node available'}), 400
        
        # check volume is in use by any VM or Container
        resources_url = f"https://{host}:8006/api2/json/cluster/resources?type=vm"
        resources_response = manager._create_session().get(resources_url, timeout=5)
        
        if resources_response.status_code == 200:
            for vm in resources_response.json().get('data', []):
                vm_node = vm.get('node')
                vmid = vm.get('vmid')
                vm_type = 'qemu' if vm.get('type') == 'qemu' else 'lxc'
                
                # Get VM/CT config to check disks and mounted ISOs
                config_url = f"https://{host}:8006/api2/json/nodes/{vm_node}/{vm_type}/{vmid}/config"
                config_response = manager._create_session().get(config_url, timeout=5)
                if config_response.status_code == 200:
                    config = config_response.json().get('data', {})
                    
                    # Check all config entries for volume reference
                    for key, value in config.items():
                        if not isinstance(value, str):
                            continue
                        
                        # Check for disk images
                        if volid in value:
                            resource_name = 'VM' if vm_type == 'qemu' else 'Container'
                            return jsonify({
                                'error': f'Volume is in use by {resource_name} {vmid} ({key})',
                                'in_use': True,
                                'vmid': vmid,
                                'type': vm_type
                            }), 400
                        
                        # Check for mounted ISOs (ide*, sata*, scsi* with media=cdrom)
                        if volid.endswith('.iso'):
                            # check this ISO is mounted
                            iso_name = volid.split('/')[-1] if '/' in volid else volid
                            if iso_name in value or volid in value:
                                return jsonify({
                                    'error': f'ISO is mounted in VM {vmid} ({key})',
                                    'in_use': True,
                                    'vmid': vmid,
                                    'type': 'qemu'
                                }), 400
                    
                    # For containers, also check mount points
                    if vm_type == 'lxc':
                        for key, value in config.items():
                            if key.startswith('mp') and isinstance(value, str) and volid in value:
                                return jsonify({
                                    'error': f'Volume is mounted in Container {vmid} ({key})',
                                    'in_use': True,
                                    'vmid': vmid,
                                    'type': 'lxc'
                                }), 400
        
        # Delete the volume
        # URL encode the volid properly
        encoded_volid = volid.replace('/', '%2F')
        delete_url = f"https://{host}:8006/api2/json/nodes/{node}/storage/{storage_name}/content/{encoded_volid}"
        response = manager._create_session().delete(delete_url, timeout=10)
        
        if response.status_code == 200:
            user = request.session.get('user', 'unknown')
            log_audit(user, 'storage.content_deleted', f"Deleted {volid} from {storage_name}", cluster=manager.config.name)
            return jsonify({'success': True, 'message': f'Deleted {volid}'})
        else:
            error_msg = response.json().get('errors', response.text) if response.text else 'Delete failed'
            return jsonify({'error': error_msg}), response.status_code
            
    except Exception as e:
        logging.error(f"Error deleting content: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datastores/<storage_name>/upload', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def upload_to_datastore(cluster_id, storage_name):
    """Upload ISO or other content to a datastore"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        node = request.form.get('node') or request.args.get('node')
        content_type = request.form.get('content', 'iso')  # iso, vztmpl, etc.
        
        if not node:
            # Find a node that has this storage
            nodes_url = f"https://{host}:8006/api2/json/nodes"
            nodes_response = manager._create_session().get(nodes_url, timeout=5)
            if nodes_response.status_code == 200:
                for n in nodes_response.json().get('data', []):
                    node = n['node']
                    break
        
        if not node:
            return jsonify({'error': 'No node available'}), 400
        
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400
        
        file = request.files['file']
        if not file.filename:
            return jsonify({'error': 'No file selected'}), 400
        
        # Check file extension
        filename = file.filename
        if content_type == 'iso' and not filename.lower().endswith('.iso'):
            return jsonify({'error': 'File must be an ISO image'}), 400
        
        # Upload to Proxmox
        upload_url = f"https://{host}:8006/api2/json/nodes/{node}/storage/{storage_name}/upload"
        
        files = {
            'filename': (filename, file.stream, 'application/octet-stream')
        }
        data = {
            'content': content_type
        }
        
        response = manager._create_session().post(upload_url, files=files, data=data, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            user = request.session.get('user', 'unknown')
            log_audit(user, 'storage.upload', f"Uploaded {filename} to {storage_name}", cluster=manager.config.name)
            return jsonify({
                'success': True, 
                'message': f'Upload started: {filename}',
                'upid': result.get('data')
            })
        else:
            error_msg = response.json().get('errors', response.text) if response.text else 'Upload failed'
            return jsonify({'error': error_msg}), response.status_code
            
    except Exception as e:
        logging.error(f"Error uploading: {e}")
        return jsonify({'error': str(e)}), 500


# NS: Download ISO from URL - Jan 2026
# Like Proxmox's "Download from URL" feature
# Tracks download progress in memory (for status polling)
_url_downloads = {}  # task_id -> { status, percent, message, ... }

@app.route('/api/clusters/<cluster_id>/datastores/<storage_name>/download-url', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def download_iso_from_url(cluster_id, storage_name):
    """Download ISO/image from URL to storage (like Proxmox)"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        data = request.json or {}
        url = data.get('url', '').strip()
        filename = data.get('filename', '').strip()
        node = data.get('node', '').strip()
        
        if not url:
            return jsonify({'error': 'URL is required'}), 400
        
        # Validate URL
        if not url.startswith('http://') and not url.startswith('https://'):
            return jsonify({'error': 'URL must start with http:// or https://'}), 400
        
        # Extract filename from URL if not provided
        if not filename:
            from urllib.parse import urlparse, unquote
            parsed = urlparse(url)
            filename = unquote(parsed.path.split('/')[-1]) or 'download.iso'
        
        # Ensure proper extension
        if not any(filename.lower().endswith(ext) for ext in ['.iso', '.img', '.qcow2', '.raw']):
            filename += '.iso'
        
        host = manager.current_host or manager.config.host
        
        # Find node if not specified
        if not node:
            nodes_url = f"https://{host}:8006/api2/json/nodes"
            nodes_response = manager._create_session().get(nodes_url, timeout=5)
            if nodes_response.status_code == 200:
                for n in nodes_response.json().get('data', []):
                    node = n['node']
                    break
        
        if not node:
            return jsonify({'error': 'No node available'}), 400
        
        # Try using Proxmox's native download-url API (PVE 7.0+)
        download_url = f"https://{host}:8006/api2/json/nodes/{node}/storage/{storage_name}/download-url"
        
        download_data = {
            'url': url,
            'filename': filename,
            'content': 'iso'
        }
        
        # Check if it's HTTPS and might need checksum verification disabled
        if url.startswith('https://'):
            download_data['verify-certificates'] = 0  # Skip SSL verification for downloads
        
        response = manager._create_session().post(download_url, data=download_data, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            upid = result.get('data')
            
            # Generate task ID for tracking
            task_id = f"dl_{int(time.time())}_{os.urandom(4).hex()}"
            
            _url_downloads[task_id] = {
                'status': 'downloading',
                'percent': 0,
                'message': f'Downloading {filename}...',
                'upid': upid,
                'cluster_id': cluster_id,
                'node': node,
                'filename': filename,
                'started': time.time()
            }
            
            # Start background thread to poll Proxmox task status
            def poll_download_status():
                try:
                    while task_id in _url_downloads:
                        task_info = _url_downloads[task_id]
                        if task_info['status'] in ['completed', 'error']:
                            break
                        
                        # Poll Proxmox task status
                        status_url = f"https://{host}:8006/api2/json/nodes/{node}/tasks/{upid}/status"
                        try:
                            status_resp = manager._create_session().get(status_url, timeout=10)
                            if status_resp.status_code == 200:
                                status_data = status_resp.json().get('data', {})
                                
                                if status_data.get('status') == 'stopped':
                                    if status_data.get('exitstatus') == 'OK':
                                        _url_downloads[task_id] = {
                                            'status': 'completed',
                                            'percent': 100,
                                            'message': f'Download complete: {filename}'
                                        }
                                    else:
                                        _url_downloads[task_id] = {
                                            'status': 'error',
                                            'percent': 0,
                                            'message': status_data.get('exitstatus', 'Download failed')
                                        }
                                    break
                                else:
                                    # Still running - try to get progress from task log
                                    log_url = f"https://{host}:8006/api2/json/nodes/{node}/tasks/{upid}/log"
                                    log_resp = manager._create_session().get(log_url, timeout=10)
                                    if log_resp.status_code == 200:
                                        log_data = log_resp.json().get('data', [])
                                        for entry in reversed(log_data):
                                            text = entry.get('t', '')
                                            # Look for progress percentage in log
                                            import re
                                            match = re.search(r'(\d+(?:\.\d+)?)\s*%', text)
                                            if match:
                                                _url_downloads[task_id]['percent'] = float(match.group(1))
                                                _url_downloads[task_id]['message'] = f'Downloading... {match.group(1)}%'
                                                break
                        except Exception as e:
                            logging.debug(f"Error polling download status: {e}")
                        
                        time.sleep(2)
                    
                    # Cleanup old entries after 5 minutes
                    time.sleep(300)
                    if task_id in _url_downloads:
                        del _url_downloads[task_id]
                        
                except Exception as e:
                    logging.error(f"Error in download status poll: {e}")
                    if task_id in _url_downloads:
                        _url_downloads[task_id] = {
                            'status': 'error',
                            'percent': 0,
                            'message': str(e)
                        }
            
            import threading
            threading.Thread(target=poll_download_status, daemon=True).start()
            
            user = request.session.get('user', 'unknown')
            log_audit(user, 'storage.download', f"Started download: {filename} from {url[:50]}...", cluster=manager.config.name)
            
            return jsonify({
                'success': True,
                'task_id': task_id,
                'message': f'Download started: {filename}'
            })
        else:
            # Proxmox API error
            try:
                error_data = response.json()
                error_msg = error_data.get('errors', {})
                if isinstance(error_msg, dict):
                    error_msg = ', '.join(f"{k}: {v}" for k, v in error_msg.items())
                elif not error_msg:
                    error_msg = response.text or 'Download failed'
            except:
                error_msg = response.text or 'Download failed'
            
            return jsonify({'error': f'Proxmox API error: {error_msg}'}), response.status_code
            
    except Exception as e:
        logging.error(f"Error starting download: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datastores/<storage_name>/download-status/<task_id>', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_download_status(cluster_id, storage_name, task_id):
    """Get status of URL download task"""
    if task_id not in _url_downloads:
        return jsonify({'status': 'unknown', 'message': 'Task not found'}), 404
    
    return jsonify(_url_downloads[task_id])


# ============================================
# NS: ESXi Integration - Dec 2025
# Uses native Proxmox ESXi storage import feature (PVE 8+)
# Much better than custom implementation - lets proxmox handle the heavy lifting
# ============================================

# Track ESXi storages we've added to proxmox clusters
# format: { cluster_id: { storage_id: { host, username, storage_name } } }
esxi_storages = {}

ESXI_CONFIG_FILE = os.path.join(CONFIG_DIR, 'esxi_storages.json')

def load_esxi_config():
    """Load ESXi storage config from SQLite database
    
    LW: this was added for vmware migration support
    useful for vmware to proxmox migrations
    NS: migrated to sqlite jan 2026
    """
    global esxi_storages
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM esxi_storages WHERE enabled = 1')
        
        esxi_storages = {}
        for row in cursor.fetchall():
            try:
                config = json.loads(row['config'] or '{}')
                cluster_id = config.get('cluster_id', 'default')
                storage_id = row['name']
                
                if cluster_id not in esxi_storages:
                    esxi_storages[cluster_id] = {}
                
                esxi_storages[cluster_id][storage_id] = {
                    'host': row['host'],
                    'username': row['username'],
                    'datastore': row['datastore'],
                    'storage_name': storage_id,
                    **config
                }
            except:
                pass  # corrupted entries happen sometimes
        
    except Exception as e:
        logging.debug(f"Loading ESXi config from DB: {e}")
        # Fallback to JSON for backwards compat
        try:
            if os.path.exists(ESXI_CONFIG_FILE):
                with open(ESXI_CONFIG_FILE, 'r') as f:
                    esxi_storages = json.load(f)
        except:
            esxi_storages = {}

def save_esxi_config():
    """Save ESXi storage config to SQLite database
    
    NS: no passwords stored here, those are handled separately with encryption
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        for cluster_id, storages in esxi_storages.items():
            for storage_id, info in storages.items():
                cursor.execute('''
                    INSERT OR REPLACE INTO esxi_storages 
                    (name, host, username, datastore, enabled, config)
                    VALUES (?, ?, ?, ?, 1, ?)
                ''', (
                    storage_id,
                    info.get('host', ''),
                    info.get('username', ''),
                    info.get('datastore', ''),
                    json.dumps({'cluster_id': cluster_id, **{k:v for k,v in info.items() if k not in ['host', 'username', 'datastore']}})
                ))
        
        db.conn.commit()
    except Exception as e:
        logging.error(f"Couldn't save ESXi config to DB: {e}")

load_esxi_config()


@app.route('/api/clusters/<cluster_id>/esxi-hosts', methods=['GET'])
@require_auth(perms=['storage.view'])
def get_esxi_hosts(cluster_id):
    """Get all ESXi storages configured for this cluster
    
    MK: Returns the esxi storages we've added to proxmox, not direct connections
    """
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    hosts = esxi_storages.get(cluster_id, {})
    
    result = []
    for storage_id, info in hosts.items():
        # check if storage is actualy online in proxmox
        connected = False
        try:
            # LW: query proxmox to see if the esxi storage is working
            storage_status = mgr._api_get(
                f"https://{mgr.host}:8006/api2/json/storage/{storage_id}"
            )
            if storage_status and storage_status.status_code == 200:
                connected = True
        except:
            pass
        
        result.append({
            'id': storage_id,
            'host': info.get('host', ''),
            'connected': connected,
            'storage_name': info.get('storage_name', storage_id)
        })
    
    return jsonify(result)


@app.route('/api/clusters/<cluster_id>/esxi-hosts', methods=['POST'])
@require_auth(perms=['admin.cluster'])
def connect_esxi_host(cluster_id):
    """Add ESXi host as Proxmox storage
    
    NS: This registers the ESXi as a storage in Proxmox using the native import feature
    Way more reliable than trying to do it ourselves with pyvmomi
    """
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    
    host = data.get('host', '').strip()
    username = data.get('username', 'root')
    password = data.get('password', '')
    skip_verify = data.get('skip_cert_verification', True)
    
    if not host or not password:
        return jsonify({'error': 'Host and password required'}), 400
    
    # generate storage name from host
    # MK: proxmox storage names can't have dots so we replace them
    storage_name = 'esxi-' + host.replace('.', '-').replace(':', '-')[:20]
    
    try:
        # Add ESXi as storage via Proxmox API
        # this is what the GUI does when you add an ESXi storage
        storage_data = {
            'storage': storage_name,
            'type': 'esxi',
            'server': host,
            'username': username,
            'password': password,
            'content': 'import',  # only for importing VMs
        }
        
        if skip_verify:
            storage_data['skip-cert-verification'] = 1
        
        response = mgr._api_post(
            f"https://{mgr.host}:8006/api2/json/storage",
            data=storage_data
        )
        
        if response.status_code not in [200, 201]:
            # might already exist or other error
            err_text = response.text
            if 'already exists' in err_text.lower():
                logging.info(f"[ESXI] Storage {storage_name} already exists, reusing")
            else:
                logging.error(f"[ESXI] Failed to add storage: {err_text}")
                return jsonify({'error': f'Failed to add ESXi storage: {err_text}'}), 500
        
        # store in our config
        if cluster_id not in esxi_storages:
            esxi_storages[cluster_id] = {}
        
        storage_id = hashlib.md5(host.encode()).hexdigest()[:8]
        esxi_storages[cluster_id][storage_id] = {
            'host': host,
            'username': username,
            'storage_name': storage_name
            # no password stored
        }
        save_esxi_config()
        
        usr = request.session.get('user', 'system')
        log_audit(usr, 'esxi.storage_added', f"Added ESXi storage: {host} as {storage_name}")
        
        logging.info(f"[ESXI] Successfully added ESXi storage {storage_name} for {host}")
        
        return jsonify({
            'success': True,
            'id': storage_id,
            'storage_name': storage_name
        })
        
    except Exception as e:
        logging.error(f"[ESXI] Error adding storage: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/esxi-hosts/<host_id>', methods=['DELETE'])
@require_auth(perms=['admin.cluster'])
def disconnect_esxi_host(cluster_id, host_id):
    """Remove ESXi storage from Proxmox"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    hosts = esxi_storages.get(cluster_id, {})
    
    if host_id not in hosts:
        return jsonify({'error': 'ESXi storage not found'}), 404
    
    storage_name = hosts[host_id].get('storage_name', '')
    host = hosts[host_id].get('host', '')
    
    try:
        # remove storage from proxmox
        if storage_name:
            response = mgr._api_delete(
                f"https://{mgr.host}:8006/api2/json/storage/{storage_name}"
            )
            # dont care too much if it fails, maybe already removed
            if response.status_code not in [200, 404]:
                logging.warning(f"[ESXI] Storage removal returned {response.status_code}")
    except Exception as e:
        logging.warning(f"[ESXI] Error removing storage: {e}")
    
    # remove from our config either way
    del esxi_storages[cluster_id][host_id]
    save_esxi_config()
    
    usr = request.session.get('user', 'system')
    log_audit(usr, 'esxi.storage_removed', f"Removed ESXi storage: {host}")
    
    return jsonify({'success': True})


@app.route('/api/clusters/<cluster_id>/esxi-hosts/<host_id>/vms', methods=['GET'])
@require_auth(perms=['storage.view'])
def get_esxi_vms(cluster_id, host_id):
    """Get VMs from ESXi host via Proxmox storage API
    
    LW: This queries proxmox which queries esxi - we dont talk to esxi directly
    Much cleaner and handles auth/rate limiting for us
    """
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    hosts = esxi_storages.get(cluster_id, {})
    
    if host_id not in hosts:
        return jsonify({'error': 'ESXi storage not found'}), 404
    
    storage_name = hosts[host_id].get('storage_name', '')
    if not storage_name:
        return jsonify({'error': 'Storage name not configured'}), 400
    
    try:
        # get a node to query from (any node works for shared storage queries)
        nodes = list(mgr.nodes.keys()) if mgr.nodes else []
        if not nodes:
            return jsonify({'error': 'No nodes available'}), 500
        
        node = nodes[0]
        
        # query storage content from proxmox
        # NS: this returns the VMs available for import
        response = mgr._api_get(
            f"https://{mgr.host}:8006/api2/json/nodes/{node}/storage/{storage_name}/content"
        )
        
        if response.status_code != 200:
            logging.error(f"[ESXI] Failed to get storage content: {response.text}")
            return jsonify({'error': 'Failed to get VM list from ESXi'}), 500
        
        content = response.json().get('data', [])
        
        vms = []
        for item in content:
            # each item is a VM that can be imported
            volid = item.get('volid', '')
            
            # try to get more details via import-metadata
            vm_info = {
                'id': volid,
                'name': item.get('name', volid.split('/')[-1] if '/' in volid else volid),
                'volid': volid,
                'power_state': 'unknown',  # proxmox doesnt tell us this directly
                'guest_os': 'Unknown',
                'num_cpu': 0,
                'memory_mb': 0
            }
            
            # MK: try to get import metadata for more details
            try:
                meta_resp = mgr._api_get(
                    f"https://{mgr.host}:8006/api2/json/nodes/{node}/storage/{storage_name}/import-metadata",
                    params={'volume': volid}
                )
                if meta_resp.status_code == 200:
                    meta = meta_resp.json().get('data', {})
                    
                    # parse the create-args for hardware info
                    create_args = meta.get('create-args', {})
                    vm_info['num_cpu'] = create_args.get('cores', create_args.get('sockets', 1))
                    vm_info['memory_mb'] = create_args.get('memory', 0)
                    vm_info['guest_os'] = create_args.get('ostype', 'Unknown')
                    
                    # use the proper name if available
                    if create_args.get('name'):
                        vm_info['name'] = create_args['name']
            except:
                pass  # metadata is optional, dont fail if we cant get it
            
            vms.append(vm_info)
        
        return jsonify(vms)
        
    except Exception as e:
        logging.error(f"[ESXI] Error listing VMs: {e}")
        return jsonify({'error': f'Failed to list VMs: {e}'}), 500


@app.route('/api/clusters/<cluster_id>/esxi-hosts/<host_id>/migrate', methods=['POST'])
@require_auth(perms=['vm.clone'])
def migrate_esxi_vm(cluster_id, host_id):
    """Import a VM from ESXi to Proxmox using native import
    
    NS: Uses the Proxmox qm import functionality which handles all the 
    vmdk conversion, disk copying etc. Much better than doing it ourselves.
    
    Requires PVE 8.1+ with pve-esxi-import-tools package
    """
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    hosts = esxi_storages.get(cluster_id, {})
    
    if host_id not in hosts:
        return jsonify({'error': 'ESXi storage not found'}), 404
    
    data = request.json or {}
    volid = data.get('vm_id') or data.get('volid')  # accept both
    target_storage = data.get('storage', 'local')
    target_node = data.get('node', '')
    live_import = data.get('live_import', False)
    
    if not volid:
        return jsonify({'error': 'VM volume ID required'}), 400
    
    storage_name = hosts[host_id].get('storage_name', '')
    if not storage_name:
        return jsonify({'error': 'Storage not configured'}), 400
    
    try:
        # figure out target node
        nodes = list(mgr.nodes.keys()) if mgr.nodes else []
        if not nodes:
            return jsonify({'error': 'No nodes available'}), 500
        
        if target_node and target_node in nodes:
            node = target_node
        else:
            node = nodes[0]
        
        # get next free vmid
        vmid_resp = mgr._api_get(f"https://{mgr.host}:8006/api2/json/cluster/nextid")
        if vmid_resp.status_code != 200:
            return jsonify({'error': 'Could not get next VMID'}), 500
        
        new_vmid = vmid_resp.json().get('data', 100)
        
        # LW: first get import metadata to build the VM config
        meta_resp = mgr._api_get(
            f"https://{mgr.host}:8006/api2/json/nodes/{node}/storage/{storage_name}/import-metadata",
            params={'volume': volid}
        )
        
        if meta_resp.status_code != 200:
            return jsonify({'error': 'Could not get VM metadata'}), 500
        
        meta = meta_resp.json().get('data', {})
        create_args = meta.get('create-args', {})
        
        # build the VM creation request
        # this is basically what the GUI does
        vm_config = {
            'vmid': new_vmid,
            'name': create_args.get('name', f'imported-{new_vmid}'),
            'memory': create_args.get('memory', 2048),
            'cores': create_args.get('cores', 1),
            'sockets': create_args.get('sockets', 1),
            'ostype': create_args.get('ostype', 'other'),
        }
        
        # set the import source - this is the magic parameter
        # format: storage:path/to/vm.vmx
        vm_config['import-from'] = f"{storage_name}:{volid.split(':')[-1]}" if ':' in volid else f"{storage_name}:{volid}"
        
        # set target storage for disks
        # MK: we need to specify where to put each disk
        disks = meta.get('disks', {})
        for disk_key, disk_info in disks.items():
            vm_config[disk_key] = f"{target_storage}:0,import-from={disk_info.get('import-from', '')}"
        
        # if no disks found in metadata, just set scsi0 as default
        if not disks:
            vm_config['scsi0'] = f"{target_storage}:0"
        
        # add network - use first available bridge
        vm_config['net0'] = 'virtio,bridge=vmbr0'
        
        if live_import:
            vm_config['live-restore'] = 1
        
        logging.info(f"[ESXI] Starting import of {volid} to VMID {new_vmid}")
        
        # create the VM (this starts the import)
        response = mgr._api_post(
            f"https://{mgr.host}:8006/api2/json/nodes/{node}/qemu",
            data=vm_config
        )
        
        if response.status_code not in [200, 201]:
            err = response.text
            logging.error(f"[ESXI] Import failed: {err}")
            return jsonify({'error': f'Import failed: {err}'}), 500
        
        task_id = response.json().get('data', '')
        
        usr = request.session.get('user', 'system')
        log_audit(usr, 'esxi.vm_imported', f"Started ESXi VM import: {volid} -> VMID {new_vmid}")
        
        return jsonify({
            'success': True,
            'vmid': new_vmid,
            'task': task_id,
            'message': f'Import started for VM {new_vmid}'
        })
        
    except Exception as e:
        logging.error(f"[ESXI] Import error: {e}")
        return jsonify({'error': str(e)}), 500


# ============================================
# Storage Balancing with Storage Clusters
# ============================================
# LW: Refactored Dec 2025 for enterprise scale (2000+ VMs, multiple clusters)
# NS: Added threading locks, rate limiting, caching - we had issues with race conditions

# Storage clusters configuration - saved per proxmox cluster
# Format: { cluster_id: { 'clusters': [ { id, name, storages: [], threshold, enabled, auto_balance, max_concurrent } ] } }
storage_clusters_config = {}
STORAGE_CLUSTERS_FILE = 'storage_clusters.json'

# Thread safety locks - MK: learned this the hard way with concurrent migrations
_storage_config_lock = threading.RLock()  # RLock allows same thread to acquire multiple times
_migration_lock = threading.Lock()
_cache_lock = threading.Lock()

# Track active auto-balance migrations to prevent duplicates
active_auto_migrations = {}

# API Rate limiting - NS: Proxmox API can get overwhelmed with too many requests
# especially on large clusters, so we throttle ourselves
class APIRateLimiter:
    """Rate limiter for Proxmox API calls per cluster
    
    LW: We tested with 3000 VMs in our lab and the Proxmox API started returning 503s
    when we hit it too fast. This prevents that.
    """
    def __init__(self, calls_per_second=10, burst_limit=20):
        self.calls_per_second = calls_per_second
        self.burst_limit = burst_limit
        self._tokens = defaultdict(lambda: burst_limit)  # per-cluster tokens
        self._last_update = defaultdict(lambda: time.time())
        self._lock = threading.Lock()
    
    def acquire(self, cluster_id: str, timeout: float = 30.0) -> bool:
        """Acquire permission to make an API call. Returns False if timed out."""
        start_time = time.time()
        
        while True:
            with self._lock:
                now = time.time()
                elapsed = now - self._last_update[cluster_id]
                
                # Replenish tokens based on time passed
                self._tokens[cluster_id] = min(
                    self.burst_limit,
                    self._tokens[cluster_id] + (elapsed * self.calls_per_second)
                )
                self._last_update[cluster_id] = now
                
                if self._tokens[cluster_id] >= 1:
                    self._tokens[cluster_id] -= 1
                    return True
            
            # Check timeout
            if time.time() - start_time > timeout:
                logging.warning(f"API rate limit timeout for cluster {cluster_id}")
                return False
            
            # Wait a bit before retrying
            time.sleep(0.1)
    
    def get_stats(self, cluster_id: str) -> dict:
        """Get current rate limit stats for monitoring"""
        with self._lock:
            return {
                'available_tokens': round(self._tokens[cluster_id], 2),
                'max_tokens': self.burst_limit,
                'calls_per_second': self.calls_per_second
            }

# Global rate limiter instance
# MK: 10 calls/sec with burst of 20 should be safe for most Proxmox setups
_api_rate_limiter = APIRateLimiter(calls_per_second=10, burst_limit=20)


# Caching layer for storage/VM data - reduces API calls significantly
class StorageDataCache:
    """Cache for storage and VM data to reduce Proxmox API load
    
    NS: With 2000 VMs, fetching all configs every minute was killing the API.
    Now we cache for 30-60 seconds and only refresh what we need.
    """
    def __init__(self):
        self._cache = {}  # { cluster_id: { key: { 'data': ..., 'expires': timestamp } } }
        self._lock = threading.Lock()
    
    def get(self, cluster_id: str, key: str) -> tuple:
        """Get cached data. Returns (data, hit) where hit is True if cache hit."""
        with self._lock:
            if cluster_id not in self._cache:
                return None, False
            
            entry = self._cache[cluster_id].get(key)
            if not entry:
                return None, False
            
            if time.time() > entry['expires']:
                del self._cache[cluster_id][key]
                return None, False
            
            return entry['data'], True
    
    def set(self, cluster_id: str, key: str, data: any, ttl_seconds: int = 30):
        """Cache data with TTL"""
        with self._lock:
            if cluster_id not in self._cache:
                self._cache[cluster_id] = {}
            
            self._cache[cluster_id][key] = {
                'data': data,
                'expires': time.time() + ttl_seconds
            }
    
    def invalidate(self, cluster_id: str, key: str = None):
        """Invalidate cache entry or entire cluster cache"""
        with self._lock:
            if cluster_id in self._cache:
                if key:
                    self._cache[cluster_id].pop(key, None)
                else:
                    del self._cache[cluster_id]
    
    def get_stats(self) -> dict:
        """Get cache statistics"""
        with self._lock:
            total_entries = sum(len(c) for c in self._cache.values())
            return {
                'clusters_cached': len(self._cache),
                'total_entries': total_entries
            }

# Global cache instance
_storage_cache = StorageDataCache()


def load_storage_clusters():
    """Load storage clusters configuration from SQLite - thread safe
    
    NS: storage clusters = ceph/gluster/etc pooled across nodes
    MK: handy for managing distributed storage like ceph pools
    migrated to sqlite jan 2026
    """
    global storage_clusters_config
    with _storage_config_lock:
        try:
            db = get_db()
            cursor = db.conn.cursor()
            cursor.execute('SELECT * FROM storage_clusters WHERE enabled = 1')
            
            storage_clusters_config = {}
            for row in cursor.fetchall():
                cluster_id = row['cluster_id']
                if cluster_id not in storage_clusters_config:
                    storage_clusters_config[cluster_id] = {'clusters': []}
                
                storage_clusters_config[cluster_id]['clusters'].append({
                    'name': row['name'],
                    'type': row['storage_type'],
                    'nodes': json.loads(row['nodes'] or '[]'),
                    **json.loads(row['config'] or '{}')
                })
        except Exception as e:
            logging.debug(f"Loading storage clusters from DB: {e}")
            # Fallback to JSON for backwards compat
            try:
                if os.path.exists(STORAGE_CLUSTERS_FILE):
                    with open(STORAGE_CLUSTERS_FILE, 'r') as f:
                        storage_clusters_config = json.load(f)
            except:
                storage_clusters_config = {}

def save_storage_clusters():
    """Save storage clusters configuration to SQLite - thread safe
    
    NS: uses lock because multiple threads might try to save at once
    """
    with _storage_config_lock:
        try:
            db = get_db()
            cursor = db.conn.cursor()
            
            for cluster_id, config in storage_clusters_config.items():
                clusters = config.get('clusters', [])
                for sc in clusters:
                    cursor.execute('''
                        INSERT OR REPLACE INTO storage_clusters 
                        (cluster_id, name, storage_type, nodes, config, enabled)
                        VALUES (?, ?, ?, ?, ?, 1)
                    ''', (
                        cluster_id,
                        sc.get('name', ''),
                        sc.get('type', 'ceph'),
                        json.dumps(sc.get('nodes', [])),
                        json.dumps({k:v for k,v in sc.items() if k not in ['name', 'type', 'nodes']})
                    ))
            
            db.conn.commit()
        except Exception as e:
            logging.error(f"Error saving storage clusters to DB: {e}")

# Load on startup
load_storage_clusters()


@app.route('/api/clusters/<cluster_id>/storage-clusters', methods=['GET'])
@require_auth(perms=["storage.view"])
def get_storage_clusters(cluster_id):
    """Get all storage clusters for a proxmox cluster"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    with _storage_config_lock:
        config = storage_clusters_config.get(cluster_id, {'clusters': []})
        # Return copy to prevent modification
        return jsonify(list(config.get('clusters', [])))


@app.route('/api/clusters/<cluster_id>/storage-clusters', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_storage_cluster(cluster_id):
    """Create a new storage cluster - thread safe"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    data = request.json or {}
    name = data.get('name', '').strip()
    storages = data.get('storages', [])
    threshold = data.get('threshold', 20)
    
    if not name:
        return jsonify({'error': 'Name is required'}), 400
    if len(storages) < 2:
        return jsonify({'error': 'At least 2 storages required'}), 400
    
    with _storage_config_lock:
        if cluster_id not in storage_clusters_config:
            storage_clusters_config[cluster_id] = {'clusters': []}
        
        # Generate unique ID
        import uuid
        new_cluster = {
            'id': str(uuid.uuid4())[:8],
            'name': name,
            'storages': storages,
            'threshold': threshold,
            'enabled': True,
            'auto_balance': data.get('auto_balance', False),
            'max_concurrent': data.get('max_concurrent', 1),
            'check_interval': data.get('check_interval', 3600),  # seconds
            'last_auto_run': None,
            'created': datetime.now().isoformat()
        }
        
        storage_clusters_config[cluster_id]['clusters'].append(new_cluster)
        save_storage_clusters()
    
    # Invalidate cache for this cluster
    _storage_cache.invalidate(cluster_id)
    
    # NS: Fixed audit log call - was causing 500 error
    user = request.session.get('user', 'unknown')
    manager = cluster_managers.get(cluster_id)
    cluster_name = manager.config.name if manager else cluster_id
    log_audit(user, 'storage_cluster.created', f"Created storage cluster '{name}' with storages: {', '.join(storages)}", cluster=cluster_name)
    
    return jsonify(new_cluster), 201


@app.route('/api/clusters/<cluster_id>/storage-clusters/<sc_id>', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def update_storage_cluster(cluster_id, sc_id):
    """Update a storage cluster - thread safe"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    with _storage_config_lock:
        if cluster_id not in storage_clusters_config:
            return jsonify({'error': 'Storage cluster not found'}), 404
        
        clusters = storage_clusters_config[cluster_id].get('clusters', [])
        
        for i, sc in enumerate(clusters):
            if sc['id'] == sc_id:
                # Update fields
                if 'name' in data:
                    sc['name'] = data['name']
                if 'storages' in data:
                    sc['storages'] = data['storages']
                if 'threshold' in data:
                    sc['threshold'] = data['threshold']
                if 'enabled' in data:
                    sc['enabled'] = data['enabled']
                if 'auto_balance' in data:
                    sc['auto_balance'] = data['auto_balance']
                if 'max_concurrent' in data:
                    sc['max_concurrent'] = data['max_concurrent']
                if 'check_interval' in data:
                    sc['check_interval'] = data['check_interval']
                
                storage_clusters_config[cluster_id]['clusters'][i] = sc
                save_storage_clusters()
                
                # Invalidate cache
                _storage_cache.invalidate(cluster_id)
                
                user = request.session.get('user', 'unknown')
                log_audit(user, 'storage_cluster.updated', f"Updated storage cluster '{sc['name']}'", cluster=manager.config.name)
                
                return jsonify(sc)
    
    return jsonify({'error': 'Storage cluster not found'}), 404


@app.route('/api/clusters/<cluster_id>/storage-clusters/<sc_id>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def delete_storage_cluster(cluster_id, sc_id):
    """Delete a storage cluster - thread safe"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    
    with _storage_config_lock:
        if cluster_id not in storage_clusters_config:
            return jsonify({'error': 'Storage cluster not found'}), 404
        
        clusters = storage_clusters_config[cluster_id].get('clusters', [])
        for i, sc in enumerate(clusters):
            if sc['id'] == sc_id:
                deleted = clusters.pop(i)
                storage_clusters_config[cluster_id]['clusters'] = clusters
                save_storage_clusters()
                
                # Invalidate cache
                _storage_cache.invalidate(cluster_id)
                
                user = request.session.get('user', 'unknown')
                log_audit(user, 'storage_cluster.deleted', f"Deleted storage cluster '{deleted['name']}'", cluster=manager.config.name)
                
                return jsonify({'success': True})
    
    return jsonify({'error': 'Storage cluster not found'}), 404


@app.route('/api/clusters/<cluster_id>/storage-clusters/<sc_id>/status', methods=['GET'])
@require_auth(perms=["storage.view"])
def get_storage_cluster_status(cluster_id, sc_id):
    """Get status and recommendations for a storage cluster
    
    LW: Optimized Dec 2025 for enterprise scale (2000+ VMs)
    - Uses caching to reduce API calls
    - Rate limiting to prevent API overload
    - Batch processing for large VM lists
    - Early exit when we have enough recommendations
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    # Find the storage cluster config (thread safe)
    sc_config = None
    with _storage_config_lock:
        if cluster_id in storage_clusters_config:
            for sc in storage_clusters_config[cluster_id].get('clusters', []):
                if sc['id'] == sc_id:
                    sc_config = dict(sc)  # Copy to avoid holding lock
                    break
    
    if not sc_config:
        return jsonify({'error': 'Storage cluster not found'}), 404
    
    try:
        host = manager.current_host or manager.config.host
        
        # Try to get storage stats from cache first
        cache_key = f"storage_stats:{sc_id}"
        storage_stats, cache_hit = _storage_cache.get(cluster_id, cache_key)
        
        if not cache_hit:
            # Rate limit API calls
            if not _api_rate_limiter.acquire(cluster_id):
                return jsonify({'error': 'API rate limit exceeded, please try again'}), 429
            
            storage_stats = []
            nodes = []
            
            # Get nodes
            nodes_url = f"https://{host}:8006/api2/json/nodes"
            nodes_response = manager._create_session().get(nodes_url, timeout=10)
            if nodes_response.status_code == 200:
                nodes = [n['node'] for n in nodes_response.json().get('data', [])]
            
            # Get storage info from first node
            if nodes:
                if not _api_rate_limiter.acquire(cluster_id):
                    return jsonify({'error': 'API rate limit exceeded'}), 429
                
                node = nodes[0]
                storage_url = f"https://{host}:8006/api2/json/nodes/{node}/storage"
                storage_response = manager._create_session().get(storage_url, timeout=10)
                
                if storage_response.status_code == 200:
                    for storage in storage_response.json().get('data', []):
                        # Only include storages that are in this storage cluster
                        if storage['storage'] not in sc_config['storages']:
                            continue
                        
                        total = storage.get('total', 0)
                        used = storage.get('used', 0)
                        usage_percent = (used / total * 100) if total > 0 else 0
                        
                        storage_stats.append({
                            'storage': storage['storage'],
                            'type': storage.get('type'),
                            'total': total,
                            'used': used,
                            'avail': storage.get('avail', 0),
                            'usage_percent': round(usage_percent, 1)
                        })
            
            # Cache storage stats for 30 seconds
            _storage_cache.set(cluster_id, cache_key, storage_stats, ttl_seconds=30)
        
        # Calculate imbalance within this storage cluster
        if len(storage_stats) >= 2:
            usages = [s['usage_percent'] for s in storage_stats]
            imbalance = max(usages) - min(usages)
        else:
            imbalance = 0
        
        # Generate recommendations if imbalance exceeds threshold
        recommendations = []
        threshold = sc_config.get('threshold', 20)
        max_recommendations = int(request.args.get('max_recommendations', 10))  # configurable
        
        if imbalance > threshold and len(storage_stats) >= 2 and sc_config.get('enabled', True):
            # Find most and least used storage in THIS cluster
            sorted_stats = sorted(storage_stats, key=lambda x: x['usage_percent'], reverse=True)
            source_storage = sorted_stats[0]
            target_storage = sorted_stats[-1]
            
            # Try to get VM list from cache
            vm_cache_key = f"vm_list:{cluster_id}"
            all_vms, vm_cache_hit = _storage_cache.get(cluster_id, vm_cache_key)
            
            if not vm_cache_hit:
                if not _api_rate_limiter.acquire(cluster_id):
                    # Return what we have so far without recommendations
                    return jsonify({
                        'id': sc_config['id'],
                        'name': sc_config['name'],
                        'enabled': sc_config.get('enabled', True),
                        'storages': storage_stats,
                        'imbalance': round(imbalance, 1),
                        'threshold': threshold,
                        'recommendations': [],
                        'rate_limited': True
                    })
                
                # Find VMs on the source storage that could be moved
                resources_url = f"https://{host}:8006/api2/json/cluster/resources?type=vm"
                resources_response = manager._create_session().get(resources_url, timeout=15)
                
                if resources_response.status_code == 200:
                    all_vms = resources_response.json().get('data', [])
                    # Cache VM list for 60 seconds (VMs don't change that often)
                    _storage_cache.set(cluster_id, vm_cache_key, all_vms, ttl_seconds=60)
                else:
                    all_vms = []
            
            # NS: Process VMs in batches to avoid blocking too long
            # and to spread out API calls over time
            vms_checked = 0
            max_vms_to_check = 100  # Don't check more than 100 VMs per request
            
            for vm in all_vms:
                if len(recommendations) >= max_recommendations:
                    break
                if vms_checked >= max_vms_to_check:
                    break
                
                vm_node = vm.get('node')
                vmid = vm.get('vmid')
                vm_type = 'qemu' if vm.get('type') == 'qemu' else 'lxc'
                vm_status = vm.get('status', '')
                
                # Try to get VM config from cache
                config_cache_key = f"vm_config:{vmid}"
                vm_config, config_cache_hit = _storage_cache.get(cluster_id, config_cache_key)
                
                if not config_cache_hit:
                    # Rate limit each config fetch
                    if not _api_rate_limiter.acquire(cluster_id, timeout=5):
                        continue  # Skip this VM if rate limited
                    
                    vms_checked += 1
                    
                    # Check VM has active tasks (snapshot, backup, etc.)
                    try:
                        status_url = f"https://{host}:8006/api2/json/nodes/{vm_node}/{vm_type}/{vmid}/status/current"
                        status_response = manager._create_session().get(status_url, timeout=5)
                        if status_response.status_code == 200:
                            status_data = status_response.json().get('data', {})
                            if status_data.get('lock'):
                                continue  # Skip VMs with active operations
                    except:
                        pass
                    
                    # Get VM config to find disks
                    config_url = f"https://{host}:8006/api2/json/nodes/{vm_node}/{vm_type}/{vmid}/config"
                    config_response = manager._create_session().get(config_url, timeout=5)
                    
                    if config_response.status_code == 200:
                        vm_config = config_response.json().get('data', {})
                        # Cache VM config for 5 minutes
                        _storage_cache.set(cluster_id, config_cache_key, vm_config, ttl_seconds=300)
                    else:
                        continue
                
                if not vm_config:
                    continue
                
                for key, value in vm_config.items():
                    if not isinstance(value, str):
                        continue
                    if not any(key.startswith(prefix) for prefix in ['scsi', 'sata', 'virtio', 'ide', 'rootfs', 'mp']):
                        continue
                    
                    # Check disk is on source storage (must be in this storage cluster)
                    if value.startswith(source_storage['storage'] + ':'):
                        disk_size = 0
                        if 'size=' in value:
                            try:
                                size_match = value.split('size=')[1].split(',')[0]
                                if 'G' in size_match:
                                    disk_size = float(size_match.replace('G', '')) * 1024**3
                                elif 'M' in size_match:
                                    disk_size = float(size_match.replace('M', '')) * 1024**2
                                elif 'T' in size_match:
                                    disk_size = float(size_match.replace('T', '')) * 1024**4
                            except:
                                pass
                        
                        recommendations.append({
                            'type': 'move_disk',
                            'vmid': vmid,
                            'vm_name': vm.get('name', f'VM {vmid}'),
                            'vm_status': vm_status,
                            'disk': key,
                            'source': source_storage['storage'],
                            'target': target_storage['storage'],
                            'disk_size': disk_size,
                            'reason': f"Balance: {source_storage['storage']} ({source_storage['usage_percent']}%) → {target_storage['storage']} ({target_storage['usage_percent']}%)"
                        })
                        
                        if len(recommendations) >= max_recommendations:
                            break
        
        # Include rate limiter stats for monitoring
        rate_stats = _api_rate_limiter.get_stats(cluster_id)
        cache_stats = _storage_cache.get_stats()
        
        return jsonify({
            'id': sc_config['id'],
            'name': sc_config['name'],
            'enabled': sc_config.get('enabled', True),
            'storages': storage_stats,
            'imbalance': round(imbalance, 1),
            'threshold': threshold,
            'recommendations': recommendations,
            # MK: Include stats for debugging large clusters
            '_stats': {
                'rate_limiter': rate_stats,
                'cache': cache_stats,
                'cache_hit': cache_hit if 'cache_hit' in dir() else False
            }
        })
        
    except Exception as e:
        logging.error(f"Error getting storage cluster status: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/storage-balancing/migrate', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def execute_storage_migration(cluster_id):
    """Execute a storage migration (move disk to different storage)"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    data = request.json or {}
    vmid = data.get('vmid')
    disk = data.get('disk')
    target_storage = data.get('target')
    
    if not all([vmid, disk, target_storage]):
        return jsonify({'error': 'Missing required parameters: vmid, disk, target'}), 400
    
    try:
        host = manager.current_host or manager.config.host
        
        # Find the VM
        resources_url = f"https://{host}:8006/api2/json/cluster/resources?type=vm"
        resources_response = manager._create_session().get(resources_url, timeout=5)
        
        vm_node = None
        vm_type = None
        
        if resources_response.status_code == 200:
            for vm in resources_response.json().get('data', []):
                if vm.get('vmid') == vmid:
                    vm_node = vm.get('node')
                    vm_type = 'qemu' if vm.get('type') == 'qemu' else 'lxc'
                    
                    # Check for lock
                    status_url = f"https://{host}:8006/api2/json/nodes/{vm_node}/{vm_type}/{vmid}/status/current"
                    status_response = manager._create_session().get(status_url, timeout=5)
                    if status_response.status_code == 200:
                        status_data = status_response.json().get('data', {})
                        if status_data.get('lock'):
                            return jsonify({
                                'error': f'VM {vmid} has an active operation ({status_data.get("lock")}). Cannot migrate disk.',
                                'locked': True
                            }), 400
                    break
        
        if not vm_node or not vm_type:
            return jsonify({'error': f'VM {vmid} not found'}), 404
        
        # Execute disk move
        move_url = f"https://{host}:8006/api2/json/nodes/{vm_node}/{vm_type}/{vmid}/move_disk"
        move_data = {
            'disk': disk,
            'storage': target_storage,
            'delete': 1  # Delete source after move
        }
        
        response = manager._create_session().post(move_url, data=move_data, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            user = request.session.get('user', 'unknown')
            log_audit(user, 'storage_balancing.disk_moved', f"Moved {disk} of VM {vmid} to {target_storage}")
            
            return jsonify({
                'success': True,
                'message': f'Disk migration started',
                'upid': result.get('data')
            })
        else:
            error_msg = response.json().get('errors', response.text) if response.text else 'Migration failed'
            return jsonify({'error': error_msg}), response.status_code
            
    except Exception as e:
        logging.error(f"Error executing storage migration: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/storage-balancing/stats', methods=['GET'])
@require_auth(roles=[ROLE_ADMIN])
def get_storage_balancing_stats(cluster_id):
    """Get storage balancing stats for monitoring large clusters
    
    MK: Added Dec 2025 for enterprise deployments
    Shows rate limiter status, cache stats, active migrations
    Useful for debugging performance issues
    """
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # Get rate limiter stats
    rate_stats = _api_rate_limiter.get_stats(cluster_id)
    
    # Get cache stats
    cache_stats = _storage_cache.get_stats()
    
    # Get active migrations for this cluster
    active_migrations = []
    with _migration_lock:
        for key, migrations in active_auto_migrations.items():
            if key.startswith(cluster_id + ':'):
                for m in migrations:
                    active_migrations.append({
                        'storage_cluster': key.split(':')[1],
                        'vmid': m.get('vmid'),
                        'disk': m.get('disk'),
                        'started': m.get('started'),
                        'active': m.get('active', False)
                    })
    
    # Get storage cluster configs
    with _storage_config_lock:
        clusters = storage_clusters_config.get(cluster_id, {}).get('clusters', [])
        cluster_info = [{
            'id': sc['id'],
            'name': sc['name'],
            'enabled': sc.get('enabled', True),
            'auto_balance': sc.get('auto_balance', False),
            'last_auto_run': sc.get('last_auto_run'),
            'check_interval': sc.get('check_interval', 3600)
        } for sc in clusters]
    
    return jsonify({
        'rate_limiter': rate_stats,
        'cache': cache_stats,
        'active_migrations': active_migrations,
        'storage_clusters': cluster_info,
        # NS: Performance tuning info
        'config': {
            'rate_limit_calls_per_second': 10,
            'rate_limit_burst': 20,
            'cache_ttl_storage': 30,
            'cache_ttl_vms': 60,
            'cache_ttl_vm_config': 300,
            'max_vms_per_status_check': 100,
            'max_vms_per_auto_balance_cycle': 50
        }
    })


def run_auto_storage_balance():
    """Background worker for automatic storage balancing
    
    LW: Refactored Dec 2025 for enterprise scale
    - Thread-safe access to config
    - Rate limiting for API calls
    - Caching to reduce load
    - Better error handling
    - Max VMs per cycle to prevent long-running loops
    """
    logging.info("Auto-balance worker started")
    
    while True:
        try:
            time.sleep(60)  # Check every minute
            
            # Get a snapshot of config (thread safe)
            with _storage_config_lock:
                config_snapshot = dict(storage_clusters_config)
            
            for cluster_id, config in config_snapshot.items():
                if cluster_id not in cluster_managers:
                    continue
                    
                manager = cluster_managers[cluster_id]
                if not manager.is_connected:
                    continue
                
                for sc in config.get('clusters', []):
                    if not sc.get('enabled') or not sc.get('auto_balance'):
                        continue
                    
                    # Check interval
                    check_interval = sc.get('check_interval', 3600)
                    last_run = sc.get('last_auto_run')
                    if last_run:
                        try:
                            last_run_time = datetime.fromisoformat(last_run)
                            if (datetime.now() - last_run_time).total_seconds() < check_interval:
                                continue
                        except:
                            pass
                    
                    # Check we have active migrations for this cluster (thread safe)
                    active_key = f"{cluster_id}:{sc['id']}"
                    with _migration_lock:
                        if active_key in active_auto_migrations:
                            # Clean up completed migrations
                            active_auto_migrations[active_key] = [
                                m for m in active_auto_migrations[active_key] 
                                if m.get('active') and (datetime.now() - datetime.fromisoformat(m['started'])).total_seconds() < 7200
                            ]
                            active_count = len(active_auto_migrations[active_key])
                            if active_count >= sc.get('max_concurrent', 1):
                                continue
                    
                    try:
                        # Rate limit - wait for token before making API calls
                        if not _api_rate_limiter.acquire(cluster_id, timeout=10):
                            logging.debug(f"Auto-balance skipped for {sc['name']} - rate limited")
                            continue
                        
                        host = manager.current_host or manager.config.host
                        
                        # Try to get storage stats from cache first
                        cache_key = f"auto_balance_storage:{sc['id']}"
                        storage_stats, cache_hit = _storage_cache.get(cluster_id, cache_key)
                        
                        if not cache_hit:
                            storage_stats = []
                            nodes_url = f"https://{host}:8006/api2/json/nodes"
                            nodes_response = manager._create_session().get(nodes_url, timeout=10)
                            nodes = []
                            if nodes_response.status_code == 200:
                                nodes = [n['node'] for n in nodes_response.json().get('data', [])]
                            
                            if nodes:
                                if not _api_rate_limiter.acquire(cluster_id, timeout=5):
                                    continue
                                    
                                storage_url = f"https://{host}:8006/api2/json/nodes/{nodes[0]}/storage"
                                storage_response = manager._create_session().get(storage_url, timeout=10)
                                
                                if storage_response.status_code == 200:
                                    for storage in storage_response.json().get('data', []):
                                        if storage['storage'] not in sc['storages']:
                                            continue
                                        total = storage.get('total', 0)
                                        used = storage.get('used', 0)
                                        usage_percent = (used / total * 100) if total > 0 else 0
                                        storage_stats.append({
                                            'storage': storage['storage'],
                                            'usage_percent': usage_percent
                                        })
                            
                            # Cache for 60 seconds
                            _storage_cache.set(cluster_id, cache_key, storage_stats, ttl_seconds=60)
                        
                        if len(storage_stats) < 2:
                            continue
                        
                        # Calculate imbalance
                        usages = [s['usage_percent'] for s in storage_stats]
                        imbalance = max(usages) - min(usages)
                        
                        if imbalance <= sc.get('threshold', 20):
                            # Update last run time (thread safe)
                            with _storage_config_lock:
                                # Re-find the cluster in case it changed
                                for sc_update in storage_clusters_config.get(cluster_id, {}).get('clusters', []):
                                    if sc_update['id'] == sc['id']:
                                        sc_update['last_auto_run'] = datetime.now().isoformat()
                                        break
                                save_storage_clusters()
                            continue
                        
                        # Find source and target
                        sorted_stats = sorted(storage_stats, key=lambda x: x['usage_percent'], reverse=True)
                        source_storage = sorted_stats[0]['storage']
                        target_storage = sorted_stats[-1]['storage']
                        
                        # Get VM list from cache or API
                        vm_cache_key = f"auto_balance_vms:{cluster_id}"
                        all_vms, vm_cache_hit = _storage_cache.get(cluster_id, vm_cache_key)
                        
                        if not vm_cache_hit:
                            if not _api_rate_limiter.acquire(cluster_id, timeout=5):
                                continue
                            
                            resources_url = f"https://{host}:8006/api2/json/cluster/resources?type=vm"
                            resources_response = manager._create_session().get(resources_url, timeout=15)
                            
                            if resources_response.status_code == 200:
                                all_vms = resources_response.json().get('data', [])
                                _storage_cache.set(cluster_id, vm_cache_key, all_vms, ttl_seconds=120)
                            else:
                                continue
                        
                        # Get node storage availability (cached)
                        node_storages_key = f"node_storages:{cluster_id}"
                        node_storages, ns_cache_hit = _storage_cache.get(cluster_id, node_storages_key)
                        
                        if not ns_cache_hit:
                            node_storages = {}
                            nodes_url = f"https://{host}:8006/api2/json/nodes"
                            nodes_resp = manager._create_session().get(nodes_url, timeout=10)
                            if nodes_resp.status_code == 200:
                                for node_info in nodes_resp.json().get('data', []):
                                    node = node_info['node']
                                    if not _api_rate_limiter.acquire(cluster_id, timeout=2):
                                        break
                                    node_storage_url = f"https://{host}:8006/api2/json/nodes/{node}/storage"
                                    ns_resp = manager._create_session().get(node_storage_url, timeout=5)
                                    if ns_resp.status_code == 200:
                                        node_storages[node] = [s['storage'] for s in ns_resp.json().get('data', [])]
                            _storage_cache.set(cluster_id, node_storages_key, node_storages, ttl_seconds=300)
                        
                        # NS: Process max 50 VMs per cycle to prevent blocking
                        vms_checked = 0
                        max_vms_per_cycle = 50
                        migration_done = False
                        
                        for vm in all_vms:
                            if migration_done or vms_checked >= max_vms_per_cycle:
                                break
                            
                            vm_node = vm.get('node')
                            vmid = vm.get('vmid')
                            vm_type = 'qemu' if vm.get('type') == 'qemu' else 'lxc'
                            
                            # Check if target storage is available on this VM's node
                            if vm_node in node_storages:
                                if target_storage not in node_storages[vm_node]:
                                    continue
                            
                            # Rate limit before checking VM status
                            if not _api_rate_limiter.acquire(cluster_id, timeout=2):
                                break
                            
                            vms_checked += 1
                            
                            # Check for lock
                            try:
                                status_url = f"https://{host}:8006/api2/json/nodes/{vm_node}/{vm_type}/{vmid}/status/current"
                                status_response = manager._create_session().get(status_url, timeout=5)
                                if status_response.status_code == 200:
                                    status_data = status_response.json().get('data', {})
                                    if status_data.get('lock'):
                                        continue
                            except:
                                continue
                            
                            # Get VM config (try cache first)
                            config_cache_key = f"vm_config:{vmid}"
                            vm_config, config_hit = _storage_cache.get(cluster_id, config_cache_key)
                            
                            if not config_hit:
                                if not _api_rate_limiter.acquire(cluster_id, timeout=2):
                                    break
                                config_url = f"https://{host}:8006/api2/json/nodes/{vm_node}/{vm_type}/{vmid}/config"
                                config_response = manager._create_session().get(config_url, timeout=5)
                                
                                if config_response.status_code == 200:
                                    vm_config = config_response.json().get('data', {})
                                    _storage_cache.set(cluster_id, config_cache_key, vm_config, ttl_seconds=300)
                                else:
                                    continue
                            
                            if not vm_config:
                                continue
                            
                            for key, value in vm_config.items():
                                if not isinstance(value, str):
                                    continue
                                if not any(key.startswith(p) for p in ['scsi', 'sata', 'virtio', 'ide', 'rootfs', 'mp']):
                                    continue
                                
                                if value.startswith(source_storage + ':'):
                                    # Execute migration
                                    if not _api_rate_limiter.acquire(cluster_id, timeout=5):
                                        break
                                    
                                    move_url = f"https://{host}:8006/api2/json/nodes/{vm_node}/{vm_type}/{vmid}/move_disk"
                                    move_data = {
                                        'disk': key,
                                        'storage': target_storage,
                                        'delete': 1
                                    }
                                    
                                    move_response = manager._create_session().post(move_url, data=move_data, timeout=10)
                                    
                                    if move_response.status_code == 200:
                                        logging.info(f"Auto-balance: Migrated {key} of VM {vmid} from {source_storage} to {target_storage}")
                                        log_audit('system', 'storage_balancing.auto_migrate', 
                                                 f"Auto-migrated {key} of VM {vmid} from {source_storage} to {target_storage}")
                                        
                                        # Track migration (thread safe)
                                        with _migration_lock:
                                            if active_key not in active_auto_migrations:
                                                active_auto_migrations[active_key] = []
                                            active_auto_migrations[active_key].append({
                                                'vmid': vmid,
                                                'disk': key,
                                                'upid': move_response.json().get('data'),
                                                'active': True,
                                                'started': datetime.now().isoformat()
                                            })
                                        
                                        # Invalidate cache after migration
                                        _storage_cache.invalidate(cluster_id, config_cache_key)
                                        _storage_cache.invalidate(cluster_id, cache_key)
                                        
                                        migration_done = True
                                    else:
                                        logging.warning(f"Auto-balance: Failed to migrate {key} of VM {vmid}: {move_response.text}")
                                    
                                    break  # Only do one migration per check
                            
                            if migration_done:
                                break
                        
                        # Update last run time (thread safe)
                        with _storage_config_lock:
                            for sc_update in storage_clusters_config.get(cluster_id, {}).get('clusters', []):
                                if sc_update['id'] == sc['id']:
                                    sc_update['last_auto_run'] = datetime.now().isoformat()
                                    break
                            save_storage_clusters()
                        
                    except Exception as e:
                        logging.error(f"Error in auto-balance for {sc['name']}: {e}")
                        
        except Exception as e:
            logging.error(f"Error in auto-balance worker: {e}")
            time.sleep(60)

# Start auto-balance thread
auto_balance_thread = threading.Thread(target=run_auto_storage_balance, daemon=True)
auto_balance_thread.start()


@app.route('/api/clusters/<cluster_id>/datacenter/storage', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_storage(cluster_id):
    """Create new storage - fully compatible with Proxmox API
    
    NS: Refactored Dec 2025 to properly handle all storage types
    See: https://pve.proxmox.com/wiki/Storage
    
    Storage types and their required fields:
    - dir: path, content
    - nfs: server, export, content, [options]
    - cifs: server, share, content, [username, password, domain]
    - lvm: vgname, [base], [saferemove]
    - lvmthin: vgname, thinpool, content
    - iscsi: portal, target
    - iscsidirect: portal, target
    - rbd: pool, monhost, [username, krbd]
    - cephfs: monhost, [path, username]
    - zfspool: pool, [sparse], content
    - zfs: portal, target, pool, [blocksize, lio_tpg]
    - pbs: server, datastore, username, password, [fingerprint]
    - btrfs: path, content
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/storage"
        data = request.json or {}
        
        # Validate required fields
        storage_type = data.get('type')
        storage_id = data.get('storage')
        
        if not storage_type:
            return jsonify({'error': 'Storage type is required'}), 400
        if not storage_id:
            return jsonify({'error': 'Storage ID is required'}), 400
        
        # Validate storage ID format (Proxmox requirements)
        import re
        if not re.match(r'^[a-zA-Z][a-zA-Z0-9\-\_\.]*$', storage_id):
            return jsonify({'error': 'Storage ID must start with a letter and contain only letters, numbers, -, _, .'}), 400
        
        # Define required fields per storage type
        required_fields = {
            'dir': ['path'],
            'nfs': ['server', 'export'],
            'cifs': ['server', 'share'],
            'lvm': ['vgname'],
            'lvmthin': ['vgname', 'thinpool'],
            'iscsi': ['portal', 'target'],
            'iscsidirect': ['portal', 'target'],
            'rbd': ['pool', 'monhost'],
            'cephfs': ['monhost'],
            'zfspool': ['pool'],
            'zfs': ['portal', 'target', 'pool'],
            'pbs': ['server', 'datastore', 'username', 'password'],
            'btrfs': ['path'],
        }
        
        # Check required fields for storage type
        if storage_type in required_fields:
            missing = [f for f in required_fields[storage_type] if not data.get(f)]
            if missing:
                return jsonify({'error': f'Missing required fields for {storage_type}: {", ".join(missing)}'}), 400
        
        # Build Proxmox-compatible request data
        # Proxmox expects form-data, and 'type' must be included
        pve_data = {}
        
        # Copy all non-empty fields
        for key, value in data.items():
            if value is not None and value != '':
                # Convert Python booleans to Proxmox format
                if isinstance(value, bool):
                    pve_data[key] = 1 if value else 0
                else:
                    pve_data[key] = value
        
        # Ensure type is set
        pve_data['type'] = storage_type
        
        logging.info(f"Creating storage {storage_id} of type {storage_type}")
        logging.debug(f"Storage data: {pve_data}")
        
        response = manager._create_session().post(url, data=pve_data, timeout=15)
        
        if response.status_code == 200:
            result = response.json()
            user = request.session.get('user', 'unknown')
            log_audit(user, 'storage.created', f"Created storage '{storage_id}' of type {storage_type}", cluster=manager.config.name)
            return jsonify({'success': True, 'message': 'Storage created', 'data': result.get('data')})
        else:
            # Parse Proxmox error
            try:
                error_data = response.json()
                error_msg = error_data.get('errors', {})
                if isinstance(error_msg, dict):
                    error_msg = ', '.join([f"{k}: {v}" for k, v in error_msg.items()])
                elif not error_msg:
                    error_msg = error_data.get('message', response.text)
            except:
                error_msg = response.text
            
            logging.error(f"Failed to create storage: {error_msg}")
            return jsonify({'error': error_msg}), response.status_code
            
    except Exception as e:
        logging.error(f"Error creating storage: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/storage/<storage_id>', methods=['GET'])
@require_auth(perms=["storage.view"])
def get_storage_config(cluster_id, storage_id):
    """Get configuration for a specific storage"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/storage/{storage_id}"
        
        response = manager._create_session().get(url, timeout=10)
        
        if response.status_code == 200:
            return jsonify(response.json().get('data', {}))
        return jsonify({'error': 'Storage not found'}), 404
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/storage/<storage_id>', methods=['PUT'])
@require_auth(perms=["storage.config"])
def update_storage(cluster_id, storage_id):
    """Update storage configuration
    
    MK: Note that you cannot change the storage type after creation
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/storage/{storage_id}"
        data = request.json or {}
        
        # Remove fields that cannot be updated
        data.pop('storage', None)  # Can't change ID
        data.pop('type', None)     # Can't change type
        
        # Build Proxmox-compatible request data
        pve_data = {}
        for key, value in data.items():
            if value is not None and value != '':
                if isinstance(value, bool):
                    pve_data[key] = 1 if value else 0
                else:
                    pve_data[key] = value
        
        # Handle the 'delete' parameter for removing optional settings
        # Proxmox uses 'delete' param with comma-separated field names
        delete_fields = data.get('delete', '')
        if delete_fields:
            pve_data['delete'] = delete_fields
        
        response = manager._create_session().put(url, data=pve_data, timeout=10)
        
        if response.status_code == 200:
            user = request.session.get('user', 'unknown')
            log_audit(user, 'storage.updated', f"Updated storage '{storage_id}'", cluster=manager.config.name)
            return jsonify({'success': True, 'message': 'Storage updated'})
        else:
            try:
                error_data = response.json()
                error_msg = error_data.get('errors', error_data.get('message', response.text))
                if isinstance(error_msg, dict):
                    error_msg = ', '.join([f"{k}: {v}" for k, v in error_msg.items()])
            except:
                error_msg = response.text
            return jsonify({'error': error_msg}), response.status_code
            
    except Exception as e:
        logging.error(f"Error updating storage: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/storage/<storage_id>', methods=['DELETE'])
@require_auth(perms=["storage.delete"])
def delete_storage(cluster_id, storage_id):
    """Delete storage
    
    LW: This only removes the storage configuration, it does NOT delete any data!
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/storage/{storage_id}"
        
        response = manager._create_session().delete(url, timeout=10)
        
        if response.status_code == 200:
            user = request.session.get('user', 'unknown')
            log_audit(user, 'storage.deleted', f"Deleted storage '{storage_id}'", cluster=manager.config.name)
            return jsonify({'success': True, 'message': 'Storage deleted'})
        else:
            try:
                error_data = response.json()
                error_msg = error_data.get('errors', error_data.get('message', response.text))
            except:
                error_msg = response.text
            return jsonify({'error': error_msg}), response.status_code
            
    except Exception as e:
        logging.error(f"Error deleting storage: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/storage/<storage_id>/status', methods=['GET'])
@require_auth(perms=["storage.view"])
def get_storage_status(cluster_id, storage_id):
    """Get storage status including usage from all nodes
    
    NS: This is useful for checking if storage is actually accessible
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        
        # Get storage config first
        config_url = f"https://{host}:8006/api2/json/storage/{storage_id}"
        config_resp = manager._create_session().get(config_url, timeout=5)
        config = {}
        if config_resp.status_code == 200:
            config = config_resp.json().get('data', {})
        
        # Get nodes
        nodes_url = f"https://{host}:8006/api2/json/nodes"
        nodes_resp = manager._create_session().get(nodes_url, timeout=5)
        nodes = []
        if nodes_resp.status_code == 200:
            nodes = [n['node'] for n in nodes_resp.json().get('data', []) if n.get('status') == 'online']
        
        # Get status from each node
        node_status = []
        for node in nodes:
            try:
                status_url = f"https://{host}:8006/api2/json/nodes/{node}/storage/{storage_id}/status"
                status_resp = manager._create_session().get(status_url, timeout=5)
                if status_resp.status_code == 200:
                    status = status_resp.json().get('data', {})
                    status['node'] = node
                    node_status.append(status)
            except:
                pass
        
        return jsonify({
            'storage': storage_id,
            'config': config,
            'status': node_status
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/storage/scan', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def scan_storage(cluster_id):
    """Scan/discover storage targets (for iSCSI, NFS exports, etc.)
    
    MK: This is useful for discovering available targets before adding storage
    NS: Changed route to not require storage_id since we're scanning BEFORE creating storage
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        data = request.json or {}
        storage_type = data.get('type', 'iscsi')
        
        # Get a node to run the scan on
        nodes_url = f"https://{host}:8006/api2/json/nodes"
        nodes_resp = manager._create_session().get(nodes_url, timeout=5)
        if nodes_resp.status_code != 200:
            return jsonify({'error': 'Could not get nodes'}), 500
        
        nodes = [n['node'] for n in nodes_resp.json().get('data', []) if n.get('status') == 'online']
        if not nodes:
            return jsonify({'error': 'No online nodes available'}), 500
        
        node = nodes[0]
        
        # Different scan endpoints for different storage types
        if storage_type == 'iscsi':
            portal = data.get('portal')
            if not portal:
                return jsonify({'error': 'Portal address required for iSCSI scan'}), 400
            scan_url = f"https://{host}:8006/api2/json/nodes/{node}/scan/iscsi"
            scan_resp = manager._create_session().get(scan_url, params={'portal': portal}, timeout=30)
            
        elif storage_type == 'nfs':
            server = data.get('server')
            if not server:
                return jsonify({'error': 'Server address required for NFS scan'}), 400
            scan_url = f"https://{host}:8006/api2/json/nodes/{node}/scan/nfs"
            scan_resp = manager._create_session().get(scan_url, params={'server': server}, timeout=30)
            
        elif storage_type == 'cifs':
            server = data.get('server')
            if not server:
                return jsonify({'error': 'Server address required for CIFS scan'}), 400
            params = {'server': server}
            if data.get('username'):
                params['username'] = data['username']
            if data.get('password'):
                params['password'] = data['password']
            if data.get('domain'):
                params['domain'] = data['domain']
            scan_url = f"https://{host}:8006/api2/json/nodes/{node}/scan/cifs"
            scan_resp = manager._create_session().get(scan_url, params=params, timeout=30)
            
        elif storage_type == 'lvm':
            scan_url = f"https://{host}:8006/api2/json/nodes/{node}/scan/lvm"
            scan_resp = manager._create_session().get(scan_url, timeout=30)
            
        elif storage_type == 'lvmthin':
            vgname = data.get('vgname')
            if not vgname:
                return jsonify({'error': 'Volume group name required for LVM-thin scan'}), 400
            scan_url = f"https://{host}:8006/api2/json/nodes/{node}/scan/lvmthin"
            scan_resp = manager._create_session().get(scan_url, params={'vg': vgname}, timeout=30)
            
        elif storage_type == 'zfs':
            scan_url = f"https://{host}:8006/api2/json/nodes/{node}/scan/zfs"
            scan_resp = manager._create_session().get(scan_url, timeout=30)
            
        else:
            return jsonify({'error': f'Scan not supported for storage type: {storage_type}'}), 400
        
        if scan_resp.status_code == 200:
            return jsonify({
                'success': True,
                'type': storage_type,
                'node': node,
                'data': scan_resp.json().get('data', [])
            })
        else:
            try:
                error_msg = scan_resp.json().get('errors', scan_resp.text)
            except:
                error_msg = scan_resp.text
            return jsonify({'error': error_msg}), scan_resp.status_code
            
    except Exception as e:
        logging.error(f"Error scanning storage: {e}")
        return jsonify({'error': str(e)}), 500


# Template Download API
# template downloads from Proxmox repo and ISOs from Proxmox appliance repository

@app.route('/api/clusters/<cluster_id>/templates/available', methods=['GET'])
@require_auth(perms=['storage.view'])
def get_available_templates(cluster_id):
    """Get available templates from Proxmox appliance repository
    
    Query params:
    - type: 'lxc' (default), 'iso', or 'all'
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        template_type = request.args.get('type', 'lxc')
        
        # Get a node to query
        nodes_url = f"https://{host}:8006/api2/json/nodes"
        nodes_resp = manager._create_session().get(nodes_url, timeout=5)
        if nodes_resp.status_code != 200:
            return jsonify({'error': 'Could not get nodes'}), 500
        
        nodes = [n['node'] for n in nodes_resp.json().get('data', []) if n.get('status') == 'online']
        if not nodes:
            return jsonify({'error': 'No online nodes available'}), 500
        
        node = nodes[0]
        templates = []
        
        # Get LXC container templates (aplinfo)
        if template_type in ['lxc', 'all']:
            apl_url = f"https://{host}:8006/api2/json/nodes/{node}/aplinfo"
            apl_resp = manager._create_session().get(apl_url, timeout=30)
            if apl_resp.status_code == 200:
                for tmpl in apl_resp.json().get('data', []):
                    templates.append({
                        'type': 'lxc',
                        'template': tmpl.get('template'),
                        'package': tmpl.get('package'),
                        'headline': tmpl.get('headline'),
                        'description': tmpl.get('description', ''),
                        'os': tmpl.get('os'),
                        'version': tmpl.get('version'),
                        'section': tmpl.get('section'),
                        'source': tmpl.get('source'),
                        'sha512sum': tmpl.get('sha512sum'),
                        'infopage': tmpl.get('infopage'),
                        'location': tmpl.get('location'),
                    })
        
        # Sort by section then package name
        templates.sort(key=lambda x: (x.get('section', ''), x.get('package', '')))
        
        return jsonify(templates)
        
    except Exception as e:
        logging.error(f"Error getting available templates: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/templates/download', methods=['POST'])
@require_auth(perms=['storage.download'])
def download_template(cluster_id):
    """Download a template to storage
    
    Body:
    - storage: Target storage name (must support vztmpl content)
    - template: Template filename (e.g., 'debian-12-standard_12.2-1_amd64.tar.zst')
    - node: Optional - specific node to download on
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        data = request.json or {}
        
        storage = data.get('storage')
        template = data.get('template')
        target_node = data.get('node')
        
        if not storage:
            return jsonify({'error': 'Storage is required'}), 400
        if not template:
            return jsonify({'error': 'Template is required'}), 400
        
        # Get a node if not specified
        if not target_node:
            nodes_url = f"https://{host}:8006/api2/json/nodes"
            nodes_resp = manager._create_session().get(nodes_url, timeout=5)
            if nodes_resp.status_code != 200:
                return jsonify({'error': 'Could not get nodes'}), 500
            
            nodes = [n['node'] for n in nodes_resp.json().get('data', []) if n.get('status') == 'online']
            if not nodes:
                return jsonify({'error': 'No online nodes available'}), 500
            target_node = nodes[0]
        
        # Download template using aplinfo/download endpoint
        download_url = f"https://{host}:8006/api2/json/nodes/{target_node}/aplinfo"
        download_data = {
            'storage': storage,
            'template': template
        }
        
        logging.info(f"Downloading template {template} to {storage} on {target_node}")
        resp = manager._create_session().post(download_url, data=download_data, timeout=60)
        
        if resp.status_code == 200:
            result = resp.json()
            user = request.session.get('user', 'unknown')
            log_audit(user, 'template.downloaded', f"Downloaded template '{template}' to storage '{storage}'", cluster=manager.config.name)
            return jsonify({
                'success': True,
                'message': f'Download started for {template}',
                'data': result.get('data'),
                'upid': result.get('data')  # Usually returns task UPID
            })
        else:
            try:
                error_msg = resp.json().get('errors', resp.text)
            except:
                error_msg = resp.text
            return jsonify({'error': error_msg}), resp.status_code
            
    except Exception as e:
        logging.error(f"Error downloading template: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/storage/<storage>/content', methods=['GET'])
@require_auth(perms=['storage.view'])
def get_node_storage_content(cluster_id, node, storage):
    """Get storage content for a specific node and storage
    
    Query params:
    - content: Filter by content type (images, iso, vztmpl, backup, rootdir)
    
    MK: Added for Import Disk feature
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        content_type = request.args.get('content', '')
        
        url = f"https://{host}:8006/api2/json/nodes/{node}/storage/{storage}/content"
        if content_type:
            url += f"?content={content_type}"
        
        resp = manager._create_session().get(url, timeout=30)
        
        if resp.status_code == 200:
            data = resp.json().get('data', [])
            return jsonify(data)
        else:
            return jsonify([])
    except Exception as e:
        logging.error(f"Error getting storage content: {e}")
        return jsonify([])


@app.route('/api/clusters/<cluster_id>/nodes/<node>/storage/<storage>/download-url', methods=['POST'])
@require_auth(perms=['storage.download'])
def download_from_url(cluster_id, node, storage):
    """Download file from URL to storage
    
    Body:
    - url: URL to download from
    - filename: Target filename
    - content: Content type (iso, vztmpl)
    - checksum: Optional checksum (format: algorithm:hash)
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        data = request.json or {}
        
        url = data.get('url')
        filename = data.get('filename')
        content = data.get('content', 'iso')
        checksum = data.get('checksum')
        
        if not url:
            return jsonify({'error': 'URL is required'}), 400
        if not filename:
            return jsonify({'error': 'Filename is required'}), 400
        
        # Use Proxmox download-url API
        download_url = f"https://{host}:8006/api2/json/nodes/{node}/storage/{storage}/download-url"
        download_data = {
            'url': url,
            'filename': filename,
            'content': content
        }
        
        if checksum:
            # Format: algorithm:hash (e.g., sha256:abc123...)
            if ':' in checksum:
                algo, hash_value = checksum.split(':', 1)
                download_data['checksum-algorithm'] = algo
                download_data['checksum'] = hash_value
        
        logging.info(f"Downloading {url} as {filename} to {storage}")
        resp = manager._create_session().post(download_url, data=download_data, timeout=60)
        
        if resp.status_code == 200:
            result = resp.json()
            user = request.session.get('user', 'unknown')
            log_audit(user, 'file.downloaded', f"Downloaded '{filename}' from URL to storage '{storage}'", cluster=manager.config.name)
            return jsonify({
                'success': True,
                'message': f'Download started for {filename}',
                'upid': result.get('data')
            })
        else:
            try:
                error_msg = resp.json().get('errors', resp.text)
            except:
                error_msg = resp.text
            return jsonify({'error': error_msg}), resp.status_code
            
    except Exception as e:
        logging.error(f"Error downloading from URL: {e}")
        return jsonify({'error': str(e)}), 500


# Backup API
@app.route('/api/clusters/<cluster_id>/datacenter/backup', methods=['GET'])
@require_auth(perms=['backup.view'])
def get_backup_jobs(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/backup"
        r = manager._create_session().get(url, timeout=5)
        
        if r.status_code == 200:
            return jsonify(r.json().get('data', []))
        return jsonify([])
    except:
        return jsonify([])


@app.route('/api/clusters/<cluster_id>/datacenter/backup', methods=['POST'])
@require_auth(perms=['backup.schedule'])
def create_backup_job(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/backup"
        data = request.json or {}
        
        r = manager._create_session().post(url, data=data, timeout=10)
        
        if r.status_code == 200:
            usr = getattr(request, 'session', {}).get('user', 'system')
            log_audit(usr, 'backup.job_created', f"Created backup job", cluster=manager.config.name)
            return jsonify({'success': True, 'message': 'Backup job created'})
        return jsonify({'error': r.text}), r.status_code
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/backup/<job_id>', methods=['PUT'])
@require_auth(perms=['backup.schedule'])
def update_backup_job(cluster_id, job_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/backup/{job_id}"
        data = request.json or {}
        
        r = manager._create_session().put(url, data=data, timeout=10)
        
        if r.status_code == 200:
            usr = getattr(request, 'session', {}).get('user', 'system')
            log_audit(usr, 'backup.job_updated', f"Updated backup job {job_id}", cluster=manager.config.name)
            return jsonify({'success': True, 'message': 'Backup job updated'})
        return jsonify({'error': r.text}), r.status_code
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/backup/<job_id>', methods=['DELETE'])
@require_auth(perms=['backup.delete'])
def delete_backup_job(cluster_id, job_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/backup/{job_id}"
        
        response = manager._create_session().delete(url, timeout=10)
        
        if response.status_code == 200:
            user = getattr(request, 'session', {}).get('user', 'system')
            log_audit(user, 'backup.job_deleted', f"Deleted backup job {job_id}", cluster=manager.config.name)
            return jsonify({'success': True, 'message': 'Backup job deleted'})
        return jsonify({'error': response.text}), response.status_code
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# ============================================
# NS: VM Backup Management - Dec 2025
# Get backups for a specific VM, restore, delete
# ============================================

@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/backups', methods=['GET'])
@require_auth(perms=['backup.view'])
def get_vm_backups(cluster_id, node, vm_type, vmid):
    """Get all backups for a specific VM
    
    LW: Scans all backup-capable storages for vzdump files matching the vmid
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        session = manager._create_session()
        
        backups = []
        
        # get all storages that can hold backups
        # NS: this is kinda slow if you have lots of storages but whatever
        storage_url = f"https://{host}:8006/api2/json/nodes/{node}/storage"
        stor_resp = session.get(storage_url, timeout=5)
        
        if stor_resp.status_code != 200:
            return jsonify([])
        
        storages = stor_resp.json().get('data', [])
        
        for storage in storages:
            # MK: only check storages that can hold backups
            content = storage.get('content', '')
            if 'backup' not in content:
                continue
            
            stor_name = storage.get('storage')
            # print(f"checking storage {stor_name}")  # debug - remove later
            content_url = f"https://{host}:8006/api2/json/nodes/{node}/storage/{stor_name}/content"
            content_resp = session.get(content_url, params={'content': 'backup'}, timeout=10)
            
            if content_resp.status_code != 200:
                continue  # meh just skip it
            
            items = content_resp.json().get('data', [])
            
            for item in items:
                # vzdump naming: vzdump-{type}-{vmid}-{date}_{time}.{ext}
                # LW: proxmox naming conventions are weird but ok
                volid = item.get('volid', '')
                filename = volid.split('/')[-1] if '/' in volid else volid.split(':')[-1]
                
                # check if this backup belongs to our VM
                # format is like: vzdump-qemu-100-2025_01_15-12_00_00.vma.zst
                if f'-{vmid}-' in filename or filename.startswith(f'vzdump-{vm_type[:4]}-{vmid}'):
                    backups.append({
                        'volid': volid,
                        'storage': stor_name,
                        'filename': filename,
                        'size': item.get('size', 0),
                        'ctime': item.get('ctime', 0),  # creation time
                        'format': item.get('format', 'unknown'),
                        'notes': item.get('notes', '')
                    })
        
        # sort by creation time, newest first
        backups.sort(key=lambda x: x.get('ctime', 0), reverse=True)
        
        return jsonify(backups)
        
    except Exception as e:
        logging.error(f"[BACKUP] Error getting VM backups: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/backups/create', methods=['POST'])
@require_auth(perms=['backup.create'])
def create_vm_backup(cluster_id, node, vm_type, vmid):
    """Create a backup of a VM
    
    NS: Uses vzdump to create a backup
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    # MK: Check pool permission for vm.backup
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    if not user_can_access_vm(user, cluster_id, vmid, 'vm.backup', vm_type):
        return jsonify({'error': 'Permission denied: vm.backup'}), 403
    
    data = request.json or {}
    storage = data.get('storage', 'local')
    mode = data.get('mode', 'snapshot')  # stop, suspend, snapshot
    compress = data.get('compress', 'zstd')
    notes = data.get('notes', '')
    
    try:
        host = manager.current_host or manager.config.host
        session = manager._create_session()
        
        # vzdump endpoint
        url = f"https://{host}:8006/api2/json/nodes/{node}/vzdump"
        
        backup_params = {
            'vmid': vmid,
            'storage': storage,
            'mode': mode,
            'compress': compress
        }
        
        if notes:
            backup_params['notes-template'] = notes
        
        response = session.post(url, data=backup_params, timeout=30)
        
        if response.status_code == 200:
            task = response.json().get('data', '')
            user = getattr(request, 'session', {}).get('user', 'system')
            log_audit(user, 'backup.created', f"Started backup for {vm_type}/{vmid}", cluster=manager.config.name)
            return jsonify({'success': True, 'task': task})
        
        return jsonify({'error': response.text}), response.status_code
        
    except Exception as e:
        logging.error(f"[BACKUP] Error creating backup: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/backups/restore', methods=['POST'])
@require_auth(perms=['backup.restore'])
def restore_vm_backup(cluster_id, node, vm_type, vmid):
    """Restore a VM from backup
    
    MK: Can restore to same VMID (overwrite) or new VMID
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    # MK: Check pool permission for vm.backup
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    if not user_can_access_vm(user, cluster_id, vmid, 'vm.backup', vm_type):
        return jsonify({'error': 'Permission denied: vm.backup'}), 403
    
    data = request.json or {}
    volid = data.get('volid')
    target_vmid = data.get('target_vmid', vmid)  # default: restore to same vmid
    target_storage = data.get('storage', '')
    start_after = data.get('start', False)
    
    if not volid:
        return jsonify({'error': 'Backup volume ID required'}), 400
    
    try:
        host = manager.current_host or manager.config.host
        session = manager._create_session()
        
        # restore endpoint depends on vm type
        # MK: why does proxmox have different endpoints for this?? annoying
        if vm_type == 'qemu':
            url = f"https://{host}:8006/api2/json/nodes/{node}/qemu"
        else:
            url = f"https://{host}:8006/api2/json/nodes/{node}/lxc"
        
        restore_params = {
            'vmid': target_vmid,
            'archive': volid,
            'force': 1 if target_vmid == vmid else 0  # force overwrite if same vmid
        }
        
        if target_storage:
            restore_params['storage'] = target_storage
        
        if start_after:
            restore_params['start'] = 1
        
        # TODO: add option to restore with different name?
        # logging.debug(f"restore params: {restore_params}")
        
        response = session.post(url, data=restore_params, timeout=30)
        
        if response.status_code == 200:
            task = response.json().get('data', '')
            user = getattr(request, 'session', {}).get('user', 'system')
            log_audit(user, 'backup.restored', f"Restored {volid} to VMID {target_vmid}", cluster=manager.config.name)
            return jsonify({'success': True, 'task': task, 'vmid': target_vmid})
        
        # NS: proxmox sometimes returns weird error messages, should probably parse them better
        return jsonify({'error': response.text}), response.status_code
        
    except Exception as e:
        logging.error(f"[BACKUP] Error restoring backup: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/backups/<path:volid>', methods=['DELETE'])
@require_auth(perms=['backup.delete'])
def delete_vm_backup(cluster_id, node, vm_type, vmid, volid):
    """Delete a specific backup
    
    LW: Deletes from the storage where the backup is located
    """
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        session = manager._create_session()
        
        # volid format is usually: storage:backup/vzdump-xxx.vma.zst
        # we need to extract storage name
        if ':' in volid:
            storage = volid.split(':')[0]
        else:
            return jsonify({'error': 'Invalid volume ID format'}), 400
        
        # URL encode the volid for the path
        encoded_volid = url_quote(volid, safe='')
        
        url = f"https://{host}:8006/api2/json/nodes/{node}/storage/{storage}/content/{encoded_volid}"
        
        response = session.delete(url, timeout=30)
        
        if response.status_code == 200:
            user = getattr(request, 'session', {}).get('user', 'system')
            log_audit(user, 'backup.deleted', f"Deleted backup {volid}", cluster=manager.config.name)
            return jsonify({'success': True})
        
        return jsonify({'error': response.text}), response.status_code
        
    except Exception as e:
        logging.error(f"[BACKUP] Error deleting backup: {e}")
        return jsonify({'error': str(e)}), 500
@app.route('/api/clusters/<cluster_id>/datacenter/replication', methods=['GET'])
@require_auth(perms=["cluster.view"])
def get_replication_jobs(cluster_id):
    """Get all replication jobs"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/replication"
        response = manager._create_session().get(url, timeout=5)
        
        if response.status_code == 200:
            return jsonify(response.json().get('data', []))
        return jsonify([])
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# MK: HA Manager Status API
@app.route('/api/clusters/<cluster_id>/datacenter/ha/status', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_ha_manager_status(cluster_id):
    """Get Proxmox HA manager status (quorum, master, lrm nodes)"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        
        # Get manager status (quorum, master, lrm for each node)
        url = f"https://{host}:8006/api2/json/cluster/ha/status/manager_status"
        resp = manager._create_session().get(url, timeout=30)
        
        if resp.status_code == 200:
            data = resp.json().get('data', {})
            return jsonify(data)
        else:
            # Fallback to current status
            url2 = f"https://{host}:8006/api2/json/cluster/ha/status/current"
            resp2 = manager._create_session().get(url2, timeout=30)
            if resp2.status_code == 200:
                return jsonify(resp2.json().get('data', []))
            return jsonify([])
    except Exception as e:
        logging.error(f"Error getting HA manager status: {e}")
        return jsonify([])


# Firewall API
@app.route('/api/clusters/<cluster_id>/datacenter/firewall/options', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_firewall_options(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/firewall/options"
        r = manager._create_session().get(url, timeout=5)
        
        if r.status_code == 200:
            return jsonify(r.json().get('data', {}))
        return jsonify({})
    except:
        return jsonify({})


@app.route('/api/clusters/<cluster_id>/datacenter/firewall/options', methods=['PUT'])
@require_auth(perms=['cluster.config'])
def set_firewall_options(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/firewall/options"
        data = request.json or {}
        
        r = manager._create_session().put(url, data=data, timeout=10)
        
        if r.status_code == 200:
            usr = getattr(request, 'session', {}).get('user', 'system')
            log_audit(usr, 'firewall.options_changed', f"Firewall options updated", cluster=manager.config.name)
            return jsonify({'success': True, 'message': 'Firewall options updated'})
        return jsonify({'error': r.text}), r.status_code
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/firewall/rules', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_firewall_rules(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/firewall/rules"
        r = manager._create_session().get(url, timeout=5)
        
        if r.status_code == 200:
            return jsonify(r.json().get('data', []))
        return jsonify([])
    except:
        return jsonify([])


@app.route('/api/clusters/<cluster_id>/datacenter/firewall/rules', methods=['POST'])
@require_auth(perms=['cluster.config'])
def create_firewall_rule(cluster_id):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/firewall/rules"
        data = request.json or {}
        
        r = manager._create_session().post(url, data=data, timeout=10)
        
        if r.status_code == 200:
            usr = getattr(request, 'session', {}).get('user', 'system')
            log_audit(usr, 'firewall.rule_created', f"Firewall rule created", cluster=manager.config.name)
            return jsonify({'success': True, 'message': 'Firewall rule created'})
        return jsonify({'error': r.text}), r.status_code
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/firewall/rules/<int:pos>', methods=['PUT'])
@require_auth(perms=['cluster.config'])
def update_firewall_rule(cluster_id, pos):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/firewall/rules/{pos}"
        data = request.json or {}
        
        r = manager._create_session().put(url, data=data, timeout=10)
        
        if r.status_code == 200:
            return jsonify({'success': True, 'message': 'Firewall rule updated'})
        return jsonify({'error': r.text}), r.status_code
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/firewall/rules/<int:pos>', methods=['DELETE'])
@require_auth(perms=['cluster.config'])
def delete_firewall_rule(cluster_id, pos):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/firewall/rules/{pos}"
        
        response = manager._create_session().delete(url, timeout=10)
        
        if response.status_code == 200:
            return jsonify({'success': True, 'message': 'Firewall rule deleted'})
        return jsonify({'error': response.text}), response.status_code
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/firewall/groups', methods=['GET'])
@require_auth(perms=["cluster.view"])
def get_firewall_groups(cluster_id):
    """Get firewall security groups"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/firewall/groups"
        response = manager._create_session().get(url, timeout=5)
        
        if response.status_code == 200:
            return jsonify(response.json().get('data', []))
        return jsonify([])
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/firewall/aliases', methods=['GET'])
@require_auth(perms=["cluster.view"])
def get_firewall_aliases(cluster_id):
    """Get firewall aliases"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/firewall/aliases"
        response = manager._create_session().get(url, timeout=5)
        
        if response.status_code == 200:
            return jsonify(response.json().get('data', []))
        return jsonify([])
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/firewall/ipset', methods=['GET'])
@require_auth(perms=["cluster.view"])
def get_firewall_ipsets(cluster_id):
    """Get firewall IP sets"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/firewall/ipset"
        response = manager._create_session().get(url, timeout=5)
        
        if response.status_code == 200:
            return jsonify(response.json().get('data', []))
        return jsonify([])
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# Resource Mappings API
@app.route('/api/clusters/<cluster_id>/datacenter/mapping/pci', methods=['GET'])
@require_auth(perms=["cluster.view"])
def get_pci_mappings(cluster_id):
    """Get PCI device mappings"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/mapping/pci"
        response = manager._create_session().get(url, timeout=5)
        
        if response.status_code == 200:
            return jsonify(response.json().get('data', []))
        return jsonify([])
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/datacenter/mapping/usb', methods=['GET'])
@require_auth(perms=["cluster.view"])
def get_usb_mappings(cluster_id):
    """Get USB device mappings"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/cluster/mapping/usb"
        response = manager._create_session().get(url, timeout=5)
        
        if response.status_code == 200:
            return jsonify(response.json().get('data', []))
        return jsonify([])
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# Maintenance Mode API Routes
@app.route('/api/clusters/<cluster_id>/nodes/<node_name>/maintenance', methods=['PUT'])
@require_auth(perms=['node.maintenance'])
def set_maintenance_mode(cluster_id, node_name):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    enable = data.get('enable', True)
    skip_evacuation = data.get('skip_evacuation', False)  # MK: for non-reboot updates
    usr = getattr(request, 'session', {}).get('user', 'system')
    
    if enable:
        task = mgr.enter_maintenance_mode(node_name, skip_evacuation=skip_evacuation)
        
        if skip_evacuation:
            log_audit(usr, 'node.maintenance_entered', f"Node {node_name} entered maintenance mode (skip_evacuation=True)", cluster=mgr.config.name)
            broadcast_action('maintenance_enter', 'node', node_name, {'status': 'completed', 'skip_evacuation': True}, cluster_id, usr)
        else:
            log_audit(usr, 'node.maintenance_entered', f"Node {node_name} entered maintenance mode", cluster=mgr.config.name)
            broadcast_action('maintenance_enter', 'node', node_name, {'status': 'evacuating'}, cluster_id, usr)
        
        return jsonify({
            'message': f'Entering maintenance mode for {node_name}',
            'skip_evacuation': skip_evacuation,
            'warning': 'VMs not evacuated - they may be affected if update fails!' if skip_evacuation else None,
            'task': task.to_dict()
        })
    else:
        success = mgr.exit_maintenance_mode(node_name)
        if success:
            log_audit(usr, 'node.maintenance_exited', f"Node {node_name} exited maintenance mode", cluster=mgr.config.name)
            broadcast_action('maintenance_exit', 'node', node_name, {}, cluster_id, usr)
            return jsonify({'message': f'Exited maintenance mode for {node_name}'})
        else:
            return jsonify({'error': f'Node {node_name} is not in maintenance mode'}), 400

@app.route('/api/clusters/<cluster_id>/nodes/<node_name>/maintenance', methods=['GET'])
@require_auth(perms=['node.view'])
def get_maintenance_status(cluster_id, node_name):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    status = mgr.get_maintenance_status(node_name)
    
    return jsonify(status if status else {'maintenance_mode': False})

@app.route('/api/clusters/<cluster_id>/nodes/<node_name>/maintenance', methods=['DELETE'])
@require_auth(perms=['node.maintenance'])
def exit_maintenance_mode_api(cluster_id, node_name):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    success = mgr.exit_maintenance_mode(node_name)
    usr = getattr(request, 'session', {}).get('user', 'system')
    
    if success:
        log_audit(usr, 'node.maintenance_exited', f"Node {node_name} exited maintenance mode", cluster=mgr.config.name)
        broadcast_action('maintenance_exit', 'node', node_name, {}, cluster_id, usr)
        return jsonify({'message': f'Exited maintenance mode for {node_name}'})
    else:
        return jsonify({'error': f'Node {node_name} is not in maintenance mode'}), 400


@app.route('/api/clusters/<cluster_id>/nodes/<node_name>/maintenance/acknowledge', methods=['POST'])
@require_auth(perms=['node.maintenance'])
def acknowledge_maintenance_warning(cluster_id, node_name):
    """Acknowledge maintenance warning (e.g., when some VMs couldn't migrate)"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    
    # Store acknowledgment in maintenance task
    if node_name in manager.nodes_in_maintenance:
        manager.nodes_in_maintenance[node_name].acknowledged = True
        
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'node.maintenance_acknowledged', f"User acknowledged maintenance warning for {node_name}", cluster=manager.config.name)
        
        # Broadcast update
        broadcast_action('maintenance_acknowledged', 'node', node_name, {}, cluster_id, user)
        
        return jsonify({'message': f'Maintenance warning acknowledged for {node_name}'})
    else:
        return jsonify({'error': f'Node {node_name} is not in maintenance mode'}), 400


# =============================================================================
# NODE CLUSTER MANAGEMENT API - Join/Remove nodes from cluster
# Added by Node Management Integration
# =============================================================================

@app.route('/api/clusters/<cluster_id>/nodes/join/test', methods=['POST'])
@require_auth(perms=['cluster.admin'])
def test_node_connection(cluster_id):
    """Test SSH connection to a new node and gather system info"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    paramiko = get_paramiko()
    if not paramiko:
        return jsonify({'error': 'SSH not available. Install paramiko: pip install paramiko'}), 500
    
    data = request.get_json() or {}
    node_ip = data.get('node_ip', '').strip()
    username = data.get('username', 'root')
    password = data.get('password', '')
    ssh_port = int(data.get('ssh_port', 22))
    
    if not node_ip:
        return jsonify({'success': False, 'error': 'Node IP is required'}), 400
    if not password:
        return jsonify({'success': False, 'error': 'SSH password is required'}), 400
    
    try:
        # Connect via SSH
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(node_ip, port=ssh_port, username=username, password=password, timeout=15)
        
        # Get hostname
        stdin, stdout, stderr = ssh.exec_command('hostname')
        hostname = stdout.read().decode().strip()
        
        # Check if Proxmox is installed
        stdin, stdout, stderr = ssh.exec_command('pveversion 2>/dev/null || echo "NOT_INSTALLED"')
        pve_output = stdout.read().decode().strip()
        proxmox_installed = 'NOT_INSTALLED' not in pve_output
        proxmox_version = pve_output if proxmox_installed else None
        
        # Check if already in a cluster
        stdin, stdout, stderr = ssh.exec_command('pvecm status 2>/dev/null || echo "NO_CLUSTER"')
        cluster_output = stdout.read().decode().strip()
        already_in_cluster = 'NO_CLUSTER' not in cluster_output and 'Cluster information' in cluster_output
        
        current_cluster = None
        if already_in_cluster:
            # Extract cluster name
            for line in cluster_output.split('\n'):
                if 'Cluster Name:' in line:
                    current_cluster = line.split(':')[1].strip()
                    break
        
        ssh.close()
        
        return jsonify({
            'success': True,
            'info': {
                'hostname': hostname,
                'ip': node_ip,
                'proxmox_installed': proxmox_installed,
                'proxmox_version': proxmox_version,
                'already_in_cluster': already_in_cluster,
                'current_cluster': current_cluster
            }
        })
        
    except paramiko.AuthenticationException:
        return jsonify({'success': False, 'error': 'Authentication failed. Check username/password.'}), 401
    except paramiko.SSHException as e:
        return jsonify({'success': False, 'error': f'SSH error: {str(e)}'}), 500
    except socket.timeout:
        return jsonify({'success': False, 'error': 'Connection timeout. Check IP and network.'}), 500
    except Exception as e:
        return jsonify({'success': False, 'error': f'Connection failed: {str(e)}'}), 500


@app.route('/api/clusters/<cluster_id>/nodes/join', methods=['POST'])
@require_auth(perms=['cluster.admin'])
def join_node_to_cluster(cluster_id):
    """Add a new node to the Proxmox cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    paramiko = get_paramiko()
    if not paramiko:
        return jsonify({'error': 'SSH not available. Install paramiko: pip install paramiko'}), 500
    
    mgr = cluster_managers[cluster_id]
    data = request.get_json() or {}
    
    node_ip = data.get('node_ip', '').strip()
    username = data.get('username', 'root')
    password = data.get('password', '')
    ssh_port = int(data.get('ssh_port', 22))
    link0_address = data.get('link0_address', '').strip()
    
    if not node_ip or not password:
        return jsonify({'success': False, 'error': 'Node IP and password are required'}), 400
    
    try:
        # Get join information from existing cluster
        join_info = mgr.api_request('GET', '/cluster/config/join')
        if not join_info:
            return jsonify({'success': False, 'error': 'Could not get cluster join information'}), 500
        
        # Extract fingerprint and join address
        fingerprint = join_info.get('fingerprint', '')
        # Find the best node to join to (first online node)
        join_addr = None
        for node_data in join_info.get('nodelist', []):
            if node_data.get('ring0_addr'):
                join_addr = node_data.get('ring0_addr')
                break
        
        if not join_addr:
            # Fallback to cluster host
            join_addr = mgr.config.host
        
        if not fingerprint:
            return jsonify({'success': False, 'error': 'Could not get cluster fingerprint'}), 500
        
        # Connect to new node via SSH
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(node_ip, port=ssh_port, username=username, password=password, timeout=30)
        
        # Use interactive shell for pvecm add (it prompts for password)
        channel = ssh.invoke_shell()
        time.sleep(0.5)
        
        # Clear initial output
        if channel.recv_ready():
            channel.recv(4096)
        
        # Build and send the join command
        join_cmd = f'pvecm add {join_addr} --fingerprint {fingerprint}'
        if link0_address:
            join_cmd += f' --link0 {link0_address}'
        
        channel.send(join_cmd + '\n')
        time.sleep(2)  # Wait for password prompt
        
        # Read output to check for password prompt
        output = ''
        for _ in range(10):
            if channel.recv_ready():
                output += channel.recv(4096).decode('utf-8', errors='ignore')
            time.sleep(0.5)
            if 'password' in output.lower() or 'Password' in output:
                break
        
        # Send password for the cluster root user
        channel.send(password + '\n')
        
        # Wait for completion (join can take 30-60 seconds)
        time.sleep(5)
        full_output = output
        for _ in range(60):  # Wait up to 60 seconds
            if channel.recv_ready():
                chunk = channel.recv(4096).decode('utf-8', errors='ignore')
                full_output += chunk
                if 'successfully' in chunk.lower() or 'joined' in chunk.lower():
                    break
                if 'error' in chunk.lower() or 'failed' in chunk.lower():
                    break
            time.sleep(1)
        
        channel.close()
        ssh.close()
        
        # Check result
        if 'error' in full_output.lower() or 'failed' in full_output.lower():
            # Extract error message
            error_lines = [l for l in full_output.split('\n') if 'error' in l.lower() or 'failed' in l.lower()]
            error_msg = error_lines[0] if error_lines else 'Join command failed'
            return jsonify({'success': False, 'error': error_msg}), 500
        
        # Log the action
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'cluster.node_joined', f"Node {node_ip} joined cluster", cluster=mgr.config.name)
        
        return jsonify({
            'success': True,
            'message': f'Node successfully joined the cluster. It may take a moment to appear in the dashboard.'
        })
        
    except paramiko.AuthenticationException:
        return jsonify({'success': False, 'error': 'SSH authentication failed'}), 401
    except Exception as e:
        logging.error(f"Error joining node to cluster: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node_name>/can-remove', methods=['GET'])
@require_auth(perms=['cluster.admin'])
def check_can_remove_node(cluster_id, node_name):
    """Check if a node can be safely removed from the cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    # Check maintenance status
    in_maintenance = node_name in maintenance_tasks
    maintenance_complete = False
    if in_maintenance:
        task = maintenance_tasks[node_name]
        maintenance_complete = task.get('status') in ['completed', 'completed_with_errors']
    
    # Check if node is offline
    is_offline = True
    try:
        nodes = mgr.api_request('GET', '/nodes')
        for node in (nodes or []):
            if node.get('node') == node_name:
                is_offline = node.get('status') != 'online'
                break
    except:
        pass
    
    # Check for VMs/CTs on the node
    has_vms = False
    vm_count = 0
    try:
        # Try to get resources on the node
        resources = mgr.api_request('GET', f'/nodes/{node_name}/qemu') or []
        resources += mgr.api_request('GET', f'/nodes/{node_name}/lxc') or []
        vm_count = len(resources)
        has_vms = vm_count > 0
    except:
        # Node might be offline, which is fine
        pass
    
    # Determine blockers
    blockers = []
    if not in_maintenance:
        blockers.append('Node must be in maintenance mode first')
    if not maintenance_complete and in_maintenance:
        blockers.append('Maintenance/evacuation must be complete')
    if not is_offline:
        blockers.append('Node must be offline (shutdown) before removal')
    if has_vms and not is_offline:
        blockers.append(f'Node still has {vm_count} VM(s)/Container(s)')
    
    can_remove = len(blockers) == 0
    
    return jsonify({
        'can_remove': can_remove,
        'in_maintenance': in_maintenance,
        'maintenance_complete': maintenance_complete,
        'is_offline': is_offline,
        'has_vms': has_vms,
        'vm_count': vm_count,
        'blockers': blockers
    })


@app.route('/api/clusters/<cluster_id>/nodes/<node_name>/cluster-membership', methods=['DELETE'])
@require_auth(perms=['cluster.admin'])
def remove_node_from_cluster(cluster_id, node_name):
    """Remove a node from the Proxmox cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    paramiko = get_paramiko()
    if not paramiko:
        return jsonify({'error': 'SSH not available. Install paramiko: pip install paramiko'}), 500
    
    mgr = cluster_managers[cluster_id]
    data = request.get_json() or {}
    
    if not data.get('confirm'):
        return jsonify({'success': False, 'error': 'Confirmation required'}), 400
    
    # Double-check the node can be removed
    in_maintenance = node_name in maintenance_tasks
    if not in_maintenance:
        return jsonify({'success': False, 'error': 'Node must be in maintenance mode'}), 400
    
    try:
        # Get cluster credentials for SSH
        cluster_config = mgr.config
        ssh_user = getattr(cluster_config, 'ssh_user', None) or cluster_config.user.split('@')[0]
        ssh_password = getattr(cluster_config, 'ssh_password', None) or cluster_config.password
        
        # Find an online node to execute the removal from
        nodes = mgr.api_request('GET', '/nodes') or []
        online_node = None
        online_node_ip = None
        
        for node in nodes:
            if node.get('node') != node_name and node.get('status') == 'online':
                online_node = node.get('node')
                # Use cluster host as IP
                online_node_ip = cluster_config.host
                break
        
        if not online_node:
            return jsonify({'success': False, 'error': 'No online node found to execute removal'}), 500
        
        # Connect to an online node via SSH
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(online_node_ip, port=22, username=ssh_user, password=ssh_password, timeout=30)
        
        # Execute pvecm delnode command
        cmd = f'pvecm delnode {node_name}'
        stdin, stdout, stderr = ssh.exec_command(cmd, timeout=60)
        
        exit_code = stdout.channel.recv_exit_status()
        stdout_text = stdout.read().decode('utf-8', errors='ignore')
        stderr_text = stderr.read().decode('utf-8', errors='ignore')
        
        ssh.close()
        
        if exit_code != 0:
            error_msg = stderr_text or stdout_text or 'Unknown error'
            return jsonify({'success': False, 'error': f'Failed to remove node: {error_msg}'}), 500
        
        # Clean up maintenance task
        if node_name in maintenance_tasks:
            del maintenance_tasks[node_name]
        
        # MK: Clean up excluded_nodes - remove the deleted node
        excluded = getattr(mgr.config, 'excluded_nodes', []) or []
        if node_name in excluded:
            excluded.remove(node_name)
            mgr.config.excluded_nodes = excluded
            logging.info(f"Removed {node_name} from excluded_nodes")
        
        # MK: Clean up fallback_hosts - remove IPs of deleted node
        fallback = getattr(mgr.config, 'fallback_hosts', []) or []
        node_ip = mgr._get_node_ip(node_name) if hasattr(mgr, '_get_node_ip') else None
        if node_ip and node_ip in fallback:
            fallback.remove(node_ip)
            mgr.config.fallback_hosts = fallback
            logging.info(f"Removed {node_ip} from fallback_hosts")
        
        # Save changes to database
        save_config()
        
        # Log the action
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'cluster.node_removed', f"Node {node_name} removed from cluster", cluster=mgr.config.name)
        
        # Broadcast the change
        broadcast_action('node_removed', 'cluster', node_name, {}, cluster_id, user)
        
        return jsonify({
            'success': True,
            'message': f'Node {node_name} has been removed from the cluster'
        })
        
    except paramiko.AuthenticationException:
        return jsonify({'success': False, 'error': 'SSH authentication failed. Check cluster credentials.'}), 401
    except Exception as e:
        logging.error(f"Error removing node from cluster: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# Node Action API (reboot/shutdown)
@app.route('/api/clusters/<cluster_id>/nodes/<node_name>/action/<action>', methods=['POST'])
@require_auth(perms=['node.reboot'])
def node_action_api(cluster_id, node_name, action):
    """Perform action on node (reboot, shutdown) - requires maintenance mode"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    if action not in ['reboot', 'shutdown']:
        return jsonify({'error': f'Invalid action: {action}. Valid: reboot, shutdown'}), 400
    
    paramiko = get_paramiko()
    if not paramiko:
        return jsonify({'error': 'SSH not available. Install paramiko: pip install paramiko'}), 500
    
    mgr = cluster_managers[cluster_id]
    
    # check node is in maintenance
    if node_name not in mgr.nodes_in_maintenance:
        return jsonify({'error': f'Node {node_name} is not in maintenance mode. Please enable maintenance mode first.'}), 400
    
    maintenance_task = mgr.nodes_in_maintenance[node_name]
    if maintenance_task.status not in ['completed', 'completed_with_errors']:
        return jsonify({'error': 'Evacuation still in progress. Please wait.'}), 400
    
    user = getattr(request, 'session', {}).get('user', 'system')
    
    try:
        node_ip = mgr._get_node_ip(node_name)
        if not node_ip:
            return jsonify({'error': f'Could not determine IP for {node_name}'}), 500
        
        ssh = mgr._ssh_connect(node_ip)
        if not ssh:
            if not getattr(mgr.config, 'ssh_key', ''):
                return jsonify({'error': 'SSH connection failed. This node may require SSH key authentication.'}), 500
            return jsonify({'error': 'SSH connection failed.'}), 500
        
        try:
            # Check if we're already root (common on Proxmox)
            stdin, stdout, stderr = ssh.exec_command('id -u')
            uid = stdout.read().decode().strip()
            is_root = (uid == '0')
            
            # Always use PTY for reliable execution
            transport = ssh.get_transport()
            channel = transport.open_session()
            channel.get_pty()
            channel.settimeout(10)
            
            # Use shutdown commands which are more reliable
            if is_root:
                if action == 'reboot':
                    channel.exec_command('shutdown -r now')
                else:
                    channel.exec_command('shutdown -h now')
            else:
                if action == 'reboot':
                    channel.exec_command('sudo shutdown -r now')
                else:
                    channel.exec_command('sudo shutdown -h now')
            
            # Wait briefly for command to be sent
            time.sleep(2)
            
            # Try to read any output
            try:
                output = channel.recv(1024).decode()
                logging.info(f"Node {action} output: {output}")
            except:
                pass
            
            channel.close()
            ssh.close()
            
            # Audit log
            log_audit(user, f'node.{action}', f"Node {node_name} {action} initiated")
            
            # Broadcast to all clients
            broadcast_action(f'node_{action}', 'node', node_name, {}, cluster_id, user)
            
            return jsonify({
                'success': True,
                'message': f'Node {node_name} {action} initiated'
            })
            
        except Exception as e:
            ssh.close()
            logging.error(f"Error executing {action} on {node_name}: {e}")
            return jsonify({'error': str(e)}), 500
            
    except Exception as e:
        logging.error(f"Node action error: {e}")
        return jsonify({'error': str(e)}), 500


# Node Update API Routes
@app.route('/api/clusters/<cluster_id>/nodes/<node_name>/update', methods=['POST'])
@require_auth(perms=['node.update'])
def start_node_update(cluster_id, node_name):
    """Start updating a node (must be in maintenance mode unless force=true)"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    paramiko = get_paramiko()
    if not paramiko:
        return jsonify({'error': 'SSH nicht verfuegbar. Bitte installiere paramiko'}), 500
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    reboot = data.get('reboot', True)
    force = data.get('force', False)
    
    # check maintenance mode (unless force)
    if not force:
        if node_name not in mgr.nodes_in_maintenance:
            return jsonify({'error': f'Node {node_name} ist nicht im Wartungsmodus.'}), 400
        
        maintenance_task = mgr.nodes_in_maintenance[node_name]
        if maintenance_task.status not in ['completed', 'completed_with_errors']:
            return jsonify({'error': f'Evacuation in progress.'}), 400
    
    task = mgr.start_node_update(node_name, reboot, force)
    
    if task:
        usr = getattr(request, 'session', {}).get('user', 'system')
        mode = "(forced)" if force else "(maintenance)"
        log_audit(usr, 'node.update_started', f"Node {node_name} update started {mode}", cluster=mgr.config.name)
        return jsonify({
            'success': True,
            'message': f'Update started for {node_name}',
            'task': task.to_dict()
        })
    else:
        return jsonify({'error': 'Update konnte nicht gestartet werden'}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node_name>/update', methods=['GET'])
@require_auth(perms=['node.view'])
def get_update_status(cluster_id, node_name):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    status = mgr.get_update_status(node_name)
    
    return jsonify(status if status else {'is_updating': False})


@app.route('/api/clusters/<cluster_id>/nodes/<node_name>/update', methods=['DELETE'])
@require_auth(perms=['node.update'])
def clear_update_status_api(cluster_id, node_name):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    success = mgr.clear_update_status(node_name)
    
    if success:
        return jsonify({'message': f'Update status cleared for {node_name}'})
    return jsonify({'error': f'No completed update found for {node_name}'}), 400


# VM Control API Routes
@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/<action>', methods=['POST'])
@require_auth()
def vm_action_api(cluster_id, node, vm_type, vmid, action):
    """Perform action on VM (start, stop, shutdown, reboot, reset, suspend, resume)
    
    NS: Updated Dec 2025 - Now checks VM-specific ACLs
    """
    logging.info(f"[VM-ACTION] Received: {action} on {vm_type}/{vmid} at {node}, cluster={cluster_id}")
    
    # check cluster access
    ok, err = check_cluster_access(cluster_id)
    if not ok:
        return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    valid_actions = ['start', 'stop', 'shutdown', 'reboot', 'reset', 'suspend', 'resume']
    if action not in valid_actions:
        return jsonify({'error': f'Invalid action. Valid actions: {valid_actions}'}), 400
    
    # check permission for action - now uses VM ACLs
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']  # MK: make sure username is set
    
    perm_map = {
        'start': 'vm.start',
        'stop': 'vm.stop',
        'shutdown': 'vm.stop',
        'reboot': 'vm.restart',
        'reset': 'vm.restart',
        'suspend': 'vm.stop',
        'resume': 'vm.start'
    }
    required_perm = perm_map.get(action, 'vm.start')
    
    # LW: Use VM-specific ACL check instead of general permission
    # MK: Added vm_type for pool permission check
    if not user_can_access_vm(user, cluster_id, vmid, required_perm, vm_type):
        logging.warning(f"[VM-ACTION] Permission denied for {request.session['user']}: {required_perm} on VM {vmid}")
        return jsonify({'error': f'Permission denied: {required_perm}'}), 403
    
    # Check for force parameter (for force stop) - handle empty body gracefully
    force = False
    try:
        if request.is_json and request.data:
            data = request.get_json(silent=True) or {}
            force = data.get('force', False)
            logging.info(f"[VM-ACTION] Force parameter: {force}, raw data: {request.data}")
    except Exception as e:
        logging.warning(f"[VM-ACTION] Error parsing body: {e}")
    
    logging.info(f"[VM-ACTION] Executing {action} with force={force}")
    manager = cluster_managers[cluster_id]
    result = manager.vm_action(node, vmid, vm_type, action, force=force)
    
    if result['success']:
        # Audit log
        usr = getattr(request, 'session', {}).get('user', 'system')
        action_map = {'start': 'vm.started', 'stop': 'vm.stopped', 'shutdown': 'vm.stopped', 
                      'reboot': 'vm.restarted', 'reset': 'vm.restarted', 'suspend': 'vm.suspended', 'resume': 'vm.resumed'}
        log_audit(usr, action_map.get(action, f'vm.{action}'), f"{vm_type.upper()} {vmid} on {node} - {action}" + (" (force)" if force else ""), cluster=manager.config.name)
        
        # Broadcast action to all clients for real-time updates
        broadcast_action(action, vm_type, str(vmid), {'node': node, 'force': force}, cluster_id, usr)
        
        # NS: Push immediate resource update for faster UI feedback
        # Proxmox needs a moment to update status, so we do a quick delayed refresh
        def delayed_resource_push():
            import time
            time.sleep(0.5)  # Give Proxmox 500ms to update status
            try:
                resources = manager.get_all_resources()
                if resources:
                    broadcast_sse('resources', resources, cluster_id)
                tasks = manager.get_tasks(limit=50)
                if tasks:
                    broadcast_sse('tasks', tasks, cluster_id)
            except:
                pass  # Silently fail - next loop will catch it anyway
        
        threading.Thread(target=delayed_resource_push, daemon=True).start()
        
        return jsonify({'message': f'{action} successful for VM {vmid}', 'data': result.get('data')})
    else:
        # Return 400 for client errors (like LXC reset), 500 for server errors
        error_msg = result.get('error', 'Unknown error')
        status_code = 400 if 'not supported' in error_msg.lower() else 500
        return jsonify({'error': error_msg}), status_code


@app.route('/api/clusters/<cluster_id>/nextid', methods=['GET'])
@require_auth(perms=['vm.view'])
def get_next_vmid_api(cluster_id):
    # check cluster access
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    result = mgr.get_next_vmid()
    
    if result['success']:
        return jsonify({'vmid': result['vmid']})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/clone', methods=['POST'])
@require_auth(perms=['vm.clone'])
def clone_vm_api(cluster_id, node, vm_type, vmid):
    """Clone a VM or container"""
    # tenant check
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # MK: Check pool permission for vm.clone
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    if not user_can_access_vm(user, cluster_id, vmid, 'vm.clone', vm_type):
        return jsonify({'error': 'Permission denied: vm.clone'}), 403
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    newid = data.get('newid')
    if not newid:
        # Get next available VMID
        next_result = manager.get_next_vmid()
        if next_result['success']:
            newid = next_result['vmid']
        else:
            return jsonify({'error': 'Could not get next VMID'}), 500
    
    result = manager.clone_vm(
        node=node,
        vmid=vmid,
        vm_type=vm_type,
        newid=int(newid),
        name=data.get('name'),
        full=data.get('full', True),
        target_node=data.get('target_node'),
        target_storage=data.get('target_storage'),
        description=data.get('description')
    )
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'vm.cloned', f"{vm_type.upper()} {vmid} cloned to {newid}" + (f" as '{data.get('name')}'" if data.get('name') else ""), cluster=manager.config.name)
        return jsonify({
            'message': f'Clone gestartet: {vmid} -> {newid}',
            'newid': newid,
            'data': result.get('data')
        })
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/console', methods=['GET'])
@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/vnc', methods=['GET'])
@require_auth()
def get_console_ticket(cluster_id, node, vm_type, vmid):
    """Get VNC console ticket for VM - NS: Now uses VM ACLs"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # MK: Check VM-specific access
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    
    # MK: Added vm_type for pool permission check
    if not user_can_access_vm(user, cluster_id, vmid, 'vm.console', vm_type):
        return jsonify({'error': 'Permission denied: vm.console'}), 403
    
    mgr = cluster_managers[cluster_id]
    result = mgr.get_vnc_ticket(node, vmid, vm_type)
    
    if result['success']:
        return jsonify(result)
    return jsonify({'error': result.get('error', 'Failed')}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/shell', methods=['POST'])
@require_auth(perms=['node.shell'])
def get_node_shell_ticket(cluster_id, node):
    """Get shell ticket for node - requires node.shell permission"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    result = mgr.get_node_shell_ticket(node)
    
    # audit - shell access is sensitive
    usr = getattr(request, 'session', {}).get('user', 'system')
    log_audit(usr, 'node.shell_access', f"Shell access requested for node {node}", cluster=mgr.config.name)
    
    if result['success']:
        return jsonify(result)
    else:
        return jsonify({'error': result['error']}), 500


# VM Config API Routes
@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/config', methods=['GET'])
@require_auth(perms=['vm.view'])
def get_vm_config_api(cluster_id, node, vm_type, vmid):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    result = mgr.get_vm_config(node, vmid, vm_type)
    
    if result['success']:
        return jsonify(result['config'])
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/lock', methods=['GET'])
@require_auth(perms=['vm.view'])
def get_vm_lock_status_api(cluster_id, node, vm_type, vmid):
    """Get lock status of a VM/CT"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    try:
        result = mgr.get_vm_lock_status(node, vmid, vm_type)
        
        if result.get('success'):
            return jsonify({
                'locked': result.get('locked', False),
                'lock_reason': result.get('lock_reason'),
                'lock_description': result.get('lock_description'),
                'unlock_command': f"qm unlock {vmid}" if vm_type == 'qemu' else f"pct unlock {vmid}"
            })
        else:
            # MK: Return not-locked instead of error for better UX
            # The VM config might not be accessible but that doesn't mean it's locked
            logging.warning(f"Could not get lock status for {vm_type}/{vmid}: {result.get('error')}")
            return jsonify({
                'locked': False,
                'lock_reason': None,
                'lock_description': None,
                'unlock_command': None,
                'note': 'Could not determine lock status'
            })
    except Exception as e:
        logging.error(f"Error getting lock status for {vm_type}/{vmid}: {e}")
        return jsonify({
            'locked': False,
            'lock_reason': None,
            'lock_description': None,
            'unlock_command': None
        })


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/unlock', methods=['POST'])
@require_auth(perms=['vm.power'])
def unlock_vm_api(cluster_id, node, vm_type, vmid):
    """Unlock a VM/CT - use with caution!"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    result = mgr.unlock_vm(node, vmid, vm_type)
    
    if result['success']:
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'vm.unlock', f"Unlocked {vm_type}/{vmid} on {node} (was: {result.get('lock_reason', 'unknown')})", cluster=mgr.config.name)
        return jsonify({
            'message': result['message'],
            'was_locked': result.get('was_locked', False),
            'lock_reason': result.get('lock_reason')
        })
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/rrd/<timeframe>', methods=['GET'])
@require_auth(perms=['vm.view'])
def get_vm_rrd_api(cluster_id, node, vm_type, vmid, timeframe):
    """Get VM RRD metrics data for graphs
    
    Timeframes: hour, day, week, month, year
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    valid_timeframes = ['hour', 'day', 'week', 'month', 'year']
    if timeframe not in valid_timeframes:
        return jsonify({'error': f'Invalid timeframe. Valid: {valid_timeframes}'}), 400
    
    mgr = cluster_managers[cluster_id]
    result = mgr.get_vm_rrd(node, vmid, vm_type, timeframe)
    
    if result['success']:
        return jsonify(result['data'])
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/config', methods=['PUT'])
@require_auth(perms=['vm.config'])
def update_vm_config_api(cluster_id, node, vm_type, vmid):
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # MK: Check pool permission for vm.config
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    if not user_can_access_vm(user, cluster_id, vmid, 'vm.config', vm_type):
        return jsonify({'error': 'Permission denied: vm.config'}), 403
    
    manager = cluster_managers[cluster_id]
    config_updates = request.json or {}
    
    result = manager.update_vm_config(node, vmid, vm_type, config_updates)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        changes = ', '.join([f"{k}={v}" for k, v in config_updates.items()][:5])
        log_audit(user, 'vm.config_changed', f"{vm_type.upper()} {vmid} config updated: {changes}", cluster=manager.config.name)
        return jsonify({'message': result['message']})
    else:
        return jsonify({'error': result['error']}), 500


# =====================================================
# PCI / USB / SERIAL PASSTHROUGH API
# =====================================================

@app.route('/api/clusters/<cluster_id>/nodes/<node>/hardware/pci', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_pci_devices(cluster_id, node):
    """Get available PCI devices on a node for passthrough"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/nodes/{node}/hardware/pci"
        response = manager._create_session().get(url, timeout=10)
        
        if response.status_code == 200:
            devices = response.json().get('data', [])
            # Enhance device info with friendly names
            for device in devices:
                device['display_name'] = f"{device.get('vendor_name', 'Unknown')} {device.get('device_name', device.get('id', 'Unknown'))}"
                device['passthrough_capable'] = device.get('iommugroup', -1) >= 0
            return jsonify(devices)
        return jsonify([])
    except Exception as e:
        logging.error(f"Error getting PCI devices: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/hardware/usb', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_usb_devices(cluster_id, node):
    """Get available USB devices on a node for passthrough"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/nodes/{node}/hardware/usb"
        response = manager._create_session().get(url, timeout=10)
        
        if response.status_code == 200:
            devices = response.json().get('data', [])
            # Add display name
            for device in devices:
                vendor = device.get('manufacturer', device.get('vendid', 'Unknown'))
                product = device.get('product', device.get('prodid', 'Unknown'))
                device['display_name'] = f"{vendor} - {product}"
            return jsonify(devices)
        return jsonify([])
    except Exception as e:
        logging.error(f"Error getting USB devices: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/qemu/<int:vmid>/passthrough', methods=['GET'])
@require_auth(perms=['vm.view'])
def get_vm_passthrough_devices(cluster_id, node, vmid):
    """Get current passthrough devices configured for a VM"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    try:
        host = manager.current_host or manager.config.host
        url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
        r = manager._create_session().get(url, timeout=10)
        
        if r.status_code != 200:
            return jsonify({'error': 'Failed: VM config'}), 500
        
        config = r.json().get('data', {})
        
        # extract passthrough devices
        passthrough = {
            'pci': [],
            'usb': [],
            'serial': []
        }
        
        for key, value in config.items():
            # PCI devices
            if key.startswith('hostpci'):
                slot = key.replace('hostpci', '')
                passthrough['pci'].append({
                    'slot': slot,
                    'key': key,
                    'value': value,
                    'parsed': _parse_pci_config(value)
                })
            
            # USB devices
            if key.startswith('usb') and key[3:].isdigit():
                slot = key.replace('usb', '')
                passthrough['usb'].append({
                    'slot': slot,
                    'key': key,
                    'value': value,
                    'parsed': _parse_usb_config(value)
                })
            
            # Serial ports
            if key.startswith('serial'):
                slot = key.replace('serial', '')
                passthrough['serial'].append({
                    'slot': slot,
                    'key': key,
                    'value': value
                })
        
        return jsonify(passthrough)
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/qemu/<int:vmid>/passthrough/pci', methods=['POST'])
@require_auth(perms=['vm.config'])
def add_pci_passthrough(cluster_id, node, vmid):
    """Add a PCI device passthrough to a VM"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    data = request.json or {}
    device_id = data.get('device_id')
    
    if not device_id:
        return jsonify({'error': 'device_id required'}), 400
    
    try:
        host = manager.current_host or manager.config.host
        
        # Find next available hostpci slot
        config_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
        config_response = manager._create_session().get(config_url, timeout=10)
        config = config_response.json().get('data', {}) if config_response.status_code == 200 else {}
        
        # Find free slot (0-15)
        used_slots = [int(k.replace('hostpci', '')) for k in config.keys() if k.startswith('hostpci')]
        next_slot = 0
        while next_slot in used_slots and next_slot < 16:
            next_slot += 1
        
        if next_slot >= 16:
            return jsonify({'error': 'No free PCI slots available'}), 400
        
        # Build PCI passthrough config
        pci_config = device_id
        if data.get('pcie'):
            pci_config += ',pcie=1'
        if data.get('rombar') is False:
            pci_config += ',rombar=0'
        if data.get('x-vga'):
            pci_config += ',x-vga=1'
        
        # Update VM config
        update_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
        update_data = {f'hostpci{next_slot}': pci_config}
        response = manager._create_session().put(update_url, data=update_data, timeout=15)
        
        if response.status_code == 200:
            user = getattr(request, 'session', {}).get('user', 'system')
            log_audit(user, 'vm.pci_added', f"VM {vmid}: Added PCI device {device_id} at slot {next_slot}", cluster=manager.config.name)
            return jsonify({'message': f'PCI device added at hostpci{next_slot}', 'slot': next_slot})
        else:
            return jsonify({'error': response.text}), 500
            
    except Exception as e:
        logging.error(f"Error adding PCI passthrough: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/qemu/<int:vmid>/passthrough/usb', methods=['POST'])
@require_auth(perms=['vm.config'])
def add_usb_passthrough(cluster_id, node, vmid):
    """Add a USB device passthrough to a VM"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    data = request.json or {}
    
    # USB can be specified by vendor:product ID or by host bus/port
    vendor_id = data.get('vendorid')
    product_id = data.get('productid')
    host_bus = data.get('hostbus')
    host_port = data.get('hostport')
    
    if not ((vendor_id and product_id) or (host_bus and host_port)):
        return jsonify({'error': 'Either vendorid+productid or hostbus+hostport required'}), 400
    
    try:
        host = manager.current_host or manager.config.host
        
        # Find next available usb slot
        config_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
        config_response = manager._create_session().get(config_url, timeout=10)
        config = config_response.json().get('data', {}) if config_response.status_code == 200 else {}
        
        # Find free slot (0-4)
        used_slots = [int(k.replace('usb', '')) for k in config.keys() if k.startswith('usb') and k[3:].isdigit()]
        next_slot = 0
        while next_slot in used_slots and next_slot < 5:
            next_slot += 1
        
        if next_slot >= 5:
            return jsonify({'error': 'No free USB slots available (max 5)'}), 400
        
        # Build USB config
        if vendor_id and product_id:
            usb_config = f"host={vendor_id}:{product_id}"
        else:
            usb_config = f"host={host_bus}-{host_port}"
        
        if data.get('usb3'):
            usb_config += ',usb3=1'
        
        # Update VM config
        update_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
        update_data = {f'usb{next_slot}': usb_config}
        response = manager._create_session().put(update_url, data=update_data, timeout=15)
        
        if response.status_code == 200:
            user = getattr(request, 'session', {}).get('user', 'system')
            log_audit(user, 'vm.usb_added', f"VM {vmid}: Added USB device at slot {next_slot}", cluster=manager.config.name)
            return jsonify({'message': f'USB device added at usb{next_slot}', 'slot': next_slot})
        else:
            return jsonify({'error': response.text}), 500
            
    except Exception as e:
        logging.error(f"Error adding USB passthrough: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/qemu/<int:vmid>/passthrough/serial', methods=['POST'])
@require_auth(perms=['vm.config'])
def add_serial_port(cluster_id, node, vmid):
    """Add a serial port to a VM"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    data = request.json or {}
    serial_type = data.get('type', 'socket')  # socket, pty, or /dev/xxx
    
    try:
        host = manager.current_host or manager.config.host
        
        # Find next available serial slot
        config_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
        config_response = manager._create_session().get(config_url, timeout=10)
        config = config_response.json().get('data', {}) if config_response.status_code == 200 else {}
        
        # Find free slot (0-3)
        used_slots = [int(k.replace('serial', '')) for k in config.keys() if k.startswith('serial')]
        next_slot = 0
        while next_slot in used_slots and next_slot < 4:
            next_slot += 1
        
        if next_slot >= 4:
            return jsonify({'error': 'No free serial slots available (max 4)'}), 400
        
        # Update VM config
        update_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
        update_data = {f'serial{next_slot}': serial_type}
        response = manager._create_session().put(update_url, data=update_data, timeout=15)
        
        if response.status_code == 200:
            user = getattr(request, 'session', {}).get('user', 'system')
            log_audit(user, 'vm.serial_added', f"VM {vmid}: Added serial port at slot {next_slot}", cluster=manager.config.name)
            return jsonify({'message': f'Serial port added at serial{next_slot}', 'slot': next_slot})
        else:
            return jsonify({'error': response.text}), 500
            
    except Exception as e:
        logging.error(f"Error adding serial port: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/qemu/<int:vmid>/passthrough/<device_type>/<key>', methods=['DELETE'])
@require_auth(perms=['vm.config'])
def remove_passthrough_device(cluster_id, node, vmid, device_type, key):
    """Remove a passthrough device from a VM"""
    manager, error = get_connected_manager(cluster_id)
    if error:
        return error
    
    # Validate device type and key
    valid_prefixes = {'pci': 'hostpci', 'usb': 'usb', 'serial': 'serial'}
    if device_type not in valid_prefixes:
        return jsonify({'error': 'Invalid device type'}), 400
    
    # Key should be like hostpci0, usb1, serial0
    expected_prefix = valid_prefixes[device_type]
    if not key.startswith(expected_prefix):
        return jsonify({'error': f'Invalid key for {device_type}'}), 400
    
    try:
        host = manager.current_host or manager.config.host
        
        # Delete by setting to empty/delete
        update_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/config"
        update_data = {'delete': key}
        response = manager._create_session().put(update_url, data=update_data, timeout=15)
        
        if response.status_code == 200:
            user = getattr(request, 'session', {}).get('user', 'system')
            log_audit(user, f'vm.{device_type}_removed', f"VM {vmid}: Removed {key}", cluster=manager.config.name)
            return jsonify({'message': f'Device {key} removed'})
        else:
            return jsonify({'error': response.text}), 500
            
    except Exception as e:
        logging.error(f"Error removing passthrough device: {e}")
        return jsonify({'error': str(e)}), 500


def _parse_pci_config(config_str):
    """Parse PCI passthrough config string"""
    result = {'device': None, 'options': {}}
    if not config_str:
        return result
    
    parts = config_str.split(',')
    result['device'] = parts[0]
    
    for part in parts[1:]:
        if '=' in part:
            key, value = part.split('=', 1)
            result['options'][key] = value
    
    return result


def _parse_usb_config(config_str):
    """Parse USB passthrough config string"""
    result = {'host': None, 'options': {}}
    if not config_str:
        return result
    
    parts = config_str.split(',')
    for part in parts:
        if '=' in part:
            key, value = part.split('=', 1)
            if key == 'host':
                result['host'] = value
            else:
                result['options'][key] = value
    
    return result


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/resize', methods=['PUT'])
@require_auth(perms=['vm.config'])
def resize_vm_disk_api(cluster_id, node, vm_type, vmid):
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    disk = data.get('disk')
    size = data.get('size')
    
    if not disk or not size:
        return jsonify({'error': 'disk and size required'}), 400
    
    result = manager.resize_vm_disk(node, vmid, vm_type, disk, size)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'vm.disk_resized', f"{vm_type.upper()} {vmid} disk {disk} resized to {size}", cluster=manager.config.name)
        return jsonify({'message': result['message']})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/storage', methods=['GET'])
@require_auth(perms=['storage.view'])
def get_storage_list_api(cluster_id, node):
    """Get available storage on a node"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    storage = manager.get_storage_list(node)
    return jsonify(storage)


@app.route('/api/clusters/<cluster_id>/nodes/<node>/networks', methods=['GET'])
@require_auth(perms=['node.view'])
def get_network_list_api(cluster_id, node):
    """Get available networks on a node"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    networks = manager.get_network_list(node)
    return jsonify(networks)


@app.route('/api/clusters/<cluster_id>/nodes/<node>/isos', methods=['GET'])
@require_auth(perms=['storage.view'])
def get_iso_list_api(cluster_id, node):
    """Get available ISO images on a node"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    storage = request.args.get('storage')
    isos = manager.get_iso_list(node, storage)
    return jsonify(isos)


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/disks', methods=['POST'])
@require_auth(perms=['vm.config'])
def add_disk_api(cluster_id, node, vm_type, vmid):
    """Add a disk to VM or container"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    disk_config = request.json or {}
    
    result = manager.add_disk(node, vmid, vm_type, disk_config)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'vm.disk_added', f"{vm_type.upper()} {vmid} - disk added: {disk_config.get('size', 'unknown')}GB on {disk_config.get('storage', 'default')}", cluster=manager.config.name)
        return jsonify({'message': result['message']})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/disks/<disk_id>', methods=['DELETE'])
@require_auth(perms=['vm.config'])
def remove_disk_api(cluster_id, node, vm_type, vmid, disk_id):
    """Remove disk from VM - NS: Now also cleans up boot order"""
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    delete_data = request.args.get('delete_data', 'false').lower() == 'true'
    
    # MK: First get current config to check boot order
    config_result = manager.get_vm_config(node, vmid, vm_type)
    old_boot = ''
    if config_result.get('success'):
        # MK: Boot order is in raw config, not parsed
        raw_config = config_result.get('config', {}).get('raw', {})
        old_boot = raw_config.get('boot', '')
    
    result = manager.remove_disk(node, vmid, vm_type, disk_id, delete_data)
    
    if result['success']:
        # LW: Auto-update boot order if it contains the removed disk
        if old_boot and disk_id in old_boot:
            try:
                # Parse boot order (format: order=scsi0;ide2;net0)
                if 'order=' in old_boot:
                    parts = old_boot.split('order=')[1].split(';')
                    new_parts = [p for p in parts if p != disk_id and p.strip()]
                    if new_parts:
                        new_boot = 'order=' + ';'.join(new_parts)
                    else:
                        new_boot = ''  # no devices left
                    
                    # Update VM config with new boot order
                    if new_boot != old_boot:
                        manager.update_vm_config(node, vmid, vm_type, {'boot': new_boot})
                        logging.info(f"[DISK] Auto-updated boot order after removing {disk_id}: {new_boot}")
            except Exception as e:
                logging.warning(f"[DISK] Failed to auto-update boot order: {e}")
        
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'vm.disk_removed', f"{vm_type.upper()} {vmid} - disk {disk_id} removed" + (" (data deleted)" if delete_data else ""), cluster=manager.config.name)
        return jsonify({'message': result['message']})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/disks/<disk_id>/move', methods=['POST'])
@require_auth(perms=['vm.config'])
def move_disk_api(cluster_id, node, vm_type, vmid, disk_id):
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    target_storage = data.get('storage')
    delete_original = data.get('delete', True)
    
    if not target_storage:
        return jsonify({'error': 'Target storage required'}), 400
    
    result = manager.move_disk(node, vmid, vm_type, disk_id, target_storage, delete_original)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'vm.disk_moved', f"{vm_type.upper()} {vmid} - disk {disk_id} moved to {target_storage}", cluster=manager.config.name)
        return jsonify({'message': result['message'], 'task': result.get('task')})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/qemu/<int:vmid>/cdrom', methods=['PUT'])
@require_auth(perms=['vm.config'])
def set_cdrom_api(cluster_id, node, vmid):
    """Set or eject CD-ROM"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    iso_path = data.get('iso')  # None to eject
    drive = data.get('drive', 'ide2')
    
    result = manager.set_cdrom(node, vmid, iso_path, drive)
    
    if result['success']:
        return jsonify({'message': result['message']})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/networks', methods=['POST'])
@require_auth(perms=['vm.config'])
def add_network_api(cluster_id, node, vm_type, vmid):
    """Add a network interface"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    net_config = request.json or {}
    
    result = manager.add_network(node, vmid, vm_type, net_config)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'vm.network_added', f"{vm_type.upper()} {vmid} - network added: bridge={net_config.get('bridge', 'default')}", cluster=manager.config.name)
        return jsonify({'message': result['message']})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/networks/<net_id>', methods=['PUT'])
@require_auth(perms=['vm.config'])
def update_network_api(cluster_id, node, vm_type, vmid, net_id):
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    net_config = request.json or {}
    
    result = manager.update_network(node, vmid, vm_type, net_id, net_config)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'vm.network_updated', f"{vm_type.upper()} {vmid} - network {net_id} updated", cluster=manager.config.name)
        return jsonify({'message': result['message']})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/networks/<net_id>', methods=['DELETE'])
@require_auth(perms=['vm.config'])
def remove_network_api(cluster_id, node, vm_type, vmid, net_id):
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    
    result = manager.remove_network(node, vmid, vm_type, net_id)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'vm.network_removed', f"{vm_type.upper()} {vmid} - network {net_id} removed", cluster=manager.config.name)
        return jsonify({'message': result['message']})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/networks/<net_id>/link', methods=['PUT'])
@require_auth(perms=['vm.config'])
def toggle_network_link_api(cluster_id, node, vm_type, vmid, net_id):
    """Toggle network link_down state - simulates cable unplug
    
    NS: This is a hot-pluggable operation for QEMU VMs (no reboot needed)
    LW: Very useful for testing network failover scenarios
    """
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # Only QEMU supports link_down toggle
    if vm_type != 'qemu':
        return jsonify({'error': 'Network disconnect only supported for QEMU VMs'}), 400
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    link_down = data.get('link_down', False)
    
    result = manager.toggle_network_link(node, vmid, net_id, link_down)
    
    if result['success']:
        user = getattr(request, 'session', {}).get('user', 'system')
        action = 'disconnected' if link_down else 'connected'
        log_audit(user, 'vm.network_link_toggle', f"QEMU {vmid} - network {net_id} {action}", cluster=manager.config.name)
        return jsonify({'message': result['message']})
    else:
        return jsonify({'error': result['error']}), 500


# ==================== SNAPSHOT API ROUTES ====================

@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/snapshot-capability', methods=['GET'])
@require_auth(perms=['vm.view'])
def check_snapshot_capability_api(cluster_id, node, vm_type, vmid):
    """Check if VM/CT can create snapshots and why not"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    result = manager.check_snapshot_capability(node, vmid, vm_type)
    return jsonify(result)


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/snapshots', methods=['GET'])
@require_auth(perms=['vm.view'])
def get_snapshots_api(cluster_id, node, vm_type, vmid):
    """Get list of snapshots for a VM/CT"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    snapshots = manager.get_snapshots(node, vmid, vm_type)
    return jsonify(snapshots)


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/snapshots', methods=['POST'])
@require_auth(perms=['vm.snapshot'])
def create_snapshot_api(cluster_id, node, vm_type, vmid):
    # tenant check
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # MK: Check pool permission for vm.snapshot
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    if not user_can_access_vm(user, cluster_id, vmid, 'vm.snapshot', vm_type):
        return jsonify({'error': 'Permission denied: vm.snapshot'}), 403
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    
    snapname = data.get('snapname', f'snap_{int(time.time())}')
    description = data.get('description', '')
    vmstate = data.get('vmstate', False)
    
    result = mgr.create_snapshot(node, vmid, vm_type, snapname, description, vmstate)
    
    if result['success']:
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'snapshot.created', f"{vm_type.upper()} {vmid} - snapshot '{snapname}' created" + (" (with RAM)" if vmstate else ""), cluster=mgr.config.name)
        return jsonify({'message': f'Snapshot {snapname} erstellt', 'task': result.get('task')})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/snapshots/<snapname>', methods=['DELETE'])
@require_auth(perms=['vm.snapshot'])
def delete_snapshot_api(cluster_id, node, vm_type, vmid, snapname):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # MK: Check pool permission for vm.snapshot
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    if not user_can_access_vm(user, cluster_id, vmid, 'vm.snapshot', vm_type):
        return jsonify({'error': 'Permission denied: vm.snapshot'}), 403
    
    mgr = cluster_managers[cluster_id]
    result = mgr.delete_snapshot(node, vmid, vm_type, snapname)
    
    if result['success']:
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'snapshot.deleted', f"{vm_type.upper()} {vmid} - snapshot '{snapname}' deleted", cluster=mgr.config.name)
        return jsonify({'message': f'Snapshot deleted', 'task': result.get('task')})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/snapshots/<snapname>/rollback', methods=['POST'])
@require_auth(perms=['vm.snapshot'])
def rollback_snapshot_api(cluster_id, node, vm_type, vmid, snapname):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # MK: Check pool permission for vm.snapshot
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    if not user_can_access_vm(user, cluster_id, vmid, 'vm.snapshot', vm_type):
        return jsonify({'error': 'Permission denied: vm.snapshot'}), 403
    
    mgr = cluster_managers[cluster_id]
    result = mgr.rollback_snapshot(node, vmid, vm_type, snapname)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'snapshot.restored', f"{vm_type.upper()} {vmid} - rolled back to snapshot '{snapname}'", cluster=mgr.config.name)
        return jsonify({'message': f'Rollback zu {snapname} gestartet', 'task': result.get('task')})
    else:
        return jsonify({'error': result['error']}), 500


# ==================== SNAPSHOT OVERVIEW API @gyptazy ====================

@app.route('/api/snapshots/overview', methods=['GET', 'POST'])
@require_auth(perms=['vm.view'])
def snapshots_overview():
    """Get overview of old snapshots across all clusters or a specific cluster
    
    Returns snapshots older than specified date, sorted by age
    
    LW: Added cluster_id filter - when provided, only shows snapshots from that cluster
    """
    snapshots = []
    user = request.session.get('user', '')
    users_db = load_users()
    user_data = users_db.get(user, {})
    user_data['username'] = user
    cutoff_date = None
    data = request.get_json(silent=True) or {}
    date_compare = data.get("date") or datetime.now(timezone.utc).strftime("%Y-%m-%d")
    filter_limit = data.get("limit", 100)  # LW: Increased default limit
    filter_cluster = data.get("cluster_id")  # MK: Optional cluster filter
    is_admin = user_data.get('role') == ROLE_ADMIN
    user_clusters = user_data.get('clusters', [])

    for cluster_id, mgr in cluster_managers.items():
        if not mgr.is_connected:
            continue

        # LW: Filter by specific cluster if provided
        if filter_cluster and cluster_id != filter_cluster:
            continue

        if not is_admin and user_clusters and cluster_id not in user_clusters:
            continue

        try:
            resources = mgr.get_vm_resources()

            for r in resources:
                vmid = r.get('vmid')
                node = r.get('node')
                vm_name = r.get('name') or ''
                vm_type = r.get('type', 'qemu')

                if not vmid or not node:
                    continue

                manager = cluster_managers[cluster_id]
                snapshots_present = manager.get_snapshots(node, vmid, vm_type)

                for snap in snapshots_present:
                    snap_name = snap.get('name')
                    snap_ts = snap.get('snaptime')

                    # skip invalid + implicit snapshot
                    if not snap_name or not snap_ts or snap_name == 'current':
                        continue

                    snap_dt = datetime.fromtimestamp(snap_ts, tz=timezone.utc)
                    now = datetime.now(timezone.utc)
                    age_seconds = int((now - snap_dt).total_seconds())
                    cutoff_date = datetime.strptime(date_compare, "%Y-%m-%d").replace(tzinfo=timezone.utc)

                    if snap_dt >= cutoff_date:
                        continue

                    if age_seconds < 3600:
                        age = f"{age_seconds // 60} min"
                    elif age_seconds < 86400:
                        age = f"{age_seconds // 3600} h"
                    else:
                        age = f"{age_seconds // 86400} days"

                    snapshots.append({
                        "vmid": vmid,
                        "vm_name": vm_name,
                        "vm_type": vm_type,
                        "node": node,
                        "snapshot_name": snap_name,
                        "snapshot_date": snap_dt.strftime('%Y-%m-%d %H:%M'),
                        "age": age,
                        "cluster_id": cluster_id
                    })

        except Exception as e:
            logging.debug(f"Snapshot gathering failed for cluster {cluster_id}: {e}")

    # Sort and filter snapshots
    snapshots.sort(key=lambda s: s["snapshot_date"], reverse=False)
    snapshots = snapshots[:filter_limit]

    return jsonify({
        "snapshots": snapshots
    })


@app.route('/api/snapshots/delete', methods=['POST'])
@require_auth(perms=['vm.view', 'vm.snapshot'])
def snapshots_overview_delete():
    """Delete multiple snapshots at once
    
    Bulk delete for snapshot cleanup
    """
    user = request.session.get('user', '')
    users_db = load_users()
    user_data = users_db.get(user, {})
    user_data['username'] = user
    data = request.get_json(silent=True) or {}
    snapshots = data.get('snapshots', [])
    is_admin = user_data.get('role') == ROLE_ADMIN
    user_clusters = user_data.get('clusters', [])
    
    deleted_count = 0
    errors = []
    result = {'success': False}

    for snapshot in snapshots:
        try:
            cluster_id = snapshot.get('cluster_id')
            node = snapshot.get('node')
            vmid = snapshot.get('vmid')
            snapname = snapshot.get('snapshot_name')
            vm_type = snapshot.get('vm_type', 'qemu')
            
            if cluster_id not in cluster_managers:
                errors.append(f"Cluster {cluster_id} not found")
                continue
                
            mgr = cluster_managers[cluster_id]
            
            if not mgr.is_connected:
                errors.append(f"Cluster {cluster_id} not connected")
                continue

            if not is_admin and user_clusters and cluster_id not in user_clusters:
                errors.append(f"No access to cluster {cluster_id}")
                continue
            
            result = mgr.delete_snapshot(node, vmid, vm_type, snapname)
            
            if result.get('success'):
                deleted_count += 1
                log_audit(user, 'snapshot.deleted', f"{vm_type.upper()} {vmid} - snapshot '{snapname}' deleted", cluster=mgr.config.name)
            else:
                errors.append(f"Failed to delete {snapname}: {result.get('error', 'Unknown error')}")
                
        except Exception as e:
            errors.append(f"Error deleting snapshot: {e}")
            logging.debug(f"Snapshot deletion failed: {e}")

    if deleted_count > 0:
        return jsonify({
            'success': True,
            'message': f'{deleted_count} snapshot(s) deleted',
            'deleted': deleted_count,
            'errors': errors if errors else None
        })
    else:
        return jsonify({
            'success': False,
            'error': 'No snapshots deleted',
            'errors': errors
        }), 500


# ==================== REPLICATION API ROUTES ====================

@app.route('/api/clusters/<cluster_id>/replication', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_replication_jobs_api(cluster_id):
    """Get all replication jobs"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    vmid = request.args.get('vmid', type=int)
    jobs = manager.get_replication_jobs(vmid)
    return jsonify(jobs)


@app.route('/api/clusters/<cluster_id>/replication', methods=['POST'])
@require_auth(perms=['cluster.config'])
def create_replication_job_api(cluster_id):
    """Create a replication job"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    vmid = data.get('vmid')
    target_node = data.get('target')
    schedule = data.get('schedule', '*/15')
    rate = data.get('rate')
    comment = data.get('comment', '')
    
    if not vmid or not target_node:
        return jsonify({'error': 'vmid and target are required'}), 400
    
    result = manager.create_replication_job(vmid, target_node, schedule, rate, comment)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'replication.created', f"VM {vmid} replication to {target_node} (schedule: {schedule})", cluster=manager.config.name)
        return jsonify({'message': 'Replication Job erstellt', 'job_id': result.get('job_id')})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/replication/<job_id>', methods=['DELETE'])
@require_auth(perms=['cluster.config'])
def delete_replication_job_api(cluster_id, job_id):
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    keep = data.get('keep', False)
    force = data.get('force', False)
    
    result = manager.delete_replication_job(job_id, keep, force)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'replication.deleted', f"Replication job {job_id} deleted", cluster=manager.config.name)
        return jsonify({'message': f'Replication Job deleted'})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/replication/<job_id>/run', methods=['POST'])
@require_auth(perms=['cluster.config'])
def run_replication_now_api(cluster_id, job_id):
    """Trigger immediate replication"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    result = manager.run_replication_now(job_id)
    
    if result['success']:
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        log_audit(user, 'replication.triggered', f"Replication job {job_id} manually triggered", cluster=manager.config.name)
        return jsonify({'message': 'Replication gestartet'})
    else:
        return jsonify({'error': result['error']}), 500


@app.route('/api/hardware-options', methods=['GET'])
@require_auth(perms=['node.view'])
def get_hardware_options():
    """Get available hardware options (CPU types, SCSI controllers, etc.)
    
    NS: Extended Dec 2025 with machine types
    """
    # Use any manager to get options
    if cluster_managers:
        manager = list(cluster_managers.values())[0]
        return jsonify({
            'cpu_types': manager.get_cpu_types(),
            'scsi_controllers': manager.get_scsi_controllers(),
            'network_models': manager.get_network_models(),
            'disk_bus_types': manager.get_disk_bus_types(),
            'cache_modes': manager.get_cache_modes(),
            'machine_types': manager.get_machine_types()
        })
    else:
        # Return defaults if no cluster configured
        return jsonify({
            'cpu_types': ['host', 'kvm64', 'qemu64', 'x86-64-v2-AES'],
            'scsi_controllers': [{'value': 'virtio-scsi-pci', 'label': 'VirtIO SCSI'}],
            'network_models': [{'value': 'virtio', 'label': 'VirtIO'}],
            'disk_bus_types': [{'value': 'scsi', 'label': 'SCSI', 'max': 30}],
            'cache_modes': [{'value': '', 'label': 'Default'}],
            'machine_types': [
                {'value': '', 'label': 'Default'},
                {'value': 'q35', 'label': 'q35 (Latest)'},
                {'value': 'pc-q35-10.1', 'label': 'q35 10.1'},
                {'value': 'pc-q35-9.2+pve1', 'label': 'q35 9.2+pve1'},
                {'value': 'pc-q35-8.2', 'label': 'q35 8.2'},
                {'value': 'i440fx', 'label': 'i440fx (Latest)'},
                {'value': 'pc-i440fx-10.1', 'label': 'i440fx 10.1'},
                {'value': 'pc-i440fx-9.2+pve1', 'label': 'i440fx 9.2+pve1'},
                {'value': 'pc-i440fx-8.2', 'label': 'i440fx 8.2'},
            ]
        })


# WebSocket proxy for VNC - using geventwebsocket
def handle_vnc_websocket(ws, cluster_id, node, vm_type, vmid):
    """Handle VNC WebSocket connection"""
    print(f"\n{'='*60}")
    print(f"VNC WEBSOCKET: {vm_type}/{vmid} on {node}")
    print(f"{'='*60}")
    
    if cluster_id not in cluster_managers:
        print(f"ERROR: Cluster {cluster_id} not found")
        return
    
    manager = cluster_managers[cluster_id]
    host = manager.current_host or manager.config.host
    
    print(f"Target host: {host}")
    
    pve_ws = None
    running = True
    
    try:
        import gevent
        from gevent import spawn, sleep as gsleep
        import urllib.parse
        import urllib.request
        import json
        import websocket
        
        # Create SSL context
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # Step 1: Login
        print(f"Step 1: Login...")
        login_data = urlencode({
            'username': manager.config.user,
            'password': manager.config.pass_
        }).encode('utf-8')
        
        login_req = urllib.request.Request(
            f"https://{host}:8006/api2/json/access/ticket",
            data=login_data, method='POST'
        )
        
        with urllib.request.urlopen(login_req, context=ssl_context, timeout=10) as response:
            login_result = json.loads(response.read().decode('utf-8'))
        
        pve_ticket = login_result['data']['ticket']
        csrf_token = login_result['data']['CSRFPreventionToken']
        print(f"Got PVE ticket")
        
        # Step 2: Get VNC ticket
        print(f"Step 2: Get VNC ticket...")
        if vm_type == 'qemu':
            vnc_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/vncproxy"
        else:
            vnc_url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/vncproxy"
        
        vnc_data = urlencode({'websocket': '1'}).encode('utf-8')
        vnc_req = urllib.request.Request(vnc_url, data=vnc_data, method='POST')
        vnc_req.add_header('Cookie', f'PVEAuthCookie={pve_ticket}')
        vnc_req.add_header('CSRFPreventionToken', csrf_token)
        
        with urllib.request.urlopen(vnc_req, context=ssl_context, timeout=10) as response:
            vnc_result = json.loads(response.read().decode('utf-8'))
        
        vnc_ticket = vnc_result['data']['ticket']
        port = vnc_result['data']['port']
        print(f"Got VNC ticket, port={port}")
        
        # Step 3: Connect to Proxmox WebSocket
        print(f"Step 3: Connect to Proxmox...")
        encoded_vnc_ticket = url_quote(vnc_ticket, safe='')
        
        if vm_type == 'qemu':
            pve_ws_path = f"/api2/json/nodes/{node}/qemu/{vmid}/vncwebsocket?port={port}&vncticket={encoded_vnc_ticket}"
        else:
            pve_ws_path = f"/api2/json/nodes/{node}/lxc/{vmid}/vncwebsocket?port={port}&vncticket={encoded_vnc_ticket}"
        
        pve_ws_url = f"wss://{host}:8006{pve_ws_path}"
        
        pve_ws = websocket.create_connection(
            pve_ws_url,
            sslopt={"cert_reqs": ssl.CERT_NONE},
            header={"Cookie": f"PVEAuthCookie={pve_ticket}"},
            timeout=5
        )
        
        print(f"✓ Connected to Proxmox!")
        pve_ws.settimeout(0.1)
        
        bytes_sent = 0
        bytes_received = 0
        
        # Greenlet to read from Proxmox and send to client
        def proxmox_to_client():
            nonlocal bytes_received, running
            try:
                while running:
                    try:
                        data = pve_ws.recv()
                        if data:
                            bytes_received += len(data)
                            ws.send(data)
                    except websocket.WebSocketTimeoutException:
                        gsleep(0.01)
                    except websocket.WebSocketConnectionClosedException:
                        print("Proxmox closed")
                        running = False
                        break
                    except Exception as e:
                        if running:
                            print(f"PVE->Client error: {e}")
                        running = False
                        break
            except Exception as e:
                print(f"proxmox_to_client crashed: {e}")
                running = False
        
        # Start the proxmox reader greenlet
        pve_reader = spawn(proxmox_to_client)
        
        print(f"Step 4: Proxy running...")
        
        # Main loop: read from client, send to Proxmox
        while running:
            try:
                data = ws.receive()
                if data is None:
                    print("Client disconnected")
                    running = False
                    break
                if data:
                    bytes_sent += len(data)
                    pve_ws.send(data)
            except Exception as e:
                if running:
                    err_str = str(e)
                    if 'closed' not in err_str.lower():
                        print(f"Client->PVE error: {e}")
                running = False
                break
        
        running = False
        pve_reader.kill()
        
        print(f"Session ended: sent {bytes_sent}, received {bytes_received}")
        
    except Exception as e:
        print(f"ERROR: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
    finally:
        running = False
        if pve_ws:
            try:
                pve_ws.close()
            except:
                pass
        print(f"{'='*60}\n")


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/vncwebsocket')
def vnc_websocket_route(cluster_id, node, vm_type, vmid):
    """WebSocket endpoint for VNC - redirect to dedicated WS port
    
    NS: Auth via query param since WebSocket can't send custom headers
    """
    # Auth check via query param
    session_id = request.args.get('session')
    if not session_id:
        return jsonify({'error': 'Session required', 'code': 'AUTH_REQUIRED'}), 401
    
    session = validate_session(session_id)
    if not session:
        return jsonify({'error': 'Invalid session', 'code': 'INVALID_SESSION'}), 401
    
    # Check permissions
    users = load_users()
    user = users.get(session['user'], {})
    user_perms = get_user_permissions(user)
    if 'vm.console' not in user_perms and session['role'] != ROLE_ADMIN:
        return jsonify({'error': 'Permission denied', 'code': 'INSUFFICIENT_PERMISSIONS'}), 403
    
    # This route is just a fallback - actual WebSocket handling is done by the 
    # dedicated WebSocket server started in start_vnc_websocket_server()
    from flask import request
    
    print(f"\n*** VNC ROUTE HIT (HTTP): {vm_type}/{vmid} on {node} ***")
    print(f"HTTP_UPGRADE: {request.environ.get('HTTP_UPGRADE', 'NONE')}")
    print(f"wsgi.websocket: {request.environ.get('wsgi.websocket', 'NONE')}")
    
    # Try geventwebsocket first
    ws = request.environ.get('wsgi.websocket')
    if ws is not None:
        print("Using geventwebsocket handler...")
        handle_vnc_websocket(ws, cluster_id, node, vm_type, vmid)
        return ''
    
    # If not a websocket, return error
    return jsonify({'error': 'WebSocket connection required'}), 426


# Standalone VNC WebSocket Server using websockets library
def start_vnc_websocket_server(port=5001, ssl_cert=None, ssl_key=None):
    """Start a dedicated WebSocket server for VNC proxying"""
    import asyncio
    import re
    import threading
    
    try:
        import websockets
    except ImportError:
        print("WARNING: 'websockets' library not installed. VNC console will not work.")
        print("Install with: pip install websockets")
        return
    
    # Event to signal server is ready
    server_ready = threading.Event()
    
    async def vnc_handler(websocket):
        """Handle VNC WebSocket connections
        
        NS: Auth via query param since WebSocket can't send custom headers
        """
        # Get path from websocket
        path = websocket.request.path if hasattr(websocket, 'request') else websocket.path
        
        print(f"\n{'='*60}")
        print(f"VNC WebSocket connected: {path}")
        print(f"{'='*60}")
        
        # Extract session from query params
        from urllib.parse import urlparse, parse_qs
        parsed = urlparse(path)
        query_params = parse_qs(parsed.query)
        session_id = query_params.get('session', [None])[0]
        
        if not session_id:
            print("ERROR: No session provided")
            await websocket.close(1002, "Authentication required")
            return
        
        session = validate_session(session_id)
        if not session:
            print("ERROR: Invalid session")
            await websocket.close(1002, "Invalid session")
            return
        
        # Check permissions
        users = load_users()
        user = users.get(session['user'], {})
        user_perms = get_user_permissions(user)
        if 'vm.console' not in user_perms and session['role'] != ROLE_ADMIN:
            print(f"ERROR: User {session['user']} lacks vm.console permission")
            await websocket.close(1002, "Permission denied")
            return
        
        print(f"User {session['user']} authenticated for VNC")
        
        # Parse path: /api/clusters/{cluster_id}/vms/{node}/{vm_type}/{vmid}/vncwebsocket
        import re
        match = re.match(r'/api/clusters/([^/]+)/vms/([^/]+)/(qemu|lxc)/(\d+)/vncwebsocket', parsed.path)
        if not match:
            print(f"ERROR: Invalid path: {parsed.path}")
            await websocket.close(1002, "Invalid path")
            return
        
        cluster_id, node, vm_type, vmid = match.groups()
        vmid = int(vmid)
        
        print(f"Cluster: {cluster_id}, Node: {node}, Type: {vm_type}, VMID: {vmid}")
        
        if cluster_id not in cluster_managers:
            print(f"ERROR: Cluster {cluster_id} not found")
            await websocket.close(1002, "Cluster not found")
            return
        
        manager = cluster_managers[cluster_id]
        host = manager.current_host or manager.config.host
        
        print(f"Target host: {host}")
        
        pve_ws = None
        
        try:
            import urllib.parse
            import urllib.request
            import json
            import websocket as ws_client  # websocket-client for connecting to Proxmox
            
            # Create SSL context
            ssl_ctx = ssl.create_default_context()
            ssl_ctx.check_hostname = False
            ssl_ctx.verify_mode = ssl.CERT_NONE
            
            # Step 1: Login to Proxmox
            print("Step 1: Login to Proxmox...")
            login_data = urlencode({
                'username': manager.config.user,
                'password': manager.config.pass_
            }).encode('utf-8')
            
            login_req = urllib.request.Request(
                f"https://{host}:8006/api2/json/access/ticket",
                data=login_data, method='POST'
            )
            
            with urllib.request.urlopen(login_req, context=ssl_ctx, timeout=10) as response:
                login_result = json.loads(response.read().decode('utf-8'))
            
            pve_ticket = login_result['data']['ticket']
            csrf_token = login_result['data']['CSRFPreventionToken']
            print("Got PVE ticket")
            
            # Step 2: Get VNC ticket
            print("Step 2: Get VNC ticket...")
            if vm_type == 'qemu':
                vnc_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/vncproxy"
            else:
                vnc_url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/vncproxy"
            
            vnc_data = urlencode({'websocket': '1'}).encode('utf-8')
            vnc_req = urllib.request.Request(vnc_url, data=vnc_data, method='POST')
            vnc_req.add_header('Cookie', f'PVEAuthCookie={pve_ticket}')
            vnc_req.add_header('CSRFPreventionToken', csrf_token)
            
            with urllib.request.urlopen(vnc_req, context=ssl_ctx, timeout=10) as response:
                vnc_result = json.loads(response.read().decode('utf-8'))
            
            vnc_ticket = vnc_result['data']['ticket']
            port = vnc_result['data']['port']
            print(f"Got VNC ticket, port={port}")
            
            # Step 3: Connect to Proxmox WebSocket
            print("Step 3: Connect to Proxmox WebSocket...")
            encoded_vnc_ticket = url_quote(vnc_ticket, safe='')
            
            if vm_type == 'qemu':
                pve_ws_path = f"/api2/json/nodes/{node}/qemu/{vmid}/vncwebsocket?port={port}&vncticket={encoded_vnc_ticket}"
            else:
                pve_ws_path = f"/api2/json/nodes/{node}/lxc/{vmid}/vncwebsocket?port={port}&vncticket={encoded_vnc_ticket}"
            
            pve_ws_url = f"wss://{host}:8006{pve_ws_path}"
            
            pve_ws = ws_client.create_connection(
                pve_ws_url,
                sslopt={"cert_reqs": ssl.CERT_NONE},
                header={"Cookie": f"PVEAuthCookie={pve_ticket}"},
                timeout=5
            )
            
            print("Connected to Proxmox!")
            pve_ws.settimeout(0.05)  # Short timeout for non-blocking
            
            bytes_sent = 0
            bytes_received = 0
            
            print("Step 4: Starting proxy loop...")
            
            import asyncio
            
            bytes_sent = 0
            bytes_received = 0
            running = True
            
            # Set Proxmox socket to very short timeout for non-blocking behavior
            pve_ws.settimeout(0.001)
            
            async def proxmox_to_client():
                """Forward data from Proxmox to browser"""
                nonlocal bytes_received, running
                while running:
                    try:
                        # Non-blocking receive
                        try:
                            data = pve_ws.recv()
                            if data:
                                bytes_received += len(data)
                                if isinstance(data, str):
                                    data = data.encode('latin-1')
                                await websocket.send(data)
                        except ws_client.WebSocketTimeoutException:
                            # No data available, yield control
                            await asyncio.sleep(0.005)
                    except ws_client.WebSocketConnectionClosedException:
                        print("Proxmox closed connection")
                        running = False
                        break
                    except Exception as e:
                        if running:
                            print(f"PVE->Client error: {e}")
                        running = False
                        break
            
            async def client_to_proxmox():
                """Forward data from browser to Proxmox"""
                nonlocal bytes_sent, running
                try:
                    async for message in websocket:
                        if not running:
                            break
                        bytes_sent += len(message)
                        if isinstance(message, str):
                            message = message.encode('latin-1')
                        pve_ws.send(message)
                except Exception as e:
                    if running and 'close' not in str(e).lower():
                        print(f"Client->PVE error: {e}")
                    running = False
            
            # Run both directions concurrently
            task1 = asyncio.create_task(proxmox_to_client())
            task2 = asyncio.create_task(client_to_proxmox())
            
            done, pending = await asyncio.wait(
                [task1, task2],
                return_when=asyncio.FIRST_COMPLETED
            )
            
            running = False
            
            for task in pending:
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass
            
            print(f"Session ended: sent {bytes_sent}, received {bytes_received}")
            
        except Exception as e:
            print(f"ERROR: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if pve_ws:
                try:
                    pve_ws.close()
                except:
                    pass
            print(f"{'='*60}\n")
    
    async def main():
        nonlocal server_ready
        ssl_context = None
        if ssl_cert and ssl_key:
            ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
            ssl_context.load_cert_chain(ssl_cert, ssl_key)
            proto = "wss"
        else:
            proto = "ws"
        
        # LW: suppress websocket error logs from bots/scanners
        import logging as ws_logging
        ws_logging.getLogger('websockets').setLevel(ws_logging.CRITICAL)
        
        # Use the new websockets API
        async with websockets.serve(vnc_handler, "0.0.0.0", port, ssl=ssl_context):
            print(f"VNC WebSocket Server ready on {proto}://0.0.0.0:{port}")
            server_ready.set()
            await asyncio.Future()  # Run forever
    
    # Run in a separate thread
    def run_server():
        try:
            asyncio.run(main())
        except (KeyboardInterrupt, SystemExit):
            pass  # Clean shutdown, no traceback
        except RuntimeError as e:
            if "cannot be called from a running event loop" not in str(e):
                raise  # Re-raise if it's a different RuntimeError
    
    ws_thread = threading.Thread(target=run_server, daemon=True)
    ws_thread.start()
    
    # Wait for server to be ready (max 5 seconds)
    if server_ready.wait(timeout=5):
        print(f"VNC WebSocket Server started successfully")
    else:
        print(f"WARNING: VNC WebSocket Server may not be ready yet")


# Keep flask-sock version as backup (renamed)
@sock.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/vncwebsocket')
def vnc_websocket_proxy(ws, cluster_id, node, vm_type, vmid):
    """WebSocket proxy for VNC connection via Flask-Sock (same port as main app)"""
    import gevent
    from gevent import spawn, sleep as gsleep
    
    print(f"\n{'='*60}")
    print(f"VNC WEBSOCKET: {vm_type}/{vmid} on {node}")
    print(f"{'='*60}")
    
    # Authenticate via session from query params
    session_id = request.args.get('session')
    if not session_id:
        print("ERROR: No session provided")
        try:
            ws.send('Authentication required')
        except:
            pass
        return
    
    session = validate_session(session_id)
    if not session:
        print("ERROR: Invalid session")
        try:
            ws.send('Invalid session')
        except:
            pass
        return
    
    # Check permissions
    users = load_users()
    user = users.get(session['user'], {})
    user_perms = get_user_permissions(user)
    if 'vm.console' not in user_perms and session['role'] != ROLE_ADMIN:
        print(f"ERROR: User {session['user']} lacks vm.console permission")
        try:
            ws.send('Permission denied')
        except:
            pass
        return
    
    print(f"User {session['user']} authenticated for VNC")
    
    if cluster_id not in cluster_managers:
        print(f"ERROR: Cluster {cluster_id} not found")
        return
    
    manager = cluster_managers[cluster_id]
    host = manager.current_host or manager.config.host
    
    print(f"Target host: {host}")
    
    pve_ws = None
    running = True
    
    try:
        import urllib.parse
        import urllib.request
        import json
        import websocket
        
        # Create SSL context
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # Step 1: Login
        print(f"Step 1: Login...")
        login_data = urlencode({
            'username': manager.config.user,
            'password': manager.config.pass_
        }).encode('utf-8')
        
        login_req = urllib.request.Request(
            f"https://{host}:8006/api2/json/access/ticket",
            data=login_data, method='POST'
        )
        
        with urllib.request.urlopen(login_req, context=ssl_context, timeout=10) as response:
            login_result = json.loads(response.read().decode('utf-8'))
        
        pve_ticket = login_result['data']['ticket']
        csrf_token = login_result['data']['CSRFPreventionToken']
        print(f"Got PVE ticket")
        
        # Step 2: Get VNC ticket
        print(f"Step 2: Get VNC ticket...")
        if vm_type == 'qemu':
            vnc_url = f"https://{host}:8006/api2/json/nodes/{node}/qemu/{vmid}/vncproxy"
        else:
            vnc_url = f"https://{host}:8006/api2/json/nodes/{node}/lxc/{vmid}/vncproxy"
        
        vnc_data = urlencode({'websocket': '1'}).encode('utf-8')
        vnc_req = urllib.request.Request(vnc_url, data=vnc_data, method='POST')
        vnc_req.add_header('Cookie', f'PVEAuthCookie={pve_ticket}')
        vnc_req.add_header('CSRFPreventionToken', csrf_token)
        
        with urllib.request.urlopen(vnc_req, context=ssl_context, timeout=10) as response:
            vnc_result = json.loads(response.read().decode('utf-8'))
        
        vnc_ticket = vnc_result['data']['ticket']
        port = vnc_result['data']['port']
        print(f"Got VNC ticket, port={port}")
        
        # Step 3: Connect to Proxmox WebSocket
        print(f"Step 3: Connect to Proxmox...")
        encoded_vnc_ticket = url_quote(vnc_ticket, safe='')
        
        if vm_type == 'qemu':
            pve_ws_path = f"/api2/json/nodes/{node}/qemu/{vmid}/vncwebsocket?port={port}&vncticket={encoded_vnc_ticket}"
        else:
            pve_ws_path = f"/api2/json/nodes/{node}/lxc/{vmid}/vncwebsocket?port={port}&vncticket={encoded_vnc_ticket}"
        
        pve_ws_url = f"wss://{host}:8006{pve_ws_path}"
        
        pve_ws = websocket.create_connection(
            pve_ws_url,
            sslopt={"cert_reqs": ssl.CERT_NONE},
            header={"Cookie": f"PVEAuthCookie={pve_ticket}"},
            timeout=5
        )
        
        print(f"✓ Connected!")
        pve_ws.settimeout(0.1)
        
        bytes_sent = 0
        bytes_received = 0
        
        # Greenlet to read from Proxmox and send to client
        def proxmox_to_client():
            nonlocal bytes_received, running
            try:
                while running:
                    try:
                        data = pve_ws.recv()
                        if data:
                            bytes_received += len(data)
                            ws.send(data)
                    except websocket.WebSocketTimeoutException:
                        gsleep(0.01)
                    except websocket.WebSocketConnectionClosedException:
                        print("Proxmox closed")
                        running = False
                        break
                    except Exception as e:
                        if running:
                            print(f"PVE->Client error: {e}")
                        running = False
                        break
            except Exception as e:
                print(f"proxmox_to_client crashed: {e}")
                running = False
        
        # Start the proxmox reader greenlet
        pve_reader = spawn(proxmox_to_client)
        
        print(f"Step 4: Proxy running...")
        
        # Main loop: read from client, send to Proxmox
        while running:
            try:
                data = ws.receive(timeout=0.1)
                if data is None:
                    print("Client disconnected")
                    running = False
                    break
                if data:
                    bytes_sent += len(data)
                    pve_ws.send(data)
            except TimeoutError:
                gsleep(0.01)
            except Exception as e:
                if "timed out" not in str(e).lower() and "timeout" not in str(e).lower():
                    print(f"Client->PVE error: {e}")
                    running = False
                    break
                gsleep(0.01)
        
        running = False
        pve_reader.kill()
        
        print(f"Session ended: sent {bytes_sent}, received {bytes_received}")
        
    except Exception as e:
        print(f"ERROR: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
    finally:
        running = False
        if pve_ws:
            try:
                pve_ws.close()
            except:
                pass
        print(f"{'='*60}\n")


def start_ssh_websocket_server(port=5002, ssl_cert=None, ssl_key=None):
    """Start a dedicated WebSocket server for SSH terminal proxying
    
    runs as separate process to avoid gevent/asyncio conflicts.
    Gevent monkey-patches asyncio which breaks the websockets library.
    By using a subprocess, we get a clean Python interpreter.
    """
    import subprocess
    import sys
    import os
    
    # Create a standalone script that runs the SSH WebSocket server
    server_script = '''#!/usr/bin/env python3
"""Standalone SSH WebSocket Server - runs without gevent"""
import asyncio
import ssl
import json
import re
import sys
import os
import warnings
warnings.filterwarnings('ignore')

PORT = int(os.environ.get('SSH_WS_PORT', 5002))
SSL_CERT = os.environ.get('SSH_WS_SSL_CERT', '')
SSL_KEY = os.environ.get('SSH_WS_SSL_KEY', '')
PEGAPROX_URL = os.environ.get('PEGAPROX_URL', 'http://127.0.0.1:5000')

try:
    import websockets
    import paramiko
    import requests
    import urllib3
    urllib3.disable_warnings()
except ImportError as e:
    print(f"Missing library: {e}")
    sys.exit(1)

async def ssh_handler(websocket):
    """SSH WebSocket handler with user credential prompt and SSH key support
    
    MK: Supports both password and SSH key authentication
    Frontend can pre-fetch the IP and pass it as query parameter
    """
    path = websocket.request.path if hasattr(websocket, 'request') else websocket.path
    print(f"SSH WebSocket connection: {path}")
    
    from urllib.parse import urlparse, parse_qs, unquote
    parsed = urlparse(path)
    query = parse_qs(parsed.query)
    session_id = query.get('session', [None])[0]
    prefetched_ip = query.get('ip', [None])[0]  # IP pre-fetched by frontend
    if prefetched_ip:
        prefetched_ip = unquote(prefetched_ip)
        print(f"Frontend provided IP: {prefetched_ip}")
    
    # Match both /shell and /shellws
    match = re.match(r'/api/clusters/([^/]+)/nodes/([^/]+)/shell(?:ws)?', parsed.path)
    if not match:
        print(f"Invalid path: {parsed.path}")
        await websocket.send('{"status":"error","message":"Invalid path"}')
        await websocket.close(1008, "Invalid path")
        return
    
    cluster_id, node = match.groups()
    print(f"Cluster: {cluster_id}, Node: {node}")
    
    # Validate session (just to ensure user is logged in to PegaProx)
    if not session_id:
        print("No session provided")
        await websocket.send('{"status":"error","message":"No session provided"}')
        await websocket.close(1008, "No session")
        return
    
    print(f"Session ID received: {session_id[:20]}..." if len(session_id) > 20 else f"Session ID: {session_id}")
    
    try:
        print(f"Validating session with: {PEGAPROX_URL}/api/auth/validate")
        # Try both cookie and header-based auth
        headers = {'X-Session-ID': session_id}
        r = requests.get(f"{PEGAPROX_URL}/api/auth/validate", 
                        cookies={'session': session_id}, 
                        headers=headers,
                        timeout=5, verify=False)
        print(f"Session validation response: {r.status_code}")
        if r.status_code != 200:
            print(f"Session validation failed: {r.status_code} - {r.text[:100] if r.text else 'no body'}")
            await websocket.send('{"status":"error","message":"Session ungültig - bitte neu einloggen"}')
            await websocket.close(1008, "Invalid session")
            return
        print("Session validation successful")
    except requests.exceptions.ConnectionError as e:
        print(f"Connection error to main server: {e}")
        # Allow connection if main server is unreachable (for debugging)
        print("WARNING: Skipping session validation due to connection error")
    except Exception as e:
        print(f"Auth error: {e}")
        await websocket.send('{"status":"error","message":"Authentifizierungsfehler"}')
        await websocket.close(1011, "Auth error")
        return
    
    # Get node IP - use pre-fetched IP if available
    node_ip = prefetched_ip if prefetched_ip else None
    cluster_host = None
    
    # Only try API if we don't have a pre-fetched IP
    if not node_ip:
        # Method 1: Try API endpoint
        try:
            print(f"Fetching cluster creds from: {PEGAPROX_URL}/api/internal/cluster-creds/{cluster_id}")
            r = requests.get(f"{PEGAPROX_URL}/api/internal/cluster-creds/{cluster_id}", cookies={'session': session_id}, timeout=10, verify=False)
            print(f"Cluster creds response: {r.status_code}")
            if r.status_code == 200:
                creds = r.json()
                cluster_host = creds.get('host')
                node_ips = creds.get('node_ips', {})
                
                # Try exact match first, then case-insensitive
                node_ip = node_ips.get(node) or node_ips.get(node.lower())
                
                print(f"Got node_ips: {node_ips}, looking for: {node}, found: {node_ip}, cluster_host: {cluster_host}")
            else:
                print(f"Cluster creds failed: {r.status_code} - {r.text[:200] if r.text else 'no body'}")
        except Exception as e:
            print(f"Could not get node IP from API: {e}")
        
        # Method 2: Fallback - read directly from clusters config file
        if not cluster_host:
            try:
                import os
                # Try common config locations
                config_paths = [
                    'config/clusters.json',  # Relative to working dir
                    './config/clusters.json',
                    '/home/admin_321/pegaprox/config/clusters.json',
                    '/home/admin_321/pegaprox/data/clusters.json',
                    './data/clusters.json',
                    os.path.expanduser('~/.pegaprox/clusters.json'),
                    '/var/lib/pegaprox/clusters.json'
                ]
                print(f"Trying config file fallback, cwd={os.getcwd()}")
                for config_path in config_paths:
                    if os.path.exists(config_path):
                        print(f"Found config at: {config_path}")
                        with open(config_path, 'r') as f:
                            clusters = json.load(f)
                        if cluster_id in clusters:
                            cluster_host = clusters[cluster_id].get('host')
                            print(f"Got cluster_host from config file: {cluster_host}")
                            break
                        else:
                            print(f"Cluster {cluster_id} not in config, available: {list(clusters.keys())}")
            except Exception as e:
                print(f"Config file fallback failed: {e}")
        
        # Use cluster_host as fallback for node_ip
        if not node_ip and cluster_host:
            node_ip = cluster_host
            print(f"Using cluster host as fallback: {cluster_host}")
    
    # If we still don't have an IP, allow manual entry
    allow_manual_ip = False
    if not node_ip:
        print(f"No IP found - allowing manual entry")
        node_ip = ""  # Empty - user must provide
        allow_manual_ip = True
    
    print(f"Final node IP for {node}: {node_ip or '(manual entry required)'}")
    
    # Send need_credentials status - frontend will show login dialog
    await websocket.send(json.dumps({
        'status': 'need_credentials',
        'node': node,
        'ip': node_ip,
        'allowManualIp': allow_manual_ip
    }))
    
    # Wait for credentials from user
    try:
        creds_msg = await asyncio.wait_for(websocket.recv(), timeout=300)  # 5 min timeout
        creds = json.loads(creds_msg)
        ssh_user = creds.get('username', 'root')
        ssh_pass = creds.get('password', '')
        ssh_key = creds.get('privateKey', '')  # SSH private key (PEM format)
        
        # Allow user to override IP (for manual entry)
        user_ip = creds.get('host', '').strip()
        if user_ip:
            node_ip = user_ip
            print(f"Using user-provided IP: {node_ip}")
        
        if not node_ip:
            await websocket.send('{"status":"error","message":"Host/IP address required"}')
            return
        
        if not ssh_pass and not ssh_key:
            await websocket.send('{"status":"error","message":"Password or SSH key required"}')
            return
            
    except asyncio.TimeoutError:
        await websocket.send('{"status":"error","message":"Login timeout"}')
        await websocket.close(1008, "Timeout")
        return
    except Exception as e:
        print(f"Credentials receive error: {e}")
        await websocket.send('{"status":"error","message":"Failed to receive credentials"}')
        return
    
    # Send connecting status
    await websocket.send('{"status":"connecting"}')
    
    # Connect SSH
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    
    try:
        print(f"Connecting SSH to {ssh_user}@{node_ip}...")
        
        # Try SSH key authentication first if provided
        if ssh_key:
            try:
                import io
                # Parse the private key
                key_file = io.StringIO(ssh_key)
                
                # Try different key types
                pkey = None
                for key_class in [paramiko.RSAKey, paramiko.Ed25519Key, paramiko.ECDSAKey, paramiko.DSSKey]:
                    try:
                        key_file.seek(0)
                        pkey = key_class.from_private_key(key_file, password=ssh_pass if ssh_pass else None)
                        break
                    except:
                        continue
                
                if pkey:
                    print(f"Using SSH key authentication")
                    ssh.connect(node_ip, port=22, username=ssh_user, pkey=pkey, timeout=10, look_for_keys=False, allow_agent=False)
                else:
                    raise Exception("Could not parse SSH key - unsupported format")
                    
            except Exception as key_error:
                print(f"SSH key auth failed: {key_error}")
                await websocket.send(f'{{"status":"error","message":"SSH key error: {str(key_error)}"}}')
                return
        else:
            # Password authentication
            ssh.connect(node_ip, port=22, username=ssh_user, password=ssh_pass, timeout=10, look_for_keys=False, allow_agent=False)
        
        channel = ssh.invoke_shell(term='xterm-256color', width=120, height=40)
        channel.settimeout(0.1)
        
        print(f"SSH connected: {cluster_id}/{node}")
        
        # Send connected status - frontend will clear terminal
        await websocket.send('{"status":"connected"}')
        
        async def ssh_to_ws():
            while True:
                try:
                    if channel.recv_ready():
                        data = channel.recv(4096)
                        if data:
                            await websocket.send(data.decode('utf-8', errors='replace'))
                    await asyncio.sleep(0.01)
                except:
                    break
        
        async def ws_to_ssh():
            try:
                async for message in websocket:
                    if isinstance(message, str):
                        if message.startswith('{"type":"resize"'):
                            try:
                                data = json.loads(message)
                                if data.get('type') == 'resize':
                                    channel.resize_pty(width=data.get('cols', 120), height=data.get('rows', 40))
                            except:
                                pass
                        elif message.startswith('{'):
                            # Ignore other JSON messages (like old credential format)
                            pass
                        else:
                            channel.send(message)
                    else:
                        channel.send(message)
            except:
                pass
        
        await asyncio.gather(ssh_to_ws(), ws_to_ssh(), return_exceptions=True)
    except paramiko.AuthenticationException as e:
        print(f"SSH auth failed: {e}")
        await websocket.send(f'\\r\\n\\x1b[31mSSH Authentication Failed\\x1b[0m\\r\\nCheck cluster credentials.\\r\\n')
    except Exception as e:
        print(f"SSH error: {e}")
        try:
            await websocket.send(f"\\r\\n\\x1b[31mSSH Error: {e}\\x1b[0m\\r\\n")
        except:
            pass
    finally:
        try:
            ssh.close()
        except:
            pass
        print(f"SSH disconnected: {cluster_id}/{node}")

async def main():
    ssl_context = None
    if SSL_CERT and SSL_KEY and os.path.exists(SSL_CERT) and os.path.exists(SSL_KEY):
        ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
        ssl_context.load_cert_chain(SSL_CERT, SSL_KEY)
    
    async with websockets.serve(ssh_handler, "0.0.0.0", PORT, ssl=ssl_context, ping_interval=30, ping_timeout=10):
        print(f"SSH WebSocket server ready on port {PORT}")
        await asyncio.Future()

if __name__ == '__main__':
    asyncio.run(main())
'''
    
    # Write the script to a file
    script_dir = os.path.dirname(os.path.abspath(__file__))
    script_path = os.path.join(script_dir, '.ssh_ws_server.py')
    
    try:
        # kill existing process on port if any on this port first
        try:
            result = subprocess.run(
                ['fuser', '-k', f'{port}/tcp'],
                capture_output=True,
                timeout=5
            )
            if result.returncode == 0:
                print(f"Killed existing process on port {port}")
                time.sleep(0.5)  # Give it time to release the port
        except Exception as e:
            # fuser might not be available, try lsof
            try:
                result = subprocess.run(
                    ['lsof', '-t', f'-i:{port}'],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.stdout.strip():
                    pids = result.stdout.strip().split('\n')
                    for pid in pids:
                        try:
                            os.kill(int(pid), signal.SIGTERM)
                            print(f"Killed existing process {pid} on port {port}")
                        except:
                            pass
                    time.sleep(0.5)
            except:
                pass  # Neither fuser nor lsof available, hope for the best
        
        with open(script_path, 'w') as f:
            f.write(server_script)
        
        # Set environment variables for the subprocess
        env = os.environ.copy()
        env['SSH_WS_PORT'] = str(port)
        main_port = port - 2
        env['PEGAPROX_URL'] = f"https://127.0.0.1:{main_port}" if ssl_cert else f"http://127.0.0.1:{main_port}"
        if ssl_cert:
            env['SSH_WS_SSL_CERT'] = ssl_cert
        if ssl_key:
            env['SSH_WS_SSL_KEY'] = ssl_key
        
        # Start as subprocess (completely separate process, no gevent)
        # Use same working directory as main server
        proc = subprocess.Popen(
            [sys.executable, script_path],
            env=env,
            cwd=os.getcwd(),  # MK: Ensure same working dir for config file access
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            start_new_session=True
        )
        
        # Read output in background
        def read_output():
            for line in proc.stdout:
                line = line.decode('utf-8', errors='replace').strip()
                if line:
                    print(f"[SSH-WS] {line}")
        
        import threading
        output_thread = threading.Thread(target=read_output, daemon=True)
        output_thread.start()
        
        print(f"SSH WebSocket server subprocess started (PID: {proc.pid})")
        
    except Exception as e:
        print(f"Failed to start SSH WebSocket server: {e}")


# Terminal/Shell WebSocket proxy (legacy - flask-sock version, kept for non-gevent setups)
@sock.route('/api/clusters/<cluster_id>/nodes/<node>/shellwebsocket')
def node_shell_websocket_proxy(ws, cluster_id, node):
    """WebSocket proxy for node shell via SSH"""
    logging.info(f"")
    logging.info(f"========================================")
    logging.info(f"SSH SHELL: {cluster_id}/{node}")
    logging.info(f"========================================")
    
    # Check paramiko availability first
    try:
        import paramiko
    except ImportError:
        logging.error("paramiko not installed!")
        try:
            ws.send('{"status":"error","message":"SSH library (paramiko) not installed on server"}')
        except:
            pass
        return
    
    if cluster_id not in cluster_managers:
        logging.error(f"Cluster {cluster_id} not found")
        try:
            ws.send('{"status":"error","message":"Cluster not found"}')
        except:
            pass
        return
    
    manager = cluster_managers[cluster_id]
    cluster_host = manager.config.host
    
    # Get node IP address from cluster status
    logging.info(f"Step 1: Getting IP for node {node}...")
    
    # First authenticate with cluster
    if not manager.connect_to_proxmox():
        logging.error("Failed to authenticate with cluster!")
        try:
            ws.send('{"status":"error","message":"Cluster auth failed"}')
        except:
            pass
        return
    
    node_ip = None
    try:
        cluster_url = f"https://{cluster_host}:8006/api2/json/cluster/status"
        cluster_response = manager._create_session().get(cluster_url, timeout=5)
        if cluster_response.status_code == 200:
            cluster_data = cluster_response.json().get('data', [])
            for item in cluster_data:
                if item.get('type') == 'node' and item.get('name') == node:
                    node_ip = item.get('ip')
                    logging.info(f"  Found node IP: {node_ip}")
                    break
    except Exception as e:
        logging.error(f"  Error getting cluster status: {e}")
    
    if not node_ip:
        node_ip = cluster_host
        logging.info(f"  Using cluster host: {node_ip}")
    
    # Request credentials from client
    try:
        ws.send(f'{{"status":"need_credentials","node":"{node}","ip":"{node_ip}"}}')
    except Exception as e:
        logging.error(f"Failed to send need_credentials: {e}")
        return
    
    logging.info(f"Step 2: Waiting for SSH credentials...")
    
    # Wait for credentials from client
    try:
        cred_msg = ws.receive(timeout=60)
        if not cred_msg:
            logging.error("No credentials received")
            return
        
        creds = json.loads(cred_msg)
        ssh_user = creds.get('username', 'root')
        ssh_pass = creds.get('password', '')
        
        logging.info(f"  Got credentials for user: {ssh_user}")
        
    except Exception as e:
        logging.error(f"Error receiving credentials: {e}")
        try:
            ws.send('{"status":"error","message":"Credentials timeout"}')
        except:
            pass
        return
    
    # Tell client we're connecting
    try:
        ws.send('{"status":"connecting"}')
    except:
        return
    
    logging.info(f"Step 3: Connecting SSH to {ssh_user}@{node_ip}...")
    
    try:
        # Create SSH client
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        
        # Connect
        ssh.connect(
            hostname=node_ip,
            port=22,
            username=ssh_user,
            password=ssh_pass,
            timeout=30,
            allow_agent=False,
            look_for_keys=False
        )
        
        logging.info(f"Step 4: SSH connected! Opening shell...")
        
        # Get interactive shell
        channel = ssh.invoke_shell(term='xterm-256color', width=120, height=40)
        channel.settimeout(0.1)
        
        ws.send('{"status":"connected"}')
        logging.info(f"Step 5: Shell ready!")
        
        stop_event = threading.Event()
        
        # Thread: SSH -> WebSocket
        def ssh_to_ws():
            try:
                while not stop_event.is_set():
                    try:
                        if channel.recv_ready():
                            data = channel.recv(4096)
                            if data:
                                ws.send(data)
                        else:
                            import time
                            time.sleep(0.01)
                    except socket.timeout:
                        continue
                    except Exception as e:
                        logging.error(f"SSH recv error: {e}")
                        break
            except:
                pass
            finally:
                stop_event.set()
        
        ssh_thread = threading.Thread(target=ssh_to_ws)
        ssh_thread.daemon = True
        ssh_thread.start()
        
        # Main loop: WebSocket -> SSH
        while not stop_event.is_set():
            try:
                data = ws.receive()
                if data is None:
                    logging.info("Client disconnected")
                    break
                
                # Handle JSON messages
                if isinstance(data, str) and data.startswith('{'):
                    try:
                        msg = json.loads(data)
                        # Handle resize
                        if msg.get('type') == 'resize':
                            channel.resize_pty(
                                width=msg.get('cols', 120),
                                height=msg.get('rows', 40)
                            )
                        continue
                    except:
                        pass
                
                # Send to SSH
                if isinstance(data, str):
                    channel.send(data)
                else:
                    channel.send(data)
                    
            except Exception as e:
                logging.error(f"WS recv error: {e}")
                break
        
        stop_event.set()
        
    except paramiko.AuthenticationException:
        logging.error("SSH authentication failed!")
        ws.send('{"status":"error","message":"SSH Login fehlgeschlagen - falscher Username oder Passwort"}')
    except paramiko.SSHException as e:
        logging.error(f"SSH error: {e}")
        ws.send(f'{{"status":"error","message":"SSH Fehler: {str(e)}"}}')
    except Exception as e:
        logging.error(f"Shell error: {e}")
        import traceback
        traceback.print_exc()
        ws.send(f'{{"status":"error","message":"{str(e)}"}}')
    finally:
        try:
            channel.close()
        except:
            pass
        try:
            ssh.close()
        except:
            pass
        logging.info(f"========================================")
        logging.info(f"SSH SESSION ENDED: {node}")
        logging.info(f"========================================")


# Migration API Routes
@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/migrate', methods=['POST'])
@require_auth(perms=['vm.migrate'])
def migrate_vm_api(cluster_id, node, vm_type, vmid):
    """Migrate a VM or container to another node"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # MK: Check pool permission for vm.migrate
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    if not user_can_access_vm(user, cluster_id, vmid, 'vm.migrate', vm_type):
        return jsonify({'error': 'Permission denied: vm.migrate'}), 403
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    target_node = data.get('target')
    online = data.get('online', True)
    target_storage = data.get('targetstorage')
    with_local_disks = data.get('with-local-disks', False)
    force = data.get('force', False)  # For conntrack state in containers
    
    if not target_node:
        return jsonify({'error': 'Target node is required'}), 400
    
    # Build migration options
    migrate_options = {
        'online': online,
        'targetstorage': target_storage,
        'with_local_disks': with_local_disks,
        'force': force
    }
    
    result = manager.migrate_vm_manual(node, vmid, vm_type, target_node, online, migrate_options)
    
    if result.get('success'):
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'system')
        details = f"{vm_type.upper()} {vmid} migrated from {node} to {target_node}"
        if online:
            details += " (online)"
        if target_storage:
            details += f" to storage {target_storage}"
        log_audit(user, 'vm.migrated', details, cluster=manager.config.name)
        return jsonify(result)
    else:
        return jsonify(result), 400


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>', methods=['DELETE'])
@require_auth(perms=['vm.delete'])
def delete_vm_api(cluster_id, node, vm_type, vmid):
    # tenant check
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # MK: Check pool permission for vm.delete
    users = load_users()
    user = users.get(request.session['user'], {})
    user['username'] = request.session['user']
    if not user_can_access_vm(user, cluster_id, vmid, 'vm.delete', vm_type):
        return jsonify({'error': 'Permission denied: vm.delete'}), 403
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    purge = data.get('purge', False)
    destroy_unreferenced = data.get('destroyUnreferenced', False)
    
    result = manager.delete_vm(node, vmid, vm_type, purge, destroy_unreferenced)
    
    if result.get('success'):
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'vm.deleted', f"{vm_type.upper()} {vmid} deleted from {node}" + (" (purged)" if purge else ""), cluster=manager.config.name)
        broadcast_action('delete', vm_type, str(vmid), {'node': node, 'purge': purge}, cluster_id, usr)
        return jsonify({'message': f'{vm_type.upper()} {vmid} deleted', 'task': result.get('task')})
    else:
        return jsonify({'error': result.get('error', 'Delete failed')}), 500


@app.route('/api/clusters/<cluster_id>/vms/bulk-migrate', methods=['POST'])
@require_auth(perms=['vm.migrate'])
def bulk_migrate_api(cluster_id):
    """Migrate multiple VMs at once"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    vms = data.get('vms', [])  # List of {node, vmid, type}
    target_node = data.get('target')
    online = data.get('online', True)
    
    if not target_node:
        return jsonify({'error': 'Target node is required'}), 400
    
    if not vms:
        return jsonify({'error': 'No VMs specified'}), 400
    
    user = getattr(request, 'session', {}).get('user', 'system')
    log_audit(user, 'vm.bulk_migrated', f"Bulk migration of {len(vms)} VMs to {target_node}", cluster=mgr.config.name)
    
    results = []
    for vm in vms:
        result = mgr.migrate_vm_manual(vm['node'], vm['vmid'], vm['type'], target_node, online)
        results.append({
            'vmid': vm['vmid'],
            'success': result.get('success', False),
            'task': result.get('task'),
            'error': result.get('error')
        })
    
    return jsonify({
        'results': results,
        'total': len(vms),
        'successful': sum(1 for r in results if r['success'])
    })


@app.route('/api/clusters/<cluster_id>/fingerprint', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_cluster_fingerprint_api(cluster_id):
    """Get cluster SSL fingerprint for remote migration"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    result = manager.get_cluster_fingerprint()
    
    if result.get('success'):
        return jsonify(result)
    else:
        return jsonify({'error': result.get('error', 'Failed: fingerprint')}), 500


@app.route('/api/clusters/<cluster_id>/vms/<node>/<vm_type>/<int:vmid>/remote-migrate', methods=['POST'])
@require_auth(perms=['vm.migrate'])
def remote_migrate_vm_api(cluster_id, node, vm_type, vmid):
    """Cross-cluster remote migration"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    target_endpoint = data.get('target_endpoint')
    target_storage = data.get('target_storage')
    target_bridge = data.get('target_bridge')
    target_vmid = data.get('target_vmid')
    online = data.get('online', True)
    delete_source = data.get('delete_source', True)
    bwlimit = data.get('bwlimit')
    
    if not all([target_endpoint, target_storage, target_bridge]):
        return jsonify({'error': 'target_endpoint, target_storage, and target_bridge are required'}), 400
    
    result = manager.remote_migrate_vm(
        node, vmid, vm_type, 
        target_endpoint, target_storage, target_bridge,
        target_vmid, online, delete_source, bwlimit
    )
    
    if result.get('success'):
        return jsonify({'message': f'Remote migration started for {vm_type}/{vmid}', 'task': result.get('task')})
    else:
        return jsonify({'error': result.get('error', 'Remote migration failed')}), 500


@app.route('/api/cross-cluster-migrate', methods=['POST'])
@require_auth(perms=['vm.migrate'])
def cross_cluster_migrate_api():
    """
    High-level cross-cluster migration API
    
    MK: This is the fancy one - migrates VMs between completely separate
    Proxmox clusters using SSH tunnels. Takes care of:
    - Creating temp API tokens on target
    - Setting up SSH tunnel for migration traffic
    - Cleaning up tokens after migration
    
    Known issue: For large VMs (>50GB disk), online migration may fail with
    "401 Unauthorized" during RAM sync due to Proxmox WebSocket ticket timeout.
    Workaround: Use offline migration for large VMs.
    """
    data = request.json or {}
    
    source_cluster_id = data.get('source_cluster')
    target_cluster_id = data.get('target_cluster')
    vmid = data.get('vmid')
    vm_type = data.get('vm_type', 'qemu')
    source_node = data.get('source_node')
    target_node = data.get('target_node')
    target_storage = data.get('target_storage')
    target_bridge = data.get('target_bridge', 'vmbr0')
    target_vmid = data.get('target_vmid')
    online = data.get('online', True)
    delete_source = data.get('delete_source', True)
    
    if not target_node:
        return jsonify({'error': 'Target node is required for cross-cluster migration'}), 400
    
    if source_cluster_id not in cluster_managers:
        return jsonify({'error': 'Source cluster not found'}), 404
    if target_cluster_id not in cluster_managers:
        return jsonify({'error': 'Target cluster not found'}), 404
    
    source_manager = cluster_managers[source_cluster_id]
    target_manager = cluster_managers[target_cluster_id]
    
    # MK: Check VM disk size and warn about potential issues with online migration
    warnings = []
    try:
        vm_info = source_manager.get_vm_config(source_node, vmid, vm_type)
        if vm_info.get('success'):
            config = vm_info.get('config', {})
            total_disk_gb = 0
            for key, value in config.items():
                if key.startswith(('scsi', 'virtio', 'sata', 'ide')) and 'size' in str(value):
                    # Extract size from disk config
                    import re
                    size_match = re.search(r'size=(\d+)([GMT])', str(value))
                    if size_match:
                        size_val = int(size_match.group(1))
                        size_unit = size_match.group(2)
                        if size_unit == 'G':
                            total_disk_gb += size_val
                        elif size_unit == 'T':
                            total_disk_gb += size_val * 1024
                        elif size_unit == 'M':
                            total_disk_gb += size_val / 1024
            
            if total_disk_gb > 100 and online:
                warnings.append(f"VM has {total_disk_gb:.0f}GB disk - migration may take a while. Ensure stable network connection between clusters.")
                logging.info(f"[CROSS-MIGRATE] Large VM ({total_disk_gb}GB) - migration may take extended time")
    except Exception as e:
        logging.debug(f"Could not check VM size: {e}")
    
    # Generate unique token name
    import time
    token_name = f"pegaprox-migrate-{int(time.time())}"
    target_token = None
    
    try:
        # Step 1: Create temporary API token on TARGET cluster (without privilege separation)
        logging.info(f"Creating temporary API token on target cluster ({target_cluster_id}) for user {target_manager.config.user}...")
        token_result = target_manager.create_api_token(token_name)
        if not token_result.get('success'):
            return jsonify({'error': f'Could not create API token on target cluster: {token_result.get("error")}'}), 500
        
        target_token = token_result
        logging.info(f"Created token on target cluster: {target_token['token_id']}")
        
        # Step 2: Get target cluster fingerprint
        fp_result = target_manager.get_cluster_fingerprint()
        if not fp_result.get('success'):
            raise Exception(f'Could not get target fingerprint: {fp_result.get("error")}')
        
        # Step 3: Build target endpoint string
        # MK: Format must be exact - Proxmox is picky about this
        # Format: apitoken=PVEAPIToken=<user>!<tokenname>=<secret>,host=<host>,fingerprint=<fp>
        target_endpoint = (
            f"apitoken=PVEAPIToken={target_token['token_id']}={target_token['token_value']},"
            f"host={fp_result['host']},"
            f"fingerprint={fp_result['fingerprint']}"
        )
        
        logging.info(f"Starting remote migration of {vm_type}/{vmid} from {source_cluster_id} to {target_cluster_id}...")
        logging.info(f"Target host: {fp_result['host']}, Token user: {target_token['token_id'].split('!')[0]}, Online: {online}")
        
        # Step 4: Perform the migration
        result = source_manager.remote_migrate_vm(
            source_node, vmid, vm_type,
            target_endpoint, target_storage, target_bridge,
            target_vmid, online, delete_source
        )
        
        if result.get('success'):
            # Log to audit
            user = request.session.get('user', 'system')
            log_audit(
                user,
                'vm.cross_cluster_migrate',
                f"Cross-cluster migration: {vm_type}/{vmid} from {source_cluster_id} to {target_cluster_id}/{target_node}",
                request.remote_addr
            )
            
            # Schedule intelligent token cleanup - monitors task status
            task_upid = result.get('task')
            def cleanup_token_when_done():
                import time
                max_wait = 7200  # Maximum 2 hours (large VMs can take a long time!)
                poll_interval = 15  # Check every 15 seconds
                elapsed = 0
                min_wait_before_assuming_done = 300  # MK: Wait at least 5 minutes before assuming task is done
                
                logging.info(f"[TOKEN-CLEANUP] Monitoring task {task_upid} for completion...")
                
                while elapsed < max_wait:
                    try:
                        # Get task status from source cluster (where the migration task runs)
                        tasks = source_manager.get_tasks(limit=100)
                        task_found = False
                        
                        for task in tasks:
                            if task and task.get('upid') == task_upid:
                                task_found = True
                                status = task.get('status', '')
                                
                                # check task is finished
                                if status and status != 'running':
                                    if status == 'OK':
                                        logging.info(f"[TOKEN-CLEANUP] Migration task completed successfully!")
                                    else:
                                        logging.warning(f"[TOKEN-CLEANUP] Migration task ended with status: {status}")
                                    
                                    # MK: Wait a bit more after task completion to be safe
                                    # The VM might still be syncing final state
                                    time.sleep(30)
                                    
                                    # Task finished - delete token
                                    target_manager.delete_api_token(token_name)
                                    logging.info(f"[TOKEN-CLEANUP] Deleted migration token: {token_name}")
                                    return
                                break
                        
                        # MK: Fix for Issue #19 - Don't delete token too early!
                        # If task not found, it might have completed and scrolled out of task list
                        # BUT we need to wait much longer to be safe (was 60s, now 5 min minimum)
                        if not task_found and elapsed > min_wait_before_assuming_done:
                            # Double-check: Try to verify VM exists on target cluster
                            try:
                                # Check if VM exists on target (migration successful)
                                target_vms = target_manager.get_vm_resources()
                                vm_on_target = any(
                                    v.get('vmid') == vmid or v.get('vmid') == target_vmid
                                    for v in (target_vms or [])
                                )
                                if vm_on_target:
                                    logging.info(f"[TOKEN-CLEANUP] VM found on target cluster, migration likely successful")
                                else:
                                    logging.info(f"[TOKEN-CLEANUP] VM not yet on target, waiting longer...")
                                    time.sleep(poll_interval)
                                    elapsed += poll_interval
                                    continue
                            except Exception as e:
                                logging.warning(f"[TOKEN-CLEANUP] Could not verify VM on target: {e}")
                            
                            logging.info(f"[TOKEN-CLEANUP] Task no longer in task list after {elapsed}s, assuming completed")
                            target_manager.delete_api_token(token_name)
                            logging.info(f"[TOKEN-CLEANUP] Deleted migration token: {token_name}")
                            return
                            
                    except Exception as e:
                        logging.warning(f"[TOKEN-CLEANUP] Error checking task status: {e}")
                    
                    time.sleep(poll_interval)
                    elapsed += poll_interval
                
                # Timeout - delete token anyway
                logging.warning(f"[TOKEN-CLEANUP] Timeout after {max_wait}s waiting for task, deleting token anyway")
                target_manager.delete_api_token(token_name)
                logging.info(f"[TOKEN-CLEANUP] Deleted migration token: {token_name}")
            
            cleanup_thread = threading.Thread(target=cleanup_token_when_done, daemon=True)
            cleanup_thread.start()
            
            response = {
                'message': f'Cross-cluster migration started: {vm_type}/{vmid} from {source_cluster_id} to {target_cluster_id}/{target_node}',
                'task': result.get('task'),
                'online': online,
                'info': 'Temporary API token will be automatically cleaned up after migration completes.'
            }
            if warnings:
                response['warnings'] = warnings
            
            return jsonify(response)
        else:
            # Migration failed - cleanup token immediately
            error_msg = result.get('error', 'Cross-cluster migration failed')
            
            # MK: Add helpful hint for 401 errors
            if '401' in error_msg or 'Unauthorized' in error_msg or 'Broken pipe' in error_msg:
                error_msg += ". If this persists, check PegaProx version (token cleanup timing was fixed in 0.6.2)"
            
            target_manager.delete_api_token(token_name)
            return jsonify({'error': error_msg}), 500
            
    except Exception as e:
        # Cleanup token on any error
        if target_token:
            target_manager.delete_api_token(token_name)
        logging.error(f"Cross-cluster migration error: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/nodes-status', methods=['GET'])
@require_auth(perms=["node.view"])
def get_cluster_nodes_status_api(cluster_id):
    """Get list of nodes with status info - LW: alternative endpoint with more details"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    node_status = manager.get_node_status()
    
    # Return just the node names and basic info
    nodes = []
    for node_name, status in node_status.items():
        nodes.append({
            'node': node_name,
            'status': status.get('status', 'unknown'),
            'cpu_percent': status.get('cpu_percent', 0),
            'mem_percent': status.get('mem_percent', 0)
        })
    
    return jsonify(nodes)


# VM/CT Creation API Routes
@app.route('/api/clusters/<cluster_id>/nodes/<node>/nextid', methods=['GET'])
@require_auth(perms=['vm.view'])
def get_next_vmid_for_node_api(cluster_id, node):
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    result = manager.get_next_vmid()
    
    if result.get('success'):
        return jsonify({'vmid': result['vmid']})
    else:
        return jsonify(result), 400


@app.route('/api/clusters/<cluster_id>/nodes/<node>/templates', methods=['GET'])
@require_auth(perms=['storage.view'])
def get_templates_api(cluster_id, node):
    """Get available templates for VM/CT creation"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    templates = manager.get_templates(node)
    return jsonify(templates)


@app.route('/api/clusters/<cluster_id>/nodes/<node>/qemu', methods=['POST'])
@require_auth(perms=["vm.create"])
def create_vm_api(cluster_id, node):
    """Create a new VM on a node"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    vm_config = request.json or {}
    
    result = manager.create_vm(node, vm_config)
    
    if result.get('success'):
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'unknown')
        vmid = vm_config.get('vmid') or result.get('data', {}).get('vmid', 'unknown')
        vm_name = vm_config.get('name', f'vm-{vmid}')
        log_audit(user, 'vm.create', f"Created VM {vmid} ({vm_name}) on {node}", cluster=manager.config.name)
        
        # Broadcast to all clients
        broadcast_action('create', 'qemu', str(vmid), {'node': node, 'name': vm_name}, cluster_id, user)
        
        return jsonify(result)
    else:
        return jsonify(result), 400


@app.route('/api/clusters/<cluster_id>/nodes/<node>/lxc', methods=['POST'])
@require_auth(perms=["vm.create"])
def create_container_api(cluster_id, node):
    """Create a new container on a node"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    ct_config = request.json or {}
    
    result = manager.create_container(node, ct_config)
    
    if result.get('success'):
        # Audit log
        user = getattr(request, 'session', {}).get('user', 'unknown')
        vmid = ct_config.get('vmid') or result.get('data', {}).get('vmid', 'unknown')
        ct_name = ct_config.get('hostname', f'ct-{vmid}')
        log_audit(user, 'container.create', f"Created CT {vmid} ({ct_name}) on {node}", cluster=manager.config.name)
        
        # Broadcast to all clients
        broadcast_action('create', 'lxc', str(vmid), {'node': node, 'name': ct_name}, cluster_id, user)
        
        return jsonify(result)
    else:
        return jsonify(result), 400


# ==================== NODE MANAGEMENT API ENDPOINTS ====================

@app.route('/api/clusters/<cluster_id>/nodes/<node>/summary', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_summary_api(cluster_id, node):
    """Get node summary"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    return jsonify(manager.get_node_summary(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/ip', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_ip_api(cluster_id, node):
    """Get node IP address for SSH connections
    
    MK: Tries multiple methods to get the node's IP:
    1. Cluster status API (for clustered nodes)
    2. Network configuration API
    3. Fallback to cluster host
    """
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    cluster_host = mgr.current_host or mgr.config.host
    node_ip = None
    source = None
    
    try:
        host = cluster_host
        
        # Method 1: Cluster status API (has IPs for clustered nodes)
        status_url = f"https://{host}:8006/api2/json/cluster/status"
        r = mgr._create_session().get(status_url, timeout=10)
        
        if r.status_code == 200:
            for item in r.json().get('data', []):
                if item.get('type') == 'node':
                    item_name = item.get('name', '')
                    if item_name.lower() == node.lower() and item.get('ip'):
                        node_ip = item.get('ip')
                        source = 'cluster_status'
                        break
        
        # Method 2: Network configuration API
        if not node_ip:
            net_url = f"https://{host}:8006/api2/json/nodes/{node}/network"
            r = mgr._create_session().get(net_url, timeout=5)
            if r.status_code == 200:
                for iface in r.json().get('data', []):
                    iface_type = iface.get('type', '')
                    addr = iface.get('address', '')
                    cidr = iface.get('cidr', '')
                    
                    if not addr and cidr:
                        addr = cidr.split('/')[0]
                    
                    # Look for bridge or main interface with IP
                    if addr and iface_type in ['bridge', 'eth', 'bond', 'OVSBridge', 'vlan']:
                        node_ip = addr
                        source = f'network_{iface.get("iface", "unknown")}'
                        break
        
        # Method 3: Fallback to cluster host
        if not node_ip:
            node_ip = cluster_host
            source = 'cluster_host_fallback'
            
    except Exception as e:
        logging.error(f"Error getting node IP: {e}")
        node_ip = cluster_host
        source = 'error_fallback'
    
    return jsonify({
        'ip': node_ip,
        'node': node,
        'source': source
    })


@app.route('/api/clusters/<cluster_id>/nodes/<node>/rrddata', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_rrddata_api(cluster_id, node):
    """Get node performance metrics (RRD data) for charts
    
    Query params:
    - timeframe: hour, day, week, month, year (default: hour)
    """
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    timeframe = request.args.get('timeframe', 'hour')
    return jsonify(manager.get_node_rrddata(node, timeframe))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/network', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_network_api(cluster_id, node):
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    return jsonify(manager.get_node_network_config(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/network/<iface>', methods=['PUT'])
@require_auth(perms=['node.network'])
def update_node_network_api(cluster_id, node, iface):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    result = mgr.update_node_network(node, iface, request.json or {})
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/network', methods=['POST'])
@require_auth(perms=['node.network'])
def create_node_network_api(cluster_id, node):
    """Create a new network interface"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    iface = data.get('iface', '')
    iface_type = data.get('type', 'bridge')
    
    if not iface:
        return jsonify({'error': 'Interface name required'}), 400
    
    config = {k: v for k, v in data.items() if k not in ['iface', 'type']}
    result = mgr.create_node_network(node, iface, iface_type, config)
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/network/<iface>', methods=['DELETE'])
@require_auth(perms=['node.network'])
def delete_node_network_api(cluster_id, node, iface):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    result = mgr.delete_node_network(node, iface)
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/network', methods=['PUT'])
@require_auth(perms=['node.network'])
def apply_node_network_api(cluster_id, node):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    result = mgr.apply_node_network(node)
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/network', methods=['DELETE'])
@require_auth(perms=['node.network'])
def revert_node_network_api(cluster_id, node):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    result = mgr.revert_node_network(node)
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/dns', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_dns_api(cluster_id, node):
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    return jsonify(manager.get_node_dns(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/dns', methods=['PUT'])
@require_auth(perms=['node.network'])
def update_node_dns_api(cluster_id, node):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    result = mgr.update_node_dns(node, request.json or {})
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/hosts', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_hosts_api(cluster_id, node):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    return jsonify({'data': cluster_managers[cluster_id].get_node_hosts(node)})


@app.route('/api/clusters/<cluster_id>/nodes/<node>/hosts', methods=['POST'])
@require_auth(perms=['node.network'])
def update_node_hosts_api(cluster_id, node):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    result = mgr.update_node_hosts(node, data.get('data', ''))
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/time', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_time_api(cluster_id, node):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    return jsonify(cluster_managers[cluster_id].get_node_time(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/time', methods=['PUT'])
@require_auth(perms=['node.network'])
def update_node_time_api(cluster_id, node):
    """Update node timezone"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    result = mgr.update_node_time(node, data.get('timezone', 'UTC'))
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/syslog', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_syslog_api(cluster_id, node):
    """Get node system log"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    start = request.args.get('start', 0, type=int)
    limit = request.args.get('limit', 500, type=int)
    return jsonify(manager.get_node_syslog(node, start, limit))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/certificates', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_certificates_api(cluster_id, node):
    """Get node certificates"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    return jsonify(manager.get_node_certificates(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/certificates/renew', methods=['POST'])
@require_auth(perms=['node.network'])
def renew_node_certificate_api(cluster_id, node):
    """Renew node certificate"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    force = request.json.get('force', False) if request.json else False
    result = manager.renew_node_certificate(node, force)
    
    if result['success']:
        return jsonify(result)
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/certificates/custom', methods=['POST'])
@require_auth(perms=['node.network'])
def upload_node_certificate_api(cluster_id, node):
    """Upload custom certificate to node"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    certificates = data.get('certificates', '')
    key = data.get('key', '')
    restart = data.get('restart', True)
    force = data.get('force', False)
    
    if not certificates or not key:
        return jsonify({'error': 'Certificate and key are required'}), 400
    
    result = manager.upload_node_certificate(node, certificates, key, restart, force)
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/certificates/custom', methods=['DELETE'])
@require_auth(perms=['node.network'])
def delete_node_certificate_api(cluster_id, node):
    """Delete custom certificate from node"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    restart = request.args.get('restart', 'true').lower() == 'true'
    result = mgr.delete_node_certificate(node, restart)
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/replication', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_node_replication_api(cluster_id, node):
    """Get replication jobs for node"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    return jsonify(manager.get_node_replication(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/tasks', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_tasks_api(cluster_id, node):
    """Get task history for node, optionally filtered by vmid"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    start = request.args.get('start', 0, type=int)
    limit = request.args.get('limit', 50, type=int)
    errors = request.args.get('errors', 'false').lower() == 'true'
    vmid = request.args.get('vmid', None, type=int)
    
    tasks = manager.get_node_tasks(node, start, limit * 3 if vmid else limit, errors)  # Get more if filtering
    
    # Filter by vmid if specified
    if vmid and tasks:
        filtered = [t for t in tasks if t.get('id') == str(vmid) or str(vmid) in str(t.get('upid', ''))]
        return jsonify(filtered[:limit])
    
    return jsonify(tasks)


@app.route('/api/clusters/<cluster_id>/nodes/<node>/tasks/<path:upid>/log', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_task_log_api(cluster_id, node, upid):
    """Get log for a specific task
    
    NS: Fixed Dec 2025 - frontend expects { log: "..." } format
    """
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    start = request.args.get('start', 0, type=int)
    limit = request.args.get('limit', 500, type=int)
    
    log_lines = manager.get_node_task_log(node, upid, start, limit)
    # Join lines into a single string for display
    log_text = '\n'.join(log_lines) if log_lines else ''
    
    return jsonify({'log': log_text})


@app.route('/api/clusters/<cluster_id>/nodes/<node>/subscription', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_subscription_api(cluster_id, node):
    """Get node subscription status"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    return jsonify(manager.get_node_subscription(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/subscription', methods=['PUT'])
@require_auth(perms=['admin.settings'])
def update_node_subscription_api(cluster_id, node):
    """Update subscription key - admin only"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.json or {}
    result = mgr.update_node_subscription(node, data.get('key', ''))
    
    if result['success']:
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'subscription.updated', f"Subscription key updated for {node}", cluster=mgr.config.name)
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


# =============================================================================
# SMBIOS Auto-Configurator Feature
# MK: Automatically sets SMBIOS data on new VMs for Windows licensing etc.
# MK: this was surprisingly tricky to get right, proxmox smbios format is picky
# =============================================================================

SMBIOS_SCRIPT_TEMPLATE = '''#!/usr/bin/env python3
"""
SMBIOS Auto-Configurator for Proxmox VE
Deployed by PegaProx - automatically configures SMBIOS for new VMs

Runs as a systemd service, monitors for new VMs and sets SMBIOS data.
"""

import subprocess
import time
import os
import random
from datetime import datetime

# Configuration - set by PegaProx when deployed
MANUFACTURER = "{manufacturer}"
PRODUCT = "{product}"
VERSION = "{version}"
FAMILY = "{family}"

# Paths
LOG_FILE = "/var/log/pegaprox-smbios.log"
PROCESSED_VMS_FILE = "/var/lib/pegaprox-smbios-processed.txt"  # keeps track of what we already did

def log_message(message):
    """write to log file, nothing fancy"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{{timestamp}}] {{message}}"
    print(log_entry)
    try:
        with open(LOG_FILE, 'a') as f:
            f.write(log_entry + "\\n")
    except:
        pass  # if we cant log, oh well

def get_all_vms():
    """get vmids from qm list"""
    try:
        result = subprocess.run(['qm', 'list'], capture_output=True, text=True)
        vms = []
        for line in result.stdout.splitlines()[1:]:  # skip header
            parts = line.split()
            if parts:
                vms.append(parts[0])
        return vms
    except Exception as e:
        log_message(f"Error fetching VM list: {{e}}")
        return []

def load_processed_vms():
    """Load already processed VMs"""
    if os.path.exists(PROCESSED_VMS_FILE):
        with open(PROCESSED_VMS_FILE, 'r') as f:
            return set(f.read().splitlines())
    return set()

def save_processed_vm(vmid):
    """Mark VM as processed"""
    with open(PROCESSED_VMS_FILE, 'a') as f:
        f.write(f"{{vmid}}\\n")

def get_current_smbios(vmid):
    """Get current SMBIOS settings"""
    try:
        result = subprocess.run(['qm', 'config', vmid], capture_output=True, text=True)
        for line in result.stdout.splitlines():
            if line.startswith('smbios1:'):
                return line.split(':', 1)[1].strip()
        return None
    except:
        return None

def parse_smbios_string(smbios_str):
    """Parse SMBIOS string into dictionary"""
    params = {{}}
    if not smbios_str:
        return params
    for part in smbios_str.split(','):
        if '=' in part:
            key, value = part.split('=', 1)
            params[key.strip()] = value.strip()
    return params

def needs_smbios_update(vmid):
    """Check if VM needs SMBIOS configuration"""
    smbios_str = get_current_smbios(vmid)
    if not smbios_str:
        return True
    
    params = parse_smbios_string(smbios_str)
    relevant_params = {{k: v for k, v in params.items() if k != 'uuid'}}
    
    if not relevant_params:
        return True
    
    if ('manufacturer' in params or 'product' in params or 
        'version' in params or 'serial' in params or 'family' in params):
        log_message(f"VM {{vmid}} already has SMBIOS configuration")
        return False
    
    return True

def generate_unique_serial():
    """Generate unique serial number"""
    timestamp = datetime.now().strftime("%y%m%d%H%M%S")
    random_part = random.randint(1000, 9999)
    return f"PVE{{timestamp}}{{random_part}}"

def set_smbios(vmid):
    """Set SMBIOS configuration"""
    current_smbios_str = get_current_smbios(vmid)
    current_params = parse_smbios_string(current_smbios_str) if current_smbios_str else {{}}
    uuid_value = current_params.get('uuid', '')
    serial = generate_unique_serial()
    
    smbios_parts = [
        f"manufacturer={{MANUFACTURER}}",
        f"product={{PRODUCT}}",
        f"version={{VERSION}}",
        f"serial={{serial}}",
    ]
    if uuid_value:
        smbios_parts.append(f"uuid={{uuid_value}}")
    smbios_parts.append(f"family={{FAMILY}}")
    
    smbios_string = ",".join(smbios_parts)
    cmd = ['qm', 'set', vmid, '-smbios1', smbios_string]
    
    try:
        subprocess.run(cmd, capture_output=True, text=True, check=True)
        log_message(f"Set SMBIOS for VM {{vmid}} | Serial: {{serial}}")
        return True
    except subprocess.CalledProcessError as e:
        log_message(f"Error setting SMBIOS for VM {{vmid}}: {{e.stderr if e.stderr else e}}")
        return False

def check_vm_exists(vmid):
    """Check if VM exists"""
    try:
        result = subprocess.run(['qm', 'status', vmid], capture_output=True, text=True)
        return result.returncode == 0
    except:
        return False

def cleanup_processed_list(processed):
    """Remove VMs that no longer exist"""
    current_vms = set(get_all_vms())
    removed_vms = processed - current_vms
    
    if removed_vms:
        for vmid in removed_vms:
            log_message(f"VM {{vmid}} no longer exists, removing from tracking")
            processed.remove(vmid)
        with open(PROCESSED_VMS_FILE, 'w') as f:
            for vmid in processed:
                f.write(f"{{vmid}}\\n")
    return processed

def main():
    log_message("=== PegaProx SMBIOS Auto-Configurator started ===")
    log_message(f"Config: {{MANUFACTURER}} | {{PRODUCT}} | {{VERSION}} | {{FAMILY}}")
    
    processed = load_processed_vms()
    cleanup_counter = 0
    
    while True:
        try:
            cleanup_counter += 1
            if cleanup_counter >= 30:
                processed = cleanup_processed_list(processed)
                cleanup_counter = 0
            
            current_vms = get_all_vms()
            
            for vmid in current_vms:
                if vmid not in processed:
                    if check_vm_exists(vmid):
                        if needs_smbios_update(vmid):
                            log_message(f"Configuring SMBIOS for new VM {{vmid}}")
                            if set_smbios(vmid):
                                save_processed_vm(vmid)
                                processed.add(vmid)
                        else:
                            save_processed_vm(vmid)
                            processed.add(vmid)
            
            time.sleep(2)
            
        except KeyboardInterrupt:
            log_message("=== SMBIOS Auto-Configurator stopped ===")
            break
        except Exception as e:
            log_message(f"Error: {{e}}")
            time.sleep(10)

if __name__ == "__main__":
    main()
'''

SMBIOS_SERVICE_TEMPLATE = '''[Unit]
Description=PegaProx SMBIOS Auto-Configurator
After=pve-cluster.service
Wants=pve-cluster.service

[Service]
Type=simple
ExecStart=/usr/bin/python3 /opt/pegaprox-smbios-autoconfig.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
'''

@app.route('/api/clusters/<cluster_id>/smbios-autoconfig', methods=['GET'])
@require_auth(perms=['node.view'])
def get_smbios_autoconfig(cluster_id):
    """get smbios settings for the cluster, returns defaults if not configured yet"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    try:
        if cluster_id not in cluster_managers:
            return jsonify({'error': 'Cluster not found'}), 404
        
        # defaults if nothing configured - NS: proxmox doesnt allow underscores so no spaces either
        mgr = cluster_managers[cluster_id]
        settings = getattr(mgr.config, 'smbios_autoconfig', None) or {
            'enabled': False,
            'manufacturer': 'Proxmox',
            'product': 'PegaProxManagment',
            'version': 'v1',
            'family': 'ProxmoxVE'
        }
        
        return jsonify(settings)
    except Exception as e:
        logging.error(f"Error getting SMBIOS config: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/smbios-autoconfig', methods=['PUT'])
@require_auth(perms=['admin.settings'])
def update_smbios_autoconfig(cluster_id):
    """save smbios settings - gets deployed to nodes when they click deploy"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    data = request.json or {}
    mgr = cluster_managers[cluster_id]
    
    # Update settings
    mgr.config.smbios_autoconfig = {
        'enabled': data.get('enabled', False),
        'manufacturer': data.get('manufacturer', 'Proxmox'),
        'product': data.get('product', 'Virtual Machine'),
        'version': data.get('version', 'PVE8'),
        'family': data.get('family', 'ProxmoxVE')
    }
    
    # Save to database
    db = get_db()
    db.update_cluster(cluster_id, {'smbios_autoconfig': json.dumps(mgr.config.smbios_autoconfig)})
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    log_audit(usr, 'smbios_autoconfig.updated', f"SMBIOS auto-config updated", cluster=mgr.config.name)
    
    return jsonify({'success': True, 'message': 'Settings saved'})


@app.route('/api/clusters/<cluster_id>/nodes/<node>/smbios-autoconfig/status', methods=['GET'])
@require_auth(perms=['node.view'])
def get_smbios_autoconfig_status(cluster_id, node):
    """Check if SMBIOS auto-config service is running on node"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    try:
        # Get node IP and connect via SSH
        # For single-node, use cluster host; for multi-node, resolve from cluster status
        node_ip = mgr.current_host or mgr.config.host
        
        # Try to get actual node IP from cluster status
        try:
            status_url = f"https://{node_ip}:8006/api2/json/cluster/status"
            r = mgr._create_session().get(status_url, timeout=10)
            if r.status_code == 200:
                for item in r.json().get('data', []):
                    if item.get('type') == 'node' and item.get('name', '').lower() == node.lower():
                        if item.get('ip'):
                            node_ip = item.get('ip')
                            break
        except:
            pass
        
        ssh = mgr._ssh_connect(node_ip)
        if not ssh:
            return jsonify({'installed': False, 'running': False, 'error': 'SSH not available - check SSH key in cluster settings'})
        
        # Check if script exists
        stdin, stdout, stderr = ssh.exec_command('test -f /opt/pegaprox-smbios-autoconfig.py && echo exists')
        installed = 'exists' in stdout.read().decode()
        
        # Check if service is running
        stdin, stdout, stderr = ssh.exec_command('systemctl is-active pegaprox-smbios-autoconfig 2>/dev/null || echo inactive')
        status = stdout.read().decode().strip()
        running = status == 'active'
        
        # Get last log entries
        stdin, stdout, stderr = ssh.exec_command('tail -5 /var/log/pegaprox-smbios.log 2>/dev/null || echo "No logs yet"')
        logs = stdout.read().decode().strip()
        
        ssh.close()
        
        return jsonify({
            'installed': installed,
            'running': running,
            'status': status,
            'logs': logs
        })
        
    except Exception as e:
        logging.error(f"Error checking SMBIOS autoconfig status: {e}")
        return jsonify({'installed': False, 'running': False, 'error': str(e)})


@app.route('/api/clusters/<cluster_id>/nodes/<node>/smbios-autoconfig/deploy', methods=['POST'])
@require_auth(perms=['admin.settings'])
def deploy_smbios_autoconfig(cluster_id, node):
    """Deploy SMBIOS auto-config script to node"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    settings = getattr(mgr.config, 'smbios_autoconfig', None) or {}
    
    try:
        # Get node IP
        node_ip = mgr.current_host or mgr.config.host
        try:
            status_url = f"https://{node_ip}:8006/api2/json/cluster/status"
            r = mgr._create_session().get(status_url, timeout=10)
            if r.status_code == 200:
                for item in r.json().get('data', []):
                    if item.get('type') == 'node' and item.get('name', '').lower() == node.lower():
                        if item.get('ip'):
                            node_ip = item.get('ip')
                            break
        except:
            pass
        
        ssh = mgr._ssh_connect(node_ip)
        if not ssh:
            return jsonify({'error': 'SSH connection failed - check SSH key in cluster settings'}), 500
        
        # Generate script with settings
        script = SMBIOS_SCRIPT_TEMPLATE.format(
            manufacturer=settings.get('manufacturer', 'Proxmox'),
            product=settings.get('product', 'PegaProxManagment'),
            version=settings.get('version', 'v1'),
            family=settings.get('family', 'ProxmoxVE')
        )
        
        # Write script to node
        sftp = ssh.open_sftp()
        with sftp.file('/opt/pegaprox-smbios-autoconfig.py', 'w') as f:
            f.write(script)
        sftp.chmod('/opt/pegaprox-smbios-autoconfig.py', 0o755)
        
        # Write systemd service
        with sftp.file('/etc/systemd/system/pegaprox-smbios-autoconfig.service', 'w') as f:
            f.write(SMBIOS_SERVICE_TEMPLATE)
        
        sftp.close()
        
        # Enable and start service
        # NS: clear processed list so ALL vms get checked (not just new ones)
        commands = [
            'rm -f /var/lib/pegaprox-smbios-processed.txt',
            'systemctl daemon-reload',
            'systemctl enable pegaprox-smbios-autoconfig',
            'systemctl restart pegaprox-smbios-autoconfig'
        ]
        
        for cmd in commands:
            stdin, stdout, stderr = ssh.exec_command(cmd)
            stdout.read()  # Wait for completion
        
        ssh.close()
        
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'smbios_autoconfig.deployed', f"SMBIOS auto-config deployed to {node}", cluster=mgr.config.name)
        
        return jsonify({'success': True, 'message': f'SMBIOS Auto-Config deployed to {node}'})
        
    except Exception as e:
        logging.error(f"Error deploying SMBIOS autoconfig: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/smbios-autoconfig', methods=['DELETE'])
@require_auth(perms=['admin.settings'])
def remove_smbios_autoconfig(cluster_id, node):
    """Remove SMBIOS auto-config from node"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    try:
        # Get node IP
        node_ip = mgr.current_host or mgr.config.host
        try:
            status_url = f"https://{node_ip}:8006/api2/json/cluster/status"
            r = mgr._create_session().get(status_url, timeout=10)
            if r.status_code == 200:
                for item in r.json().get('data', []):
                    if item.get('type') == 'node' and item.get('name', '').lower() == node.lower():
                        if item.get('ip'):
                            node_ip = item.get('ip')
                            break
        except:
            pass
        
        ssh = mgr._ssh_connect(node_ip)
        if not ssh:
            return jsonify({'error': 'SSH connection failed - check SSH key in cluster settings'}), 500
        
        # Stop and disable service, remove files
        commands = [
            'systemctl stop pegaprox-smbios-autoconfig 2>/dev/null || true',
            'systemctl disable pegaprox-smbios-autoconfig 2>/dev/null || true',
            'rm -f /etc/systemd/system/pegaprox-smbios-autoconfig.service',
            'rm -f /opt/pegaprox-smbios-autoconfig.py',
            'rm -f /var/lib/pegaprox-smbios-processed.txt',
            'systemctl daemon-reload'
        ]
        
        for cmd in commands:
            stdin, stdout, stderr = ssh.exec_command(cmd)
            stdout.read()
        
        ssh.close()
        
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, 'smbios_autoconfig.removed', f"SMBIOS auto-config removed from {node}", cluster=mgr.config.name)
        
        return jsonify({'success': True, 'message': f'SMBIOS Auto-Config removed from {node}'})
        
    except Exception as e:
        logging.error(f"Error removing SMBIOS autoconfig: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/smbios-autoconfig/control', methods=['POST'])
@require_auth(perms=['admin.settings'])
def control_smbios_autoconfig(cluster_id, node):
    """Start/Stop/Rescan SMBIOS auto-config service on node"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    data = request.get_json() or {}
    action = data.get('action')  # 'start', 'stop', 'restart', 'rescan'
    
    if action not in ['start', 'stop', 'restart', 'rescan']:
        return jsonify({'error': 'Invalid action. Use start, stop, restart, or rescan'}), 400
    
    mgr = cluster_managers[cluster_id]
    
    try:
        # Get node IP
        node_ip = mgr.current_host or mgr.config.host
        try:
            status_url = f"https://{node_ip}:8006/api2/json/cluster/status"
            r = mgr._create_session().get(status_url, timeout=10)
            if r.status_code == 200:
                for item in r.json().get('data', []):
                    if item.get('type') == 'node' and item.get('name', '').lower() == node.lower():
                        if item.get('ip'):
                            node_ip = item.get('ip')
                            break
        except:
            pass
        
        ssh = mgr._ssh_connect(node_ip)
        if not ssh:
            return jsonify({'error': 'SSH connection failed'}), 500
        
        # NS: rescan = nuke the processed list and restart, forces re-check of all VMs
        if action == 'rescan':
            cmd = 'rm -f /var/lib/pegaprox-smbios-processed.txt && systemctl restart pegaprox-smbios-autoconfig'
        else:
            cmd = f'systemctl {action} pegaprox-smbios-autoconfig'
        
        stdin, stdout, stderr = ssh.exec_command(cmd)
        stdout.read()
        err_output = stderr.read().decode()
        
        ssh.close()
        
        if err_output and 'not found' in err_output.lower():
            return jsonify({'error': 'Service not installed on this node'}), 404
        
        usr = getattr(request, 'session', {}).get('user', 'system')
        log_audit(usr, f'smbios_autoconfig.{action}', f"SMBIOS auto-config {action} on {node}", cluster=mgr.config.name)
        
        return jsonify({'success': True, 'message': f'Service {action}ed on {node}'})
        
    except Exception as e:
        logging.error(f"Error controlling SMBIOS autoconfig: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/smbios-autoconfig/status-all', methods=['GET'])
@require_auth(perms=['node.view'])
def get_smbios_autoconfig_status_all(cluster_id):
    """Get SMBIOS auto-config status for ALL nodes in cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    try:
        # Get all nodes - try different methods
        try:
            node_status = mgr.get_node_status()
            node_names = list(node_status.keys()) if node_status else []
        except:
            try:
                nodes = mgr.get_nodes()
                node_names = [n.get('node', n.get('name', '')) for n in nodes if n]
            except:
                node_names = []
        
        if not node_names:
            return jsonify({'error': 'No nodes available'}), 400
        
        results = {}
        
        for node_name in node_names:
            if not node_name:
                continue
                
            try:
                # Get node IP
                node_ip = mgr._get_node_ip(node_name)
                if not node_ip:
                    results[node_name] = {'installed': False, 'running': False, 'error': 'Could not determine node IP'}
                    continue
                
                ssh = mgr._ssh_connect(node_ip)
                if not ssh:
                    results[node_name] = {'installed': False, 'running': False, 'error': 'SSH not available'}
                    continue
                
                try:
                    # Check if script exists
                    stdin, stdout, stderr = ssh.exec_command('test -f /opt/pegaprox-smbios-autoconfig.py && echo exists')
                    installed = 'exists' in stdout.read().decode()
                    
                    # Check if service is running
                    stdin, stdout, stderr = ssh.exec_command('systemctl is-active pegaprox-smbios-autoconfig 2>/dev/null || echo inactive')
                    status = stdout.read().decode().strip()
                    running = status == 'active'
                    
                    results[node_name] = {
                        'installed': installed,
                        'running': running,
                        'status': status
                    }
                finally:
                    ssh.close()
                    
            except Exception as e:
                results[node_name] = {'installed': False, 'running': False, 'error': str(e)}
        
        return jsonify(results)
        
    except Exception as e:
        logging.error(f"Error getting SMBIOS autoconfig status: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/smbios-autoconfig/deploy-all', methods=['POST'])
@require_auth(perms=['admin.settings'])
def deploy_smbios_autoconfig_all(cluster_id):
    """Deploy SMBIOS auto-config script to ALL nodes in cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    settings = getattr(mgr.config, 'smbios_autoconfig', None) or {}
    
    # Get all nodes in cluster
    nodes = []
    node_ips = {}
    try:
        cluster_host = mgr.current_host or mgr.config.host
        status_url = f"https://{cluster_host}:8006/api2/json/cluster/status"
        r = mgr._create_session().get(status_url, timeout=10)
        if r.status_code == 200:
            for item in r.json().get('data', []):
                if item.get('type') == 'node':
                    node_name = item.get('name')
                    nodes.append(node_name)
                    node_ips[node_name] = item.get('ip') or cluster_host
        else:
            # Single node cluster - just use cluster host
            nodes = [mgr.config.host.split('.')[0]]
            node_ips[nodes[0]] = cluster_host
    except Exception as e:
        logging.error(f"Error getting cluster nodes: {e}")
        return jsonify({'error': f'Could not get cluster nodes: {e}'}), 500
    
    if not nodes:
        return jsonify({'error': 'No nodes found in cluster'}), 404
    
    results = []
    script = SMBIOS_SCRIPT_TEMPLATE.format(
        manufacturer=settings.get('manufacturer', 'Proxmox'),
        product=settings.get('product', 'PegaProxManagment'),
        version=settings.get('version', 'v1'),
        family=settings.get('family', 'ProxmoxVE')
    )
    
    for node in nodes:
        node_ip = node_ips.get(node, mgr.config.host)
        try:
            # NS: Staggered connections to prevent SSH server overload
            if results:  # Not the first node
                time.sleep(1.0)
            
            ssh = mgr._ssh_connect(node_ip)
            if not ssh:
                results.append({'node': node, 'success': False, 'error': 'SSH connection failed'})
                continue
            
            # Write script
            sftp = ssh.open_sftp()
            with sftp.file('/opt/pegaprox-smbios-autoconfig.py', 'w') as f:
                f.write(script)
            sftp.chmod('/opt/pegaprox-smbios-autoconfig.py', 0o755)
            
            # Write systemd service
            with sftp.file('/etc/systemd/system/pegaprox-smbios-autoconfig.service', 'w') as f:
                f.write(SMBIOS_SERVICE_TEMPLATE)
            sftp.close()
            
            # Enable and start service
            # NS: clear processed list so ALL vms get checked
            for cmd in ['rm -f /var/lib/pegaprox-smbios-processed.txt', 'systemctl daemon-reload', 'systemctl enable pegaprox-smbios-autoconfig', 'systemctl restart pegaprox-smbios-autoconfig']:
                stdin, stdout, stderr = ssh.exec_command(cmd)
                stdout.read()
            
            ssh.close()
            results.append({'node': node, 'success': True})
            
        except Exception as e:
            results.append({'node': node, 'success': False, 'error': str(e)})
    
    success_count = sum(1 for r in results if r['success'])
    usr = getattr(request, 'session', {}).get('user', 'system')
    log_audit(usr, 'smbios_autoconfig.deployed_all', f"SMBIOS auto-config deployed to {success_count}/{len(nodes)} nodes", cluster=mgr.config.name)
    
    return jsonify({
        'success': success_count == len(nodes),
        'message': f'Deployed to {success_count}/{len(nodes)} nodes',
        'results': results
    })


# =============================================================================
# Custom Scripts Feature
# MK: Run custom .sh/.py scripts on cluster nodes with permission control
# =============================================================================

@app.route('/api/clusters/<cluster_id>/scripts', methods=['GET'])
@require_auth(perms=['admin.scripts'])
def get_custom_scripts(cluster_id):
    """Get all custom scripts for a cluster (excludes soft-deleted)"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    try:
        db = get_db()
        # Ensure table exists with soft delete support
        db.execute('''
            CREATE TABLE IF NOT EXISTS custom_scripts (
                id TEXT PRIMARY KEY,
                cluster_id TEXT NOT NULL,
                name TEXT NOT NULL,
                description TEXT DEFAULT '',
                type TEXT DEFAULT 'bash',
                content TEXT NOT NULL,
                target_nodes TEXT DEFAULT 'all',
                enabled INTEGER DEFAULT 1,
                last_run TEXT,
                last_status TEXT,
                last_output TEXT,
                created_at TEXT,
                updated_at TEXT,
                created_by TEXT,
                deleted_at TEXT,
                deleted_by TEXT
            )
        ''')
        # Add columns if they don't exist (migration for existing tables)
        try:
            db.execute('ALTER TABLE custom_scripts ADD COLUMN deleted_at TEXT')
        except: pass
        try:
            db.execute('ALTER TABLE custom_scripts ADD COLUMN deleted_by TEXT')
        except: pass
        try:
            db.execute('ALTER TABLE custom_scripts ADD COLUMN created_by TEXT')
        except: pass
        try:
            db.execute('ALTER TABLE custom_scripts ADD COLUMN last_output TEXT')
        except: pass
        
        # Only return non-deleted scripts
        scripts = db.query(
            'SELECT * FROM custom_scripts WHERE cluster_id = ? AND deleted_at IS NULL ORDER BY name',
            (cluster_id,)
        )
        return jsonify([dict(s) for s in scripts] if scripts else [])
    except Exception as e:
        app.logger.error(f"Error loading scripts: {e}")
        return jsonify({'error': str(e)}), 500


# Cleanup job for permanently deleting scripts after 20 days
def cleanup_deleted_scripts():
    """Permanently delete scripts that have been soft-deleted for 20+ days"""
    try:
        db = get_db()
        cutoff = (datetime.now() - timedelta(days=20)).isoformat()
        deleted = db.query(
            'SELECT id, name, cluster_id, deleted_by FROM custom_scripts WHERE deleted_at IS NOT NULL AND deleted_at < ?',
            (cutoff,)
        )
        for script in deleted:
            db.execute('DELETE FROM custom_scripts WHERE id = ?', (script['id'],))
            log_audit('system', 'script.purged', f"Permanently deleted script '{script['name']}' after 20-day retention", cluster=script['cluster_id'])
        if deleted:
            logging.info(f"Purged {len(deleted)} scripts after 20-day retention period")
    except Exception as e:
        logging.error(f"Error cleaning up deleted scripts: {e}")


def cleanup_orphaned_excluded_vms():
    """Remove excluded VM entries for VMs that no longer exist
    
    MK: This runs daily to clean up stale entries from balancing_excluded_vms
    when VMs are deleted through other means (e.g. directly in Proxmox UI)
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        # Get all excluded VM entries
        cursor.execute('SELECT cluster_id, vmid FROM balancing_excluded_vms')
        excluded_entries = cursor.fetchall()
        
        if not excluded_entries:
            return
        
        removed_count = 0
        
        for entry in excluded_entries:
            cluster_id = entry['cluster_id']
            vmid = entry['vmid']
            
            # Check if cluster still exists and is connected
            if cluster_id not in cluster_managers:
                # Cluster no longer exists, remove entry
                cursor.execute(
                    'DELETE FROM balancing_excluded_vms WHERE cluster_id = ? AND vmid = ?',
                    (cluster_id, vmid)
                )
                removed_count += 1
                continue
            
            mgr = cluster_managers[cluster_id]
            if not mgr.is_connected:
                continue  # Skip if we can't verify
            
            # Check if VM still exists
            try:
                vms = mgr.get_vm_resources()
                vm_exists = any(vm.get('vmid') == vmid for vm in vms)
                
                if not vm_exists:
                    cursor.execute(
                        'DELETE FROM balancing_excluded_vms WHERE cluster_id = ? AND vmid = ?',
                        (cluster_id, vmid)
                    )
                    removed_count += 1
                    logging.info(f"Removed orphaned excluded VM entry: cluster={cluster_id}, vmid={vmid}")
            except Exception as e:
                logging.debug(f"Could not verify VM {vmid} in cluster {cluster_id}: {e}")
        
        if removed_count > 0:
            db.conn.commit()
            logging.info(f"[CLEANUP] Removed {removed_count} orphaned excluded VM entries")
            
    except Exception as e:
        logging.error(f"Error cleaning up orphaned excluded VMs: {e}")


@app.route('/api/clusters/<cluster_id>/scripts', methods=['POST'])
@require_auth(perms=['admin.scripts'])
def create_custom_script(cluster_id):
    """Create a new custom script - requires admin.scripts permission"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    data = request.json or {}
    
    if not data.get('name') or not data.get('content'):
        return jsonify({'error': 'Name and content required'}), 400
    
    script_type = data.get('type', 'bash')
    if script_type not in ['bash', 'python']:
        return jsonify({'error': 'Type must be bash or python'}), 400
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    db = get_db()
    script_id = str(uuid.uuid4())[:8]
    
    db.execute('''
        INSERT INTO custom_scripts (id, cluster_id, name, description, type, content, target_nodes, enabled, created_at, created_by)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        script_id,
        cluster_id,
        data.get('name'),
        data.get('description', ''),
        script_type,
        data.get('content'),
        data.get('target_nodes', 'all'),
        1 if data.get('enabled', True) else 0,
        datetime.now().isoformat(),
        usr
    ))
    
    # Get cluster name for audit log
    cluster_name = cluster_managers.get(cluster_id, {})
    if hasattr(cluster_name, 'config'):
        cluster_name = cluster_name.config.name
    else:
        cluster_name = cluster_id
    
    log_audit(usr, 'script.created', f"Created script '{data.get('name')}' (ID: {script_id}, Type: {script_type})", cluster=cluster_name)
    
    return jsonify({'success': True, 'id': script_id})


@app.route('/api/clusters/<cluster_id>/scripts/<script_id>', methods=['PUT'])
@require_auth(perms=['admin.scripts'])
def update_custom_script(cluster_id, script_id):
    """Update a custom script - requires admin.scripts permission"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    data = request.json or {}
    db = get_db()
    
    # Check script exists and not deleted
    script = db.query_one('SELECT * FROM custom_scripts WHERE id = ? AND cluster_id = ? AND deleted_at IS NULL', (script_id, cluster_id))
    if not script:
        return jsonify({'error': 'Script not found'}), 404
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    
    db.execute('''
        UPDATE custom_scripts SET
            name = ?,
            description = ?,
            type = ?,
            content = ?,
            target_nodes = ?,
            enabled = ?,
            updated_at = ?
        WHERE id = ? AND cluster_id = ?
    ''', (
        data.get('name', script['name']),
        data.get('description', script['description']),
        data.get('type', script['type']),
        data.get('content', script['content']),
        data.get('target_nodes', script['target_nodes']),
        1 if data.get('enabled', script['enabled']) else 0,
        datetime.now().isoformat(),
        script_id,
        cluster_id
    ))
    
    # Get cluster name for audit log
    cluster_name = cluster_managers.get(cluster_id, {})
    if hasattr(cluster_name, 'config'):
        cluster_name = cluster_name.config.name
    else:
        cluster_name = cluster_id
    
    log_audit(usr, 'script.updated', f"Updated script '{data.get('name', script['name'])}' (ID: {script_id})", cluster=cluster_name)
    
    return jsonify({'success': True})


@app.route('/api/clusters/<cluster_id>/scripts/<script_id>', methods=['DELETE'])
@require_auth(perms=['admin.scripts'])
def delete_custom_script(cluster_id, script_id):
    """Soft-delete a custom script - will be permanently deleted after 20 days"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    db = get_db()
    
    # Check script exists
    script = db.query_one('SELECT * FROM custom_scripts WHERE id = ? AND cluster_id = ? AND deleted_at IS NULL', (script_id, cluster_id))
    if not script:
        return jsonify({'error': 'Script not found'}), 404
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    
    # Soft delete - mark as deleted but keep for 20 days
    db.execute('''
        UPDATE custom_scripts SET deleted_at = ?, deleted_by = ? WHERE id = ? AND cluster_id = ?
    ''', (datetime.now().isoformat(), usr, script_id, cluster_id))
    
    # Get cluster name for audit log
    cluster_name = cluster_managers.get(cluster_id, {})
    if hasattr(cluster_name, 'config'):
        cluster_name = cluster_name.config.name
    else:
        cluster_name = cluster_id
    
    log_audit(usr, 'script.deleted', f"Soft-deleted script '{script['name']}' (ID: {script_id}) - will be purged in 20 days", cluster=cluster_name)
    
    return jsonify({'success': True, 'message': 'Script marked for deletion. Will be permanently removed in 20 days.'})


@app.route('/api/clusters/<cluster_id>/scripts/<script_id>/run', methods=['POST'])
@require_auth(perms=['admin.scripts'])
def run_custom_script(cluster_id, script_id):
    """Run a custom script on target nodes - REQUIRES PASSWORD CONFIRMATION
    
    This is a sensitive operation that executes arbitrary code on nodes.
    Password confirmation is required to prevent accidental or unauthorized execution.
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    # SECURITY: Require password confirmation before running any script
    data = request.json or {}
    password = data.get('password')
    
    if not password:
        return jsonify({'error': 'Password confirmation required to run scripts'}), 401
    
    # Verify password against current user
    usr = getattr(request, 'session', {}).get('user', 'system')
    users = load_users()
    user_data = users.get(usr)
    
    if not user_data:
        return jsonify({'error': 'User not found'}), 401
    
    # Check password
    stored_salt = user_data.get('password_salt', '')
    stored_hash = user_data.get('password_hash', '')
    if not stored_salt or not stored_hash or not verify_password(password, stored_salt, stored_hash):
        cluster_name = cluster_managers[cluster_id].config.name if cluster_id in cluster_managers else cluster_id
        log_audit(usr, 'script.run_denied', f"Failed password verification for script execution (ID: {script_id})", cluster=cluster_name)
        return jsonify({'error': 'Invalid password'}), 401
    
    db = get_db()
    script = db.query_one('SELECT * FROM custom_scripts WHERE id = ? AND cluster_id = ? AND deleted_at IS NULL', (script_id, cluster_id))
    
    if not script:
        return jsonify({'error': 'Script not found'}), 404
    
    if not script['enabled']:
        return jsonify({'error': 'Script is disabled'}), 400
    
    mgr = cluster_managers[cluster_id]
    
    # Get cluster name for audit log
    cluster_name = mgr.config.name if hasattr(mgr, 'config') else cluster_id
    
    # Get target nodes
    target_nodes = script['target_nodes']
    nodes_to_run = []
    node_ips = {}
    
    try:
        cluster_host = mgr.current_host or mgr.config.host
        status_url = f"https://{cluster_host}:8006/api2/json/cluster/status"
        r = mgr._create_session().get(status_url, timeout=10)
        if r.status_code == 200:
            for item in r.json().get('data', []):
                if item.get('type') == 'node':
                    node_name = item.get('name')
                    if target_nodes == 'all' or node_name in target_nodes.split(','):
                        nodes_to_run.append(node_name)
                        node_ips[node_name] = item.get('ip') or cluster_host
    except Exception as e:
        logging.error(f"Error getting cluster nodes: {e}")
        return jsonify({'error': f'Could not get cluster nodes: {e}'}), 500
    
    if not nodes_to_run:
        return jsonify({'error': 'No target nodes found'}), 404
    
    # Log the execution attempt BEFORE running
    log_audit(usr, 'script.execution_started', f"Starting execution of script '{script['name']}' (ID: {script_id}) on {len(nodes_to_run)} nodes: {', '.join(nodes_to_run)}", cluster=cluster_name)
    
    results = []
    script_ext = '.py' if script['type'] == 'python' else '.sh'
    interpreter = 'python3' if script['type'] == 'python' else 'bash'
    all_output = []
    
    for node in nodes_to_run:
        node_ip = node_ips.get(node, mgr.config.host)
        try:
            ssh = mgr._ssh_connect(node_ip)
            if not ssh:
                results.append({'node': node, 'success': False, 'error': 'SSH connection failed', 'output': ''})
                all_output.append(f"=== {node} ===\nSSH connection failed\n")
                continue
            
            # Upload script to temp location
            script_path = f'/tmp/pegaprox_script_{script_id}{script_ext}'
            sftp = ssh.open_sftp()
            with sftp.file(script_path, 'w') as f:
                f.write(script['content'])
            sftp.chmod(script_path, 0o755)
            sftp.close()
            
            # Run script with timeout
            stdin, stdout, stderr = ssh.exec_command(f'{interpreter} {script_path} 2>&1', timeout=300)
            output = stdout.read().decode('utf-8', errors='replace')
            exit_code = stdout.channel.recv_exit_status()
            
            # Clean up
            ssh.exec_command(f'rm -f {script_path}')
            ssh.close()
            
            all_output.append(f"=== {node} (exit: {exit_code}) ===\n{output}\n")
            
            results.append({
                'node': node,
                'success': exit_code == 0,
                'exit_code': exit_code,
                'output': output[:10000] if output else ''  # Limit output size
            })
            
        except Exception as e:
            error_msg = str(e)
            results.append({'node': node, 'success': False, 'error': error_msg, 'output': ''})
            all_output.append(f"=== {node} ===\nError: {error_msg}\n")
    
    # Update last run info with output
    success_count = sum(1 for r in results if r['success'])
    status = 'success' if success_count == len(nodes_to_run) else ('partial' if success_count > 0 else 'failed')
    combined_output = '\n'.join(all_output)[:50000]  # Limit stored output
    
    db.execute('''
        UPDATE custom_scripts SET last_run = ?, last_status = ?, last_output = ? WHERE id = ?
    ''', (datetime.now().isoformat(), status, combined_output, script_id))
    
    # Detailed audit log of execution result
    log_audit(usr, 'script.executed', f"Script '{script['name']}' completed: {success_count}/{len(nodes_to_run)} nodes succeeded ({status})", cluster=cluster_name)
    
    return jsonify({
        'success': success_count == len(nodes_to_run),
        'message': f'Ran on {success_count}/{len(nodes_to_run)} nodes',
        'status': status,
        'results': results
    })


@app.route('/api/clusters/<cluster_id>/scripts/<script_id>/output', methods=['GET'])
@require_auth(perms=['admin.scripts'])
def get_script_output(cluster_id, script_id):
    """Get the last execution output of a script"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    db = get_db()
    script = db.query_one('SELECT name, last_run, last_status, last_output FROM custom_scripts WHERE id = ? AND cluster_id = ? AND deleted_at IS NULL', (script_id, cluster_id))
    
    if not script:
        return jsonify({'error': 'Script not found'}), 404
    
    return jsonify({
        'name': script['name'],
        'last_run': script['last_run'],
        'last_status': script['last_status'],
        'output': script['last_output'] or 'No output available'
    })


@app.route('/api/clusters/<cluster_id>/scripts/deleted', methods=['GET'])
@require_auth(perms=['admin.scripts'])
def get_deleted_scripts(cluster_id):
    """Get list of soft-deleted scripts (pending permanent deletion)"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    try:
        db = get_db()
        scripts = db.query(
            '''SELECT id, name, description, type, deleted_at, deleted_by, 
               datetime(deleted_at, '+20 days') as purge_date
               FROM custom_scripts 
               WHERE cluster_id = ? AND deleted_at IS NOT NULL 
               ORDER BY deleted_at DESC''',
            (cluster_id,)
        )
        return jsonify([dict(s) for s in scripts] if scripts else [])
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/clusters/<cluster_id>/scripts/<script_id>/restore', methods=['POST'])
@require_auth(perms=['admin.scripts'])
def restore_deleted_script(cluster_id, script_id):
    """Restore a soft-deleted script before it's permanently purged"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    db = get_db()
    script = db.query_one('SELECT * FROM custom_scripts WHERE id = ? AND cluster_id = ? AND deleted_at IS NOT NULL', (script_id, cluster_id))
    
    if not script:
        return jsonify({'error': 'Deleted script not found'}), 404
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    
    db.execute('''
        UPDATE custom_scripts SET deleted_at = NULL, deleted_by = NULL WHERE id = ? AND cluster_id = ?
    ''', (script_id, cluster_id))
    
    # Get cluster name for audit log
    cluster_name = cluster_managers.get(cluster_id, {})
    if hasattr(cluster_name, 'config'):
        cluster_name = cluster_name.config.name
    else:
        cluster_name = cluster_id
    
    log_audit(usr, 'script.restored', f"Restored deleted script '{script['name']}' (ID: {script_id})", cluster=cluster_name)
    
    return jsonify({'success': True, 'message': f"Script '{script['name']}' restored"})


# =============================================================================
# CLUSTER GROUPS & RENAME - NS: Jan 2026
# Organize clusters into collapsible groups with tenant assignment
# =============================================================================

def get_user_tenant(username: str) -> str:
    """Get tenant_id for a user, returns None for admins/no tenant"""
    users = load_users()
    user = users.get(username, {})
    return user.get('tenant')


@app.route('/api/cluster-groups', methods=['GET'])
@require_auth()
def get_cluster_groups():
    """Get all cluster groups (filtered by tenant)"""
    try:
        db = get_db()
        usr = getattr(request, 'session', {}).get('user', 'system')
        users = load_users()
        user = users.get(usr, {})
        tenant_id = user.get('tenant')
        
        # Admins see all groups, tenant users only see their tenant's groups + global groups
        if user.get('role') == ROLE_ADMIN or not tenant_id:
            groups = db.query('SELECT * FROM cluster_groups ORDER BY sort_order, name')
        else:
            # Tenant users see: their tenant's groups + groups without tenant (global)
            groups = db.query(
                'SELECT * FROM cluster_groups WHERE tenant_id = ? OR tenant_id IS NULL ORDER BY sort_order, name',
                (tenant_id,)
            )
        
        return jsonify([dict(g) for g in groups] if groups else [])
    except Exception as e:
        app.logger.error(f"Error loading cluster groups: {e}")
        return jsonify([])


@app.route('/api/cluster-groups', methods=['POST'])
@require_auth(perms=['admin.groups'])
def create_cluster_group():
    """Create a new cluster group - requires admin.groups permission"""
    data = request.json or {}
    
    if not data.get('name'):
        return jsonify({'error': 'Name required'}), 400
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    users = load_users()
    user = users.get(usr, {})
    ip = request.remote_addr
    
    # Non-admins can only create groups for their own tenant
    tenant_id = data.get('tenant_id')
    if user.get('role') != ROLE_ADMIN:
        tenant_id = user.get('tenant')  # Force to user's tenant
    
    db = get_db()
    group_id = str(uuid.uuid4())[:8]
    now = datetime.now().isoformat()
    
    db.execute('''
        INSERT INTO cluster_groups (id, name, description, color, tenant_id, sort_order, created_at, updated_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        group_id,
        data.get('name'),
        data.get('description', ''),
        data.get('color', '#E86F2D'),
        tenant_id,
        data.get('sort_order', 0),
        now, now
    ))
    
    tenant_info = f" (Tenant: {tenant_id})" if tenant_id else " (Global)"
    log_audit(usr, 'cluster_group.created', f"Created cluster group '{data.get('name')}' (ID: {group_id}){tenant_info}", ip_address=ip)
    
    return jsonify({'success': True, 'id': group_id})


@app.route('/api/cluster-groups/<group_id>', methods=['PUT'])
@require_auth(perms=['admin.groups'])
def update_cluster_group(group_id):
    """Update a cluster group - requires admin.groups permission"""
    data = request.json or {}
    db = get_db()
    
    group = db.query_one('SELECT * FROM cluster_groups WHERE id = ?', (group_id,))
    if not group:
        return jsonify({'error': 'Group not found'}), 404
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    users = load_users()
    user = users.get(usr, {})
    ip = request.remote_addr
    
    # Check tenant access - non-admins can only edit their tenant's groups
    if user.get('role') != ROLE_ADMIN:
        user_tenant = user.get('tenant')
        if group['tenant_id'] and group['tenant_id'] != user_tenant:
            log_audit(usr, 'cluster_group.update_denied', f"Access denied to group '{group['name']}' (ID: {group_id}) - tenant mismatch", ip_address=ip)
            return jsonify({'error': 'Access denied - group belongs to different tenant'}), 403
    
    # Non-admins cannot change tenant_id
    tenant_id = data.get('tenant_id', group['tenant_id'])
    if user.get('role') != ROLE_ADMIN:
        tenant_id = group['tenant_id']  # Keep original tenant
    
    db.execute('''
        UPDATE cluster_groups SET
            name = ?,
            description = ?,
            color = ?,
            tenant_id = ?,
            sort_order = ?,
            collapsed = ?,
            updated_at = ?
        WHERE id = ?
    ''', (
        data.get('name', group['name']),
        data.get('description', group['description']),
        data.get('color', group['color']),
        tenant_id,
        data.get('sort_order', group['sort_order']),
        1 if data.get('collapsed') else 0,
        datetime.now().isoformat(),
        group_id
    ))
    
    log_audit(usr, 'cluster_group.updated', f"Updated cluster group '{data.get('name', group['name'])}' (ID: {group_id})", ip_address=ip)
    
    return jsonify({'success': True})


@app.route('/api/cluster-groups/<group_id>', methods=['DELETE'])
@require_auth(perms=['admin.groups'])
def delete_cluster_group(group_id):
    """Delete a cluster group (clusters will become ungrouped) - requires admin.groups permission"""
    db = get_db()
    
    group = db.query_one('SELECT * FROM cluster_groups WHERE id = ?', (group_id,))
    if not group:
        return jsonify({'error': 'Group not found'}), 404
    
    usr = getattr(request, 'session', {}).get('user', 'system')
    users = load_users()
    user = users.get(usr, {})
    ip = request.remote_addr
    
    # Check tenant access
    if user.get('role') != ROLE_ADMIN:
        user_tenant = user.get('tenant')
        if group['tenant_id'] and group['tenant_id'] != user_tenant:
            log_audit(usr, 'cluster_group.delete_denied', f"Access denied to delete group '{group['name']}' (ID: {group_id}) - tenant mismatch", ip_address=ip)
            return jsonify({'error': 'Access denied - group belongs to different tenant'}), 403
    
    # Count affected clusters for audit
    affected = db.query_one('SELECT COUNT(*) as cnt FROM clusters WHERE group_id = ?', (group_id,))
    affected_count = affected['cnt'] if affected else 0
    
    # Remove group assignment from all clusters in this group
    db.execute('UPDATE clusters SET group_id = NULL WHERE group_id = ?', (group_id,))
    
    # Delete the group
    db.execute('DELETE FROM cluster_groups WHERE id = ?', (group_id,))
    
    log_audit(usr, 'cluster_group.deleted', f"Deleted cluster group '{group['name']}' (ID: {group_id}) - {affected_count} clusters ungrouped", ip_address=ip)
    
    return jsonify({'success': True, 'ungrouped_clusters': affected_count})


@app.route('/api/clusters/<cluster_id>/rename', methods=['PUT'])
@require_auth(perms=['admin.groups'])
def rename_cluster(cluster_id):
    """Rename a cluster (set display_name) - requires admin.groups permission"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    data = request.json or {}
    
    if not data.get('display_name'):
        return jsonify({'error': 'display_name required'}), 400
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    db = get_db()
    usr = getattr(request, 'session', {}).get('user', 'system')
    ip = request.remote_addr
    
    # Get old name for audit
    cluster = db.query_one('SELECT name, display_name FROM clusters WHERE id = ?', (cluster_id,))
    old_name = cluster['display_name'] or cluster['name'] if cluster else cluster_id
    new_name = data.get('display_name')
    
    db.execute('''
        UPDATE clusters SET display_name = ?, updated_at = ? WHERE id = ?
    ''', (new_name, datetime.now().isoformat(), cluster_id))
    
    log_audit(usr, 'cluster.renamed', f"Renamed cluster from '{old_name}' to '{new_name}' (ID: {cluster_id})", ip_address=ip)
    
    return jsonify({'success': True})


@app.route('/api/clusters/<cluster_id>/group', methods=['PUT'])
@require_auth(perms=['admin.groups'])
def assign_cluster_to_group(cluster_id):
    """Assign a cluster to a group (or remove from group with null) - requires admin.groups permission"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    data = request.json or {}
    group_id = data.get('group_id')  # Can be None to ungroup
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    db = get_db()
    usr = getattr(request, 'session', {}).get('user', 'system')
    users = load_users()
    user = users.get(usr, {})
    ip = request.remote_addr
    
    # Verify group exists and user has access to it
    if group_id:
        group = db.query_one('SELECT name, tenant_id FROM cluster_groups WHERE id = ?', (group_id,))
        if not group:
            return jsonify({'error': 'Group not found'}), 404
        
        # Check tenant access to target group
        if user.get('role') != ROLE_ADMIN:
            user_tenant = user.get('tenant')
            if group['tenant_id'] and group['tenant_id'] != user_tenant:
                log_audit(usr, 'cluster.group_assign_denied', f"Access denied to assign cluster {cluster_id} to group '{group['name']}' - tenant mismatch", ip_address=ip)
                return jsonify({'error': 'Access denied - group belongs to different tenant'}), 403
        
        group_name = group['name']
    else:
        group_name = 'Ungrouped'
    
    # Get old group for audit
    old_group = db.query_one('''
        SELECT cg.name FROM clusters c 
        LEFT JOIN cluster_groups cg ON c.group_id = cg.id 
        WHERE c.id = ?
    ''', (cluster_id,))
    old_group_name = old_group['name'] if old_group and old_group['name'] else 'Ungrouped'
    
    db.execute('''
        UPDATE clusters SET group_id = ?, updated_at = ? WHERE id = ?
    ''', (group_id, datetime.now().isoformat(), cluster_id))
    
    cluster = db.query_one('SELECT name, display_name FROM clusters WHERE id = ?', (cluster_id,))
    cluster_name = cluster['display_name'] or cluster['name'] if cluster else cluster_id
    
    log_audit(usr, 'cluster.group_changed', f"Moved cluster '{cluster_name}' from '{old_group_name}' to '{group_name}'", ip_address=ip)
    
    return jsonify({'success': True})


@app.route('/api/cluster-groups/<group_id>/collapse', methods=['PUT'])
@require_auth()
def toggle_group_collapse(group_id):
    """Toggle collapsed state of a group (user preference, no audit needed)"""
    data = request.json or {}
    db = get_db()
    
    # Verify group exists
    group = db.query_one('SELECT id FROM cluster_groups WHERE id = ?', (group_id,))
    if not group:
        return jsonify({'error': 'Group not found'}), 404
    
    db.execute('''
        UPDATE cluster_groups SET collapsed = ? WHERE id = ?
    ''', (1 if data.get('collapsed') else 0, group_id))
    
    return jsonify({'success': True})


@app.route('/api/clusters/<cluster_id>/nodes/<node>/options', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_options_api(cluster_id, node):
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    return jsonify(cluster_managers[cluster_id].get_node_options(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/options', methods=['PUT'])
@require_auth(perms=['node.maintenance'])
def update_node_options_api(cluster_id, node):
    """Update node options"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    result = manager.update_node_options(node, request.json or {})
    
    if result['success']:
        return jsonify({'message': result['message']})
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/apt/updates', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_apt_updates_api(cluster_id, node):
    """Get available APT updates"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    return jsonify(manager.get_node_apt_updates(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/apt/refresh', methods=['POST'])
@require_auth(perms=['node.update'])
def refresh_node_apt_api(cluster_id, node):
    """Refresh APT package database"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    result = manager.refresh_node_apt(node)
    
    if result['success']:
        return jsonify(result)
    return jsonify({'error': result['error']}), 500


# ==================== NODE DISK MANAGEMENT ====================

@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_disks_api(cluster_id, node):
    """Get physical disks on a node"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    disks = manager.get_node_disks(node)
    return jsonify(disks)


@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks/<path:disk>/smart', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_disk_smart_api(cluster_id, node, disk):
    """Get SMART data for a disk"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    # Decode disk path (e.g., /dev/sda -> %2Fdev%2Fsda)
    smart_data = manager.get_node_disk_smart(node, '/' + disk if not disk.startswith('/') else disk)
    return jsonify(smart_data)


@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks/lvm', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_lvm_api(cluster_id, node):
    """Get LVM volume groups on a node"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    return jsonify(manager.get_node_lvm(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks/lvm', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_node_lvm_api(cluster_id, node):
    """Create LVM volume group"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    device = data.get('device')
    name = data.get('name')
    add_storage = data.get('add_storage', True)
    
    if not device or not name:
        return jsonify({'error': 'Device and name required'}), 400
    
    result = manager.create_node_lvm(node, device, name, add_storage)
    
    if result['success']:
        return jsonify(result)
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks/lvmthin', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_lvmthin_api(cluster_id, node):
    """Get LVM-Thin pools on a node"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    return jsonify(manager.get_node_lvmthin(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks/lvmthin', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_node_lvmthin_api(cluster_id, node):
    """Create LVM-Thin pool"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    device = data.get('device')
    name = data.get('name')
    add_storage = data.get('add_storage', True)
    
    if not device or not name:
        return jsonify({'error': 'Device and name required'}), 400
    
    result = manager.create_node_lvmthin(node, device, name, add_storage)
    
    if result['success']:
        return jsonify(result)
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks/zfs', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_zfs_api(cluster_id, node):
    """Get ZFS pools on a node"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    return jsonify(manager.get_node_zfs(node))


@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks/zfs', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_node_zfs_api(cluster_id, node):
    """Create ZFS pool"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    name = data.get('name')
    devices = data.get('devices', [])
    raidlevel = data.get('raidlevel', 'single')
    compression = data.get('compression', 'on')
    ashift = data.get('ashift', 12)
    add_storage = data.get('add_storage', True)
    
    if not name or not devices:
        return jsonify({'error': 'Name and devices required'}), 400
    
    result = manager.create_node_zfs(node, name, devices, raidlevel, compression, ashift, add_storage)
    
    if result['success']:
        return jsonify(result)
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks/directory', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_node_directory_api(cluster_id, node):
    """Create directory storage"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    device = data.get('device')
    name = data.get('name')
    filesystem = data.get('filesystem', 'ext4')
    add_storage = data.get('add_storage', True)
    
    if not device or not name:
        return jsonify({'error': 'Device and name required'}), 400
    
    result = manager.create_node_directory(node, device, name, filesystem, add_storage)
    
    if result['success']:
        return jsonify(result)
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks/initgpt', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def init_node_disk_gpt_api(cluster_id, node):
    """Initialize disk with GPT partition table"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    disk = data.get('disk')
    uuid = data.get('uuid')
    
    if not disk:
        return jsonify({'error': 'Disk required'}), 400
    
    result = manager.init_disk_gpt(node, disk, uuid)
    
    if result['success']:
        return jsonify(result)
    return jsonify({'error': result['error']}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/disks/wipe', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def wipe_node_disk_api(cluster_id, node):
    """Wipe disk (delete partition table)"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    data = request.json or {}
    
    disk = data.get('disk')
    
    if not disk:
        return jsonify({'error': 'Disk required'}), 400
    
    result = manager.wipe_disk(node, disk)
    
    if result['success']:
        return jsonify(result)
    return jsonify({'error': result['error']}), 500


# ==================== UPDATE MANAGER ====================

@app.route('/api/clusters/<cluster_id>/updates/check', methods=['POST'])
@require_auth(perms=['node.update'])
def check_cluster_updates(cluster_id):
    """Check for updates on all nodes in the cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    results = {}
    
    try:
        host = mgr.current_host or mgr.config.host
        url = f"https://{host}:8006/api2/json/nodes"
        r = mgr._create_session().get(url, timeout=10)
        
        if r.status_code != 200:
            return jsonify({'error': 'Failed: nodes from cluster'}), 500
        
        nodes_data = r.json().get('data', [])
        node_names = [n.get('node') for n in nodes_data if n.get('node') and n.get('status') == 'online']
    except Exception as e:
        return jsonify({'error': f'Failed to connect to cluster: {str(e)}'}), 500
    
    if not node_names:
        return jsonify({
            'success': True,
            'nodes': {},
            'summary': {
                'total_updates': 0,
                'nodes_with_updates': 0,
                'total_nodes': 0,
                'checked_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
        })
    
    for node_name in node_names:
        try:
            updates = mgr.get_node_apt_updates(node_name)
            
            if isinstance(updates, list):
                update_list = updates
            elif isinstance(updates, dict):
                update_list = updates.get('data', [])
            else:
                update_list = []
            
            results[node_name] = {
                'success': True,
                'updates': update_list,
                'count': len(update_list)
            }
        except Exception as e:
            results[node_name] = {
                'success': False,
                'error': str(e),
                'updates': [],
                'count': 0
            }
    
    total_updates = sum(r.get('count', 0) for r in results.values())
    nodes_with_updates = sum(1 for r in results.values() if r.get('count', 0) > 0)
    
    # LW: store timestamp so we can show when last checked
    mgr._last_update_check = time.strftime('%Y-%m-%d %H:%M:%S')
    
    return jsonify({
        'success': True,
        'nodes': results,
        'summary': {
            'total_updates': total_updates,
            'nodes_with_updates': nodes_with_updates,
            'total_nodes': len(results),
            'checked_at': mgr._last_update_check
        }
    })


@app.route('/api/clusters/<cluster_id>/updates/status', methods=['GET'])
@require_auth(perms=['node.view'])
def get_cluster_update_status(cluster_id):
    """Get cached update status for cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    rolling_update = getattr(mgr, '_rolling_update', None)
    
    # Auto-clear completed/failed/cancelled status - NS Jan 2026
    if rolling_update and rolling_update.get('status') in ['completed', 'failed', 'cancelled']:
        completed_at = rolling_update.get('completed_at', '')
        if completed_at:
            try:
                from datetime import datetime
                completed_time = datetime.strptime(completed_at, '%Y-%m-%d %H:%M:%S')
                age_seconds = (datetime.now() - completed_time).total_seconds()
                # Auto-clear after 5 minutes for completed, 30 minutes for failed
                clear_after = 1800 if rolling_update.get('status') == 'failed' else 300
                if age_seconds > clear_after:
                    mgr._rolling_update = None
                    rolling_update = None
            except:
                # Invalid timestamp - clear it
                mgr._rolling_update = None
                rolling_update = None
        else:
            # No completed_at timestamp - this is legacy or broken data, clear it
            mgr._rolling_update = None
            rolling_update = None
    
    return jsonify({
        'success': True,
        'rolling_update': rolling_update,
        'last_check': getattr(mgr, '_last_update_check', None)
    })


@app.route('/api/clusters/<cluster_id>/updates/rolling', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN], perms=['node.update'])
def start_rolling_update(cluster_id):
    """Start a rolling update across all cluster nodes
    
    MK: Fixed GitHub Issue - skip up-to-date nodes and configurable timeout
    
    Parameters (via JSON body):
    - include_reboot: bool - Whether to reboot nodes after update (default: False)
    - node_order: list - Custom order of nodes to update
    - skip_up_to_date: bool - Skip nodes that have no updates available (default: True)
    - force_all: bool - Force update all nodes even if up-to-date (default: False)
    - evacuation_timeout: int - Timeout in seconds for VM evacuation (default: 1800 = 30 min)
    - update_timeout: int - Timeout in seconds for apt upgrade (default: 900 = 15 min)
    - reboot_timeout: int - Timeout in seconds for node reboot (default: 600 = 10 min)
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.get_json() or {}
    
    # Configuration options
    include_reboot = data.get('include_reboot', False)
    node_order = data.get('node_order', None)
    skip_up_to_date = data.get('skip_up_to_date', True)
    force_all = data.get('force_all', False)
    skip_evacuation = data.get('skip_evacuation', False)  # MK: Issue #22 - skip VM evacuation (NOT RECOMMENDED)
    
    # MK: Configurable timeouts (GitHub Issue fix)
    evacuation_timeout = data.get('evacuation_timeout', 1800)  # 30 minutes default (was 5 min!)
    update_timeout = data.get('update_timeout', 900)  # 15 minutes default
    reboot_timeout = data.get('reboot_timeout', 600)  # 10 minutes default
    
    # Validate timeouts (min 60s, max 2 hours)
    evacuation_timeout = max(60, min(7200, int(evacuation_timeout)))
    update_timeout = max(60, min(7200, int(update_timeout)))
    reboot_timeout = max(60, min(7200, int(reboot_timeout)))
    
    # check already running
    if hasattr(mgr, '_rolling_update') and mgr._rolling_update and mgr._rolling_update.get('status') == 'running':
        return jsonify({'error': 'Rolling update already in progress'}), 400
    
    # Get nodes from cluster status
    try:
        node_status = mgr.get_node_status()
        available_nodes = list(node_status.keys()) if node_status else []
    except Exception as e:
        return jsonify({'error': f'Failed: cluster nodes: {str(e)}'}), 500
    
    if not available_nodes:
        return jsonify({'error': 'No nodes available for update'}), 400
    
    # Get nodes to update (use custom order or default)
    if node_order:
        nodes_to_update = [n for n in node_order if n in available_nodes]
    else:
        nodes_to_update = available_nodes
    
    if not nodes_to_update:
        return jsonify({'error': 'No nodes available for update'}), 400
    
    # init rolling update state
    mgr._rolling_update = {
        'status': 'running',
        'started_at': time.strftime('%Y-%m-%d %H:%M:%S'),
        'include_reboot': include_reboot,
        'skip_up_to_date': skip_up_to_date,
        'skip_evacuation': skip_evacuation,  # MK: Issue #22
        'force_all': force_all,
        'evacuation_timeout': evacuation_timeout,
        'update_timeout': update_timeout,
        'reboot_timeout': reboot_timeout,
        'nodes': nodes_to_update,
        'current_index': 0,
        'current_node': nodes_to_update[0],
        'current_step': 'starting',
        'completed_nodes': [],
        'skipped_nodes': [],  # MK: Track skipped nodes
        'failed_nodes': [],
        'logs': []
    }
    
    # Start the rolling update in a background thread
    def run_rolling_update():
        try:
            logging.info(f"[RollingUpdate] Starting rolling update for cluster, nodes: {nodes_to_update}")
            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Rolling update started")
            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Settings: skip_up_to_date={skip_up_to_date}, skip_evacuation={skip_evacuation}, evacuation_timeout={evacuation_timeout}s")
            
            if skip_evacuation:
                mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ⚠️ WARNING: VM evacuation disabled - VMs may be affected if update fails!")
            
            for idx, node_name in enumerate(nodes_to_update):
                if not hasattr(mgr, '_rolling_update') or mgr._rolling_update.get('status') != 'running':
                    logging.info(f"[RollingUpdate] Update cancelled or stopped")
                    break
                
                mgr._rolling_update['current_index'] = idx
                mgr._rolling_update['current_node'] = node_name
                mgr._rolling_update['current_step'] = 'checking'
                mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] === Processing {node_name} ({idx+1}/{len(nodes_to_update)}) ===")
                logging.info(f"[RollingUpdate] Processing node: {node_name}")
                
                try:
                    # MK: Step 0 - Check if node has updates available (GitHub Issue fix)
                    if skip_up_to_date and not force_all:
                        mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Checking for available updates on {node_name}...")
                        
                        # First refresh apt cache
                        try:
                            mgr.refresh_node_apt(node_name)
                            time.sleep(3)  # Wait for refresh
                        except:
                            pass
                        
                        available_updates = mgr.get_node_apt_updates(node_name)
                        update_count = len(available_updates) if available_updates else 0
                        
                        if update_count == 0:
                            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ⏭ {node_name} is already up-to-date - SKIPPING")
                            mgr._rolling_update['skipped_nodes'].append(node_name)
                            logging.info(f"[RollingUpdate] Node {node_name} is up-to-date, skipping")
                            continue
                        else:
                            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Found {update_count} updates available on {node_name}")
                            logging.info(f"[RollingUpdate] Node {node_name} has {update_count} updates available")
                    
                    # Step 1: Enable maintenance mode (evacuate VMs unless skip_evacuation is set)
                    mgr._rolling_update['current_step'] = 'maintenance'
                    if skip_evacuation:
                        mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Enabling maintenance mode on {node_name} (SKIP EVACUATION)")
                        logging.info(f"[RollingUpdate] Enabling maintenance mode on {node_name} (skip_evacuation=True)")
                    else:
                        mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Enabling maintenance mode on {node_name}")
                        logging.info(f"[RollingUpdate] Enabling maintenance mode on {node_name}")
                    
                    maintenance_task = mgr.enter_maintenance_mode(node_name, skip_evacuation=skip_evacuation)
                    
                    if not maintenance_task:
                        logging.error(f"[RollingUpdate] Failed to start maintenance mode on {node_name}")
                        raise Exception(f"Failed to start maintenance mode")
                    
                    # Wait for evacuation to complete (unless skipped)
                    if skip_evacuation:
                        mgr._rolling_update['current_step'] = 'updating'
                        mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ⚠️ Skipping VM evacuation - VMs remain on node")
                        evacuation_completed = True
                    else:
                        mgr._rolling_update['current_step'] = 'evacuating'
                        mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Waiting for VM evacuation (timeout: {evacuation_timeout}s)...")
                        waited = 0
                        evacuation_completed = False
                        last_progress_log = 0
                        
                        while waited < evacuation_timeout:
                            # Check maintenance task status directly
                            if node_name in mgr.nodes_in_maintenance:
                                maintenance_task = mgr.nodes_in_maintenance[node_name]
                                if maintenance_task.status in ['completed', 'completed_with_errors']:
                                    mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ✓ Evacuation completed")
                                    evacuation_completed = True
                                    break
                                else:
                                    # Log progress every 30 seconds
                                    if waited - last_progress_log >= 30:
                                        if hasattr(maintenance_task, 'migrated_vms') and hasattr(maintenance_task, 'total_vms'):
                                            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Evacuating: {maintenance_task.migrated_vms}/{maintenance_task.total_vms} VMs migrated ({waited}s elapsed)")
                                        else:
                                            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Evacuation in progress... ({waited}s elapsed)")
                                        last_progress_log = waited
                            time.sleep(5)
                            waited += 5
                        
                        if not evacuation_completed:
                            raise Exception(f"Evacuation timed out after {evacuation_timeout}s - consider increasing evacuation_timeout")
                    
                    # Step 2: Run apt update/upgrade
                    mgr._rolling_update['current_step'] = 'updating'
                    mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Installing updates on {node_name}")
                    logging.info(f"[RollingUpdate] Installing updates on {node_name}")
                    
                    update_task = mgr.start_node_update(node_name, reboot=include_reboot)
                    
                    if not update_task:
                        logging.error(f"[RollingUpdate] start_node_update returned None for {node_name}")
                        raise Exception(f"Update failed: Could not start update task")
                    
                    # Step 3: Wait for update task to complete
                    mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Waiting for update task (timeout: {update_timeout}s)...")
                    update_waited = 0
                    last_phase = None
                    while update_waited < update_timeout:
                        if update_task.status in ['completed', 'failed']:
                            break
                        # Log phase changes
                        if hasattr(update_task, 'phase') and update_task.phase != last_phase:
                            last_phase = update_task.phase
                            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Update phase: {last_phase}")
                        time.sleep(10)
                        update_waited += 10
                    
                    if update_task.status == 'failed':
                        raise Exception(f"Update failed: {update_task.error or 'Unknown error'}")
                    
                    if update_task.status != 'completed':
                        raise Exception(f"Update timed out after {update_timeout}s (status: {update_task.status})")
                    
                    mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ✓ Updates installed")
                    
                    # Step 4: If reboot was included, wait for node to come back
                    if include_reboot:
                        mgr._rolling_update['current_step'] = 'rebooting'
                        mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Node {node_name} rebooting (timeout: {reboot_timeout}s)...")
                        logging.info(f"[RollingUpdate] Node {node_name} rebooting")
                        
                        # Wait for node to come back online
                        waited = 0
                        while waited < reboot_timeout:
                            try:
                                node_status = mgr.get_node_status()
                                if node_name in node_status and node_status[node_name].get('status') == 'online':
                                    mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ✓ Node {node_name} back online")
                                    break
                            except:
                                pass
                            time.sleep(10)
                            waited += 10
                        
                        if waited >= reboot_timeout:
                            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ⚠ Warning: Node {node_name} did not come back online within {reboot_timeout}s")
                    
                    # Step 5: Disable maintenance mode
                    mgr._rolling_update['current_step'] = 'finishing'
                    mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Disabling maintenance mode on {node_name}")
                    mgr.exit_maintenance_mode(node_name)
                    
                    mgr._rolling_update['completed_nodes'].append(node_name)
                    mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ✓ {node_name} updated successfully")
                    logging.info(f"[RollingUpdate] Node {node_name} updated successfully")
                    
                except Exception as e:
                    logging.error(f"[RollingUpdate] Error updating {node_name}: {e}")
                    mgr._rolling_update['failed_nodes'].append({'node': node_name, 'error': str(e)})
                    mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] ✗ ERROR on {node_name}: {e}")
                    # Try to exit maintenance mode even if update failed
                    try:
                        mgr.exit_maintenance_mode(node_name)
                    except:
                        pass
            
            # Final summary
            completed = len(mgr._rolling_update['completed_nodes'])
            skipped = len(mgr._rolling_update['skipped_nodes'])
            failed = len(mgr._rolling_update['failed_nodes'])
            
            mgr._rolling_update['status'] = 'completed'
            mgr._rolling_update['completed_at'] = time.strftime('%Y-%m-%d %H:%M:%S')
            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] === Rolling update completed ===")
            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Summary: {completed} updated, {skipped} skipped (up-to-date), {failed} failed")
            logging.info(f"[RollingUpdate] Rolling update completed: {completed} updated, {skipped} skipped, {failed} failed")
            
        except Exception as e:
            logging.error(f"[RollingUpdate] Rolling update failed with exception: {e}")
            mgr._rolling_update['status'] = 'failed'
            mgr._rolling_update['completed_at'] = time.strftime('%Y-%m-%d %H:%M:%S')
            mgr._rolling_update['error'] = str(e)
            mgr._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Rolling update failed: {e}")
    
    import threading
    update_thread = threading.Thread(target=run_rolling_update, daemon=True)
    update_thread.start()
    
    return jsonify({
        'success': True,
        'message': 'Rolling update started',
        'nodes': nodes_to_update,
        'include_reboot': include_reboot,
        'skip_up_to_date': skip_up_to_date,
        'evacuation_timeout': evacuation_timeout,
        'update_timeout': update_timeout,
        'reboot_timeout': reboot_timeout
    })


@app.route('/api/clusters/<cluster_id>/updates/rolling', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def cancel_rolling_update(cluster_id):
    """Cancel a running rolling update"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    
    if not hasattr(manager, '_rolling_update') or not manager._rolling_update:
        return jsonify({'error': 'No rolling update in progress'}), 400
    
    manager._rolling_update['status'] = 'cancelled'
    manager._rolling_update['completed_at'] = time.strftime('%Y-%m-%d %H:%M:%S')
    manager._rolling_update['logs'].append(f"[{time.strftime('%H:%M:%S')}] Rolling update cancelled by user")
    
    # Try to exit maintenance mode on current node
    current_node = manager._rolling_update.get('current_node')
    if current_node:
        try:
            manager.exit_maintenance_mode(current_node)
        except:
            pass
    
    return jsonify({'success': True, 'message': 'Rolling update cancelled'})


@app.route('/api/clusters/<cluster_id>/updates/rolling/clear', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def clear_rolling_update_status(cluster_id):
    """Clear completed/cancelled rolling update status (dismiss notification)"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    manager = cluster_managers[cluster_id]
    
    if hasattr(manager, '_rolling_update') and manager._rolling_update:
        status = manager._rolling_update.get('status', '')
        # Only clear if not currently running
        if status in ['completed', 'cancelled', 'failed']:
            manager._rolling_update = None
            return jsonify({'success': True, 'message': 'Status cleared'})
        else:
            return jsonify({'error': 'Cannot clear running update'}), 400
    
    return jsonify({'success': True, 'message': 'Nothing to clear'})


# ============================================
# Scheduled Updates API
# MK: Automatic rolling update scheduling (SQLite storage)
# ============================================

def load_update_schedule(cluster_id: str) -> dict:
    """Load update schedule for a cluster from SQLite"""
    default = {
        'enabled': False,
        'schedule_type': 'recurring',
        'day': 'sunday',
        'time': '03:00',
        'include_reboot': True,
        'skip_evacuation': False,
        'skip_up_to_date': True,
        'evacuation_timeout': 1800,
        'last_run': None,
        'next_run': None
    }
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        # MK: Ensure table exists (migration for existing databases)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS update_schedules (
                cluster_id TEXT PRIMARY KEY,
                enabled INTEGER DEFAULT 0,
                schedule_type TEXT DEFAULT 'recurring',
                day TEXT DEFAULT 'sunday',
                time TEXT DEFAULT '03:00',
                include_reboot INTEGER DEFAULT 1,
                skip_evacuation INTEGER DEFAULT 0,
                skip_up_to_date INTEGER DEFAULT 1,
                evacuation_timeout INTEGER DEFAULT 1800,
                last_run TEXT,
                next_run TEXT,
                created_by TEXT,
                created_at TEXT,
                updated_at TEXT
            )
        ''')
        
        cursor.execute('SELECT * FROM update_schedules WHERE cluster_id = ?', (cluster_id,))
        row = cursor.fetchone()
        if row:
            return {
                'enabled': bool(row['enabled']),
                'schedule_type': row['schedule_type'] or 'recurring',
                'day': row['day'] or 'sunday',
                'time': row['time'] or '03:00',
                'include_reboot': bool(row['include_reboot']),
                'skip_evacuation': bool(row['skip_evacuation']),
                'skip_up_to_date': bool(row['skip_up_to_date']),
                'evacuation_timeout': row['evacuation_timeout'] or 1800,
                'last_run': row['last_run'],
                'next_run': row['next_run']
            }
    except Exception as e:
        logging.error(f"Error loading update schedule: {e}")
    return default


def save_update_schedule(cluster_id: str, schedule: dict, user: str = 'system'):
    """Save update schedule for a cluster to SQLite"""
    try:
        db = get_db()
        cursor = db.conn.cursor()
        now = datetime.now().isoformat()
        
        # MK: Use INSERT OR REPLACE for older SQLite compatibility
        cursor.execute('''
            INSERT OR REPLACE INTO update_schedules 
            (cluster_id, enabled, schedule_type, day, time, include_reboot, skip_evacuation, 
             skip_up_to_date, evacuation_timeout, last_run, next_run, created_by, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            cluster_id,
            1 if schedule.get('enabled') else 0,
            schedule.get('schedule_type', 'recurring'),
            schedule.get('day', 'sunday'),
            schedule.get('time', '03:00'),
            1 if schedule.get('include_reboot', True) else 0,
            1 if schedule.get('skip_evacuation', False) else 0,
            1 if schedule.get('skip_up_to_date', True) else 0,
            schedule.get('evacuation_timeout', 1800),
            schedule.get('last_run'),
            schedule.get('next_run'),
            user,
            now,
            now
        ))
        db.conn.commit()
    except Exception as e:
        logging.error(f"Error saving update schedule: {e}")


def update_schedule_last_run(cluster_id: str, last_run: str, next_run: str):
    """Update last_run and next_run for a schedule"""
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('''
            UPDATE update_schedules SET last_run = ?, next_run = ?, updated_at = ?
            WHERE cluster_id = ?
        ''', (last_run, next_run, datetime.now().isoformat(), cluster_id))
        db.conn.commit()
    except Exception as e:
        logging.error(f"Error updating schedule last_run: {e}")


def load_all_update_schedules() -> dict:
    """Load all enabled update schedules from SQLite"""
    schedules = {}
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM update_schedules WHERE enabled = 1')
        for row in cursor.fetchall():
            schedules[row['cluster_id']] = {
                'enabled': bool(row['enabled']),
                'schedule_type': row['schedule_type'] or 'recurring',
                'day': row['day'] or 'sunday',
                'time': row['time'] or '03:00',
                'include_reboot': bool(row['include_reboot']),
                'skip_evacuation': bool(row['skip_evacuation']),
                'skip_up_to_date': bool(row['skip_up_to_date']),
                'evacuation_timeout': row['evacuation_timeout'] or 1800,
                'last_run': row['last_run'],
                'next_run': row['next_run']
            }
    except Exception as e:
        logging.error(f"Error loading all update schedules: {e}")
    return schedules


@app.route('/api/clusters/<cluster_id>/updates/schedule', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_update_schedule(cluster_id):
    """Get the scheduled update configuration for a cluster"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    schedule = load_update_schedule(cluster_id)
    return jsonify(schedule)


@app.route('/api/clusters/<cluster_id>/updates/schedule', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def set_update_schedule(cluster_id):
    """Set the scheduled update configuration for a cluster"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    data = request.json or {}
    usr = getattr(request, 'session', {}).get('user', 'system')
    
    schedule = {
        'enabled': data.get('enabled', False),
        'schedule_type': data.get('schedule_type', 'recurring'),
        'day': data.get('day', 'sunday'),
        'time': data.get('time', '03:00'),
        'include_reboot': data.get('include_reboot', True),
        'skip_evacuation': data.get('skip_evacuation', False),
        'skip_up_to_date': data.get('skip_up_to_date', True),
        'evacuation_timeout': data.get('evacuation_timeout', 1800),
        'last_run': None,
        'next_run': None
    }
    
    # Calculate next run time
    if schedule['enabled']:
        schedule['next_run'] = calculate_next_update_run(schedule['day'], schedule['time'])
    
    save_update_schedule(cluster_id, schedule, usr)
    
    # Log audit
    mgr = cluster_managers[cluster_id]
    log_audit(usr, 'update.schedule', f"Update schedule {'enabled' if schedule['enabled'] else 'disabled'} for {mgr.config.name}", cluster=mgr.config.name)
    
    return jsonify({'success': True, 'schedule': schedule})


@app.route('/api/clusters/<cluster_id>/updates/schedule', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def delete_update_schedule(cluster_id):
    """Delete/disable the scheduled update for a cluster"""
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('DELETE FROM update_schedules WHERE cluster_id = ?', (cluster_id,))
        db.conn.commit()
        
        usr = getattr(request, 'session', {}).get('user', 'system')
        mgr = cluster_managers[cluster_id]
        log_audit(usr, 'update.schedule.deleted', f"Update schedule deleted for {mgr.config.name}", cluster=mgr.config.name)
        
        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


def calculate_next_update_run(day: str, time_str: str) -> str:
    """Calculate the next scheduled run time"""
    try:
        now = datetime.now()
        hour, minute = map(int, time_str.split(':'))
        
        day_map = {
            'monday': 0, 'tuesday': 1, 'wednesday': 2, 'thursday': 3,
            'friday': 4, 'saturday': 5, 'sunday': 6, 'daily': -1
        }
        
        target_day = day_map.get(day.lower(), -1)
        
        if target_day == -1:  # Daily
            next_run = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
            if next_run <= now:
                next_run += timedelta(days=1)
        else:
            days_ahead = target_day - now.weekday()
            if days_ahead < 0:
                days_ahead += 7
            next_run = now + timedelta(days=days_ahead)
            next_run = next_run.replace(hour=hour, minute=minute, second=0, microsecond=0)
            if next_run <= now:
                next_run += timedelta(days=7)
        
        return next_run.strftime('%Y-%m-%d %H:%M:%S')
    except Exception as e:
        logging.error(f"Error calculating next run: {e}")
        return None


def check_scheduled_updates():
    """Check if any scheduled updates should run - called by scheduler"""
    try:
        schedules = load_all_update_schedules()
        now = datetime.now()
        
        for cluster_id, schedule in schedules.items():
            if not schedule.get('enabled'):
                continue
            
            if cluster_id not in cluster_managers:
                continue
            
            mgr = cluster_managers[cluster_id]
            if not mgr.is_connected:
                continue
            
            # Check schedule_type - 'once' schedules that already ran should be skipped
            schedule_type = schedule.get('schedule_type', 'recurring')
            if schedule_type == 'once' and schedule.get('last_run'):
                continue
            
            # Check if it's time to run
            day = schedule.get('day', 'sunday')
            time_str = schedule.get('time', '03:00')
            
            try:
                hour, minute = map(int, time_str.split(':'))
            except:
                continue
            
            day_map = {
                'monday': 0, 'tuesday': 1, 'wednesday': 2, 'thursday': 3,
                'friday': 4, 'saturday': 5, 'sunday': 6
            }
            
            is_correct_day = (day == 'daily' or now.weekday() == day_map.get(day.lower(), -1))
            is_correct_time = now.hour == hour and now.minute == minute
            
            if is_correct_day and is_correct_time:
                # Check if already ran today (for recurring)
                if schedule_type == 'recurring':
                    last_run = schedule.get('last_run')
                    if last_run:
                        try:
                            last_run_date = datetime.fromisoformat(last_run).date()
                            if last_run_date == now.date():
                                continue  # Already ran today
                        except:
                            pass
                
                # Check if rolling update already running
                if hasattr(mgr, '_rolling_update') and mgr._rolling_update:
                    if mgr._rolling_update.get('status') == 'running':
                        continue
                
                logging.info(f"[SCHEDULER] Starting scheduled update for cluster {cluster_id} (type: {schedule_type})")
                
                # Execute the scheduled rolling update
                action = {
                    'cluster_id': cluster_id,
                    'action': 'rolling_update',
                    'config': {
                        'include_reboot': schedule.get('include_reboot', True),
                        'skip_evacuation': schedule.get('skip_evacuation', False),
                        'skip_up_to_date': schedule.get('skip_up_to_date', True),
                        'evacuation_timeout': schedule.get('evacuation_timeout', 1800)
                    }
                }
                
                execute_scheduled_rolling_update(mgr, cluster_id, action)
                
                # Update last run time
                last_run_str = now.isoformat()
                next_run_str = calculate_next_update_run(day, time_str) if schedule_type == 'recurring' else None
                update_schedule_last_run(cluster_id, last_run_str, next_run_str)
                
                # Disable 'once' schedules after running
                if schedule_type == 'once':
                    schedule['enabled'] = False
                    schedule['last_run'] = last_run_str
                    save_update_schedule(cluster_id, schedule)
                    logging.info(f"[SCHEDULER] One-time schedule disabled for {cluster_id}")
                
    except Exception as e:
        logging.error(f"Error checking scheduled updates: {e}")


# ============================================
# APT Repository Management
# APT repo management per node
# ============================================

# Standard Proxmox repositories
# Note: Proxmox 8.x uses .sources files (DEB822 format) instead of .list
# The API handles both formats, we match by URI
PROXMOX_REPOS = {
    'pve-enterprise': {
        'name': 'Proxmox VE Enterprise',
        'file': '/etc/apt/sources.list.d/pve-enterprise.list',  # Legacy
        'sources_file': '/etc/apt/sources.list.d/pve-enterprise.sources',  # New format
        'line': 'deb https://enterprise.proxmox.com/debian/pve bookworm pve-enterprise',
        'description': 'Stable enterprise repository (requires subscription)',
        'requires_subscription': True,
        'match_uri': 'enterprise.proxmox.com/debian/pve'
    },
    'pve-no-subscription': {
        'name': 'Proxmox VE No-Subscription',
        'file': '/etc/apt/sources.list.d/pve-no-subscription.list',
        'sources_file': '/etc/apt/sources.list.d/pve-no-subscription.sources',
        'line': 'deb http://download.proxmox.com/debian/pve bookworm pve-no-subscription',
        'description': 'Testing/community repository (no subscription required)',
        'requires_subscription': False,
        'match_uri': 'download.proxmox.com/debian/pve'
    },
    'ceph-squid': {
        'name': 'Ceph Squid (19.x)',
        'file': '/etc/apt/sources.list.d/ceph.list',
        'sources_file': '/etc/apt/sources.list.d/ceph.sources',
        'line': 'deb http://download.proxmox.com/debian/ceph-squid bookworm no-subscription',
        'description': 'Ceph Squid storage repository (newest)',
        'requires_subscription': False,
        'match_uri': 'ceph-squid'
    },
    'ceph-reef': {
        'name': 'Ceph Reef (18.x)',
        'file': '/etc/apt/sources.list.d/ceph.list',
        'sources_file': '/etc/apt/sources.list.d/ceph.sources',
        'line': 'deb http://download.proxmox.com/debian/ceph-reef bookworm no-subscription',
        'description': 'Ceph Reef storage repository',
        'requires_subscription': False,
        'match_uri': 'ceph-reef'
    },
    'ceph-quincy': {
        'name': 'Ceph Quincy (17.x)',
        'file': '/etc/apt/sources.list.d/ceph.list',
        'sources_file': '/etc/apt/sources.list.d/ceph.sources',
        'line': 'deb http://download.proxmox.com/debian/ceph-quincy bookworm no-subscription',
        'description': 'Ceph Quincy storage repository (older)',
        'requires_subscription': False,
        'match_uri': 'ceph-quincy'
    },
    'ceph-enterprise': {
        'name': 'Ceph Enterprise',
        'file': '/etc/apt/sources.list.d/ceph.list',
        'sources_file': '/etc/apt/sources.list.d/ceph.sources',
        'line': 'deb https://enterprise.proxmox.com/debian/ceph-squid bookworm enterprise',
        'description': 'Ceph Enterprise repository (requires subscription)',
        'requires_subscription': True,
        'match_uri': 'enterprise.proxmox.com/debian/ceph'
    }
}


@app.route('/api/clusters/<cluster_id>/nodes/<node>/repos', methods=['GET'])
@require_auth(perms=['node.view'])
def get_node_repos(cluster_id, node):
    """Get APT repository configuration for a node
    
    MK: Fixed to match by full URI path, not just domain
    e.g. /debian/pve vs /debian/ceph-reef
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    try:
        host = mgr.current_host or mgr.config.host
        
        # Get all repos via Proxmox API
        file_url = f"https://{host}:8006/api2/json/nodes/{node}/apt/repositories"
        r = mgr._create_session().get(file_url, timeout=10)
        
        if r.status_code != 200:
            return jsonify({'error': 'Failed to get repositories from Proxmox API'}), 500
        
        api_data = r.json().get('data', {})
        api_files = api_data.get('files', [])
        
        logging.debug(f"[REPOS] Got {len(api_files)} files from Proxmox API for node {node}")
        for f in api_files:
            logging.debug(f"[REPOS] File: {f.get('path')} with {len(f.get('repositories', []))} repos")
        
        repos = []
        
        # Check each known repo
        for repo_id, repo_info in PROXMOX_REPOS.items():
            repo_data = {
                'id': repo_id,
                'name': repo_info['name'],
                'description': repo_info['description'],
                'file': repo_info['file'],  # Expected file (may differ from actual)
                'actual_file': None,  # Where we actually found it
                'expected_line': repo_info['line'],
                'requires_subscription': repo_info.get('requires_subscription', False),
                'enabled': False,
                'exists': False,
                'content': None,
                'index': None  # Index within the file for toggle
            }
            
            # Use the match_uri if defined, otherwise parse from line
            match_uri = repo_info.get('match_uri', '')
            if not match_uri:
                expected_parts = repo_info['line'].split()
                expected_url = expected_parts[1] if len(expected_parts) > 1 else ''
                url_without_proto = expected_url.replace('https://', '').replace('http://', '')
                url_parts = url_without_proto.split('/')
                match_uri = '/'.join(url_parts[:3]) if len(url_parts) >= 3 else url_without_proto
            
            logging.debug(f"[REPOS] Looking for {repo_id}: match_uri={match_uri}")
            
            # Search in ALL files
            for file_info in api_files:
                file_path = file_info.get('path', '')
                
                for idx, repo_entry in enumerate(file_info.get('repositories', [])):
                    repo_uris = repo_entry.get('URIs', [])
                    
                    for uri in repo_uris:
                        uri_clean = uri.replace('https://', '').replace('http://', '')
                        
                        # Match by the match_uri string
                        if match_uri in uri_clean:
                            repo_data['exists'] = True
                            repo_data['actual_file'] = file_path
                            repo_data['index'] = idx
                            
                            # Proxmox API: Enabled is 1 for enabled, 0 for disabled
                            enabled_val = repo_entry.get('Enabled')
                            if enabled_val is None:
                                repo_data['enabled'] = True
                            else:
                                repo_data['enabled'] = (enabled_val == 1)
                            
                            repo_data['content'] = repo_entry
                            repo_data['file'] = file_path
                            logging.info(f"[REPOS] Found {repo_id} in {file_path}[{idx}]: enabled={repo_data['enabled']}, uri={uri}")
                            break
                    
                    if repo_data['exists']:
                        break
                
                if repo_data['exists']:
                    break
            
            repos.append(repo_data)
        
        # Also add any other Proxmox-related repos found that we don't have defined
        # This helps when Proxmox adds new repos
        known_uris = set()
        for repo_info in PROXMOX_REPOS.values():
            known_uris.add(repo_info.get('match_uri', ''))
        
        for file_info in api_files:
            file_path = file_info.get('path', '')
            
            for idx, repo_entry in enumerate(file_info.get('repositories', [])):
                repo_uris = repo_entry.get('URIs', [])
                
                for uri in repo_uris:
                    uri_clean = uri.replace('https://', '').replace('http://', '')
                    
                    # Only show Proxmox-related repos that aren't already in our list
                    if ('proxmox.com' in uri_clean or 'download.proxmox' in uri_clean):
                        # Check if this is already covered by known repos
                        already_known = any(known_uri in uri_clean for known_uri in known_uris if known_uri)
                        
                        if not already_known:
                            # This is an unknown Proxmox repo - show it
                            enabled_val = repo_entry.get('Enabled')
                            is_enabled = enabled_val == 1 if enabled_val is not None else True
                            
                            # Generate a unique ID
                            repo_id_other = f"other-{hash(uri) % 10000}"
                            
                            repos.append({
                                'id': repo_id_other,
                                'name': uri_clean.split('/')[0],  # Domain as name
                                'description': f'Found in {file_path}',
                                'file': file_path,
                                'actual_file': file_path,
                                'expected_line': f"deb {uri}",
                                'requires_subscription': 'enterprise' in uri_clean.lower(),
                                'enabled': is_enabled,
                                'exists': True,
                                'content': repo_entry,
                                'index': idx,
                                'uri': uri,
                                'is_other': True  # Flag for UI
                            })
        
        return jsonify({
            'success': True,
            'node': node,
            'repositories': repos
        })
        
    except Exception as e:
        logging.error(f"Failed to get repositories: {e}")
        return jsonify({'error': f'Failed to get repositories: {str(e)}'}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/repos/<repo_id>', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN], perms=['node.update'])
def update_node_repo(cluster_id, node, repo_id):
    """Enable or disable a repository on a node
    
    MK: Fixed to match by full URI path, not just domain
    NS: Extended to support "other" repos by file path and index
    """
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    data = request.get_json() or {}
    enabled = data.get('enabled', True)
    
    # Check if this is a known repo or an "other" repo
    if repo_id.startswith('other-'):
        # For "other" repos, we need the file path and index from the request
        file_path = data.get('file')
        repo_index = data.get('index')
        
        if file_path is None or repo_index is None:
            return jsonify({'error': 'file and index required for custom repository toggle'}), 400
        
        repo_name = data.get('name', repo_id)
    elif repo_id not in PROXMOX_REPOS:
        return jsonify({'error': f'Unknown repository: {repo_id}'}), 400
    else:
        repo_info = PROXMOX_REPOS[repo_id]
        repo_name = repo_info['name']
    
    try:
        host = mgr.current_host or mgr.config.host
        
        # Use Proxmox API to modify repository
        url = f"https://{host}:8006/api2/json/nodes/{node}/apt/repositories"
        
        # For known repos, we need to find them first
        if not repo_id.startswith('other-'):
            # First get current repos to find the index
            r = mgr._create_session().get(url, timeout=10)
            if r.status_code != 200:
                return jsonify({'error': 'Failed to get current repositories'}), 500
            
            api_repos = r.json().get('data', {})
            
            # Use match_uri for consistent matching
            match_uri = repo_info.get('match_uri', '')
            if not match_uri:
                expected_parts = repo_info['line'].split()
                expected_url = expected_parts[1] if len(expected_parts) > 1 else ''
                url_without_proto = expected_url.replace('https://', '').replace('http://', '')
                url_parts = url_without_proto.split('/')
                match_uri = '/'.join(url_parts[:3]) if len(url_parts) >= 3 else url_without_proto
            
            # Find the repo in ANY file
            repo_index = None
            found_file_path = None
            
            for file_info in api_repos.get('files', []):
                current_path = file_info.get('path', '')
                
                for idx, repo_entry in enumerate(file_info.get('repositories', [])):
                    repo_uris = repo_entry.get('URIs', [])
                    for uri in repo_uris:
                        uri_clean = uri.replace('https://', '').replace('http://', '')
                        # Match by match_uri
                        if match_uri in uri_clean:
                            repo_index = idx
                            found_file_path = current_path
                            logging.info(f"[REPOS] Found {repo_id} at index {idx} in {current_path}")
                            break
                    if repo_index is not None:
                        break
                if repo_index is not None:
                    break
            
            if repo_index is None:
                return jsonify({
                    'error': 'Repository not found. Manual setup required.',
                    'hint': f'Add the repository to /etc/apt/sources.list or create {repo_info["file"]}'
                }), 400
        else:
            # For "other" repos, we already have file_path and repo_index from the request
            found_file_path = file_path
        
        # Toggle the repo
        toggle_url = f"https://{host}:8006/api2/json/nodes/{node}/apt/repositories"
        payload = {
            'path': found_file_path,
            'index': repo_index,
            'enabled': 1 if enabled else 0
        }
        
        logging.info(f"[REPOS] Toggling {repo_id}: path={found_file_path}, index={repo_index}, enabled={enabled}")
        
        r = mgr._create_session().post(toggle_url, data=payload, timeout=10)
        
        if r.status_code in [200, 204]:
            action = 'enabled' if enabled else 'disabled'
            log_audit(request.session['user'], 'node.repo.updated', 
                     f"Repository {repo_name} {action} on {node}")
            
            return jsonify({
                'success': True,
                'message': f"Repository {repo_name} {action}",
                'repo': repo_id,
                'enabled': enabled
            })
        else:
            return jsonify({
                'error': f'Failed to update repository: {r.status_code}',
                'details': r.text
            }), 500
        
    except Exception as e:
        return jsonify({'error': f'Failed to update repository: {str(e)}'}), 500


@app.route('/api/clusters/<cluster_id>/nodes/<node>/repos/refresh', methods=['POST'])
@require_auth(perms=['node.update'])
def refresh_node_repos(cluster_id, node):
    """Run apt update on a node to refresh package lists"""
    ok, err = check_cluster_access(cluster_id)
    if not ok: return err
    
    if cluster_id not in cluster_managers:
        return jsonify({'error': 'Cluster not found'}), 404
    
    mgr = cluster_managers[cluster_id]
    
    try:
        host = mgr.current_host or mgr.config.host
        url = f"https://{host}:8006/api2/json/nodes/{node}/apt/update"
        
        r = mgr._create_session().post(url, timeout=30)
        
        if r.status_code == 200:
            task_id = r.json().get('data')
            return jsonify({
                'success': True,
                'message': 'Package list refresh started',
                'task_id': task_id
            })
        else:
            return jsonify({'error': f'Failed to refresh: {r.status_code}'}), 500
            
    except Exception as e:
        return jsonify({'error': f'Failed to refresh repositories: {str(e)}'}), 500


@app.route('/api/timezones', methods=['GET'])
def get_timezones_api():
    """Get list of available timezones"""
    # Return a static list - works for any cluster
    return jsonify([
        'UTC', 'Europe/Berlin', 'Europe/Vienna', 'Europe/Zurich', 'Europe/London',
        'Europe/Paris', 'Europe/Amsterdam', 'Europe/Brussels', 'Europe/Rome',
        'Europe/Madrid', 'Europe/Warsaw', 'Europe/Prague', 'Europe/Budapest',
        'America/New_York', 'America/Chicago', 'America/Los_Angeles',
        'Asia/Tokyo', 'Asia/Shanghai', 'Asia/Singapore',
        'Australia/Sydney', 'Pacific/Auckland',
    ])


# ==================== END NODE MANAGEMENT API ENDPOINTS ====================

# ============================================
# WebSocket Live Updates
# ============================================

# Store connected WebSocket clients
ws_clients = {}
ws_clients_lock = threading.Lock()

def broadcast_update(update_type: str, data: dict, cluster_id: str = None):
    """Broadcast update to all connected WebSocket clients"""
    try:
        message = json.dumps({
            'type': update_type,
            'data': data,
            'cluster_id': cluster_id,
            'timestamp': datetime.now().isoformat()
        })
        
        # Limit message size
        if len(message) > 500000:  # 500KB limit
            logging.warning(f"Broadcast message too large ({len(message)} bytes), skipping")
            return
        
        disconnected = []
        
        # Get clients list under lock, then send outside lock
        clients_to_send = []
        with ws_clients_lock:
            for client_id, client_info in list(ws_clients.items()):
                ws = client_info.get('ws')
                client_lock = client_info.get('lock')
                if ws is None or client_lock is None:
                    disconnected.append(client_id)
                    continue
                
                # Only send if client is subscribed to this cluster or all clusters
                subscribed = client_info.get('clusters')
                if cluster_id is None or subscribed is None or cluster_id in subscribed:
                    clients_to_send.append((client_id, ws, client_lock))
        
        # Send to clients outside the main lock
        for client_id, ws, client_lock in clients_to_send:
            try:
                with client_lock:
                    ws.send(message)
            except Exception as e:
                logging.debug(f"Failed to send to client {client_id}: {e}")
                disconnected.append(client_id)
        
        # Remove disconnected clients
        if disconnected:
            with ws_clients_lock:
                for client_id in set(disconnected):  # Use set to avoid duplicates
                    if client_id in ws_clients:
                        del ws_clients[client_id]
                        logging.info(f"Removed disconnected client: {client_id}")
    except Exception as e:
        logging.error(f"Broadcast error: {e}")

def broadcast_action(action: str, resource_type: str, resource_id: str, details: dict = None, cluster_id: str = None, user: str = None):
    """Broadcast an action event to all clients for real-time UI updates"""
    broadcast_update('action', {
        'action': action,
        'resource_type': resource_type,
        'resource_id': resource_id,
        'details': details or {},
        'user': user
    }, cluster_id)

@sock.route('/api/ws/updates')
def ws_live_updates(ws):
    """WebSocket endpoint for live updates"""
    client_id = str(uuid.uuid4())
    client_lock = threading.Lock()
    
    # Authenticate via first message
    try:
        auth_msg = ws.receive(timeout=10)
        auth_data = json.loads(auth_msg)
        session_id = auth_data.get('session_id')
        
        session = validate_session(session_id)
        if not session:
            ws.send(json.dumps({'type': 'error', 'message': 'Authentication required'}))
            return
        
        username = session['user']
        subscribed_clusters = auth_data.get('clusters', None)
        
        with ws_clients_lock:
            ws_clients[client_id] = {
                'ws': ws,
                'lock': client_lock,
                'user': username,
                'clusters': subscribed_clusters,
                'connected_at': datetime.now().isoformat()
            }
        
        logging.info(f"WebSocket client connected: {username} ({client_id})")
        ws.send(json.dumps({'type': 'connected', 'client_id': client_id}))
        
        # Keep connection alive
        while True:
            try:
                # Wait for incoming messages with timeout
                msg = ws.receive(timeout=30)
                if msg is None:
                    break
                
                data = json.loads(msg)
                msg_type = data.get('type')
                
                if msg_type == 'ping':
                    with client_lock:
                        ws.send(json.dumps({'type': 'pong'}))
                elif msg_type == 'pong':
                    pass
                elif msg_type == 'subscribe':
                    with ws_clients_lock:
                        if client_id in ws_clients:
                            ws_clients[client_id]['clusters'] = data.get('clusters')
                            
            except Exception as e:
                err_str = str(e).lower()
                if 'timed out' in err_str:
                    # Send ping on timeout
                    try:
                        with client_lock:
                            ws.send(json.dumps({'type': 'ping'}))
                    except:
                        break
                else:
                    logging.debug(f"WebSocket error for {client_id}: {e}")
                    break
                
    except Exception as e:
        logging.error(f"WebSocket connection error: {e}")
    finally:
        with ws_clients_lock:
            if client_id in ws_clients:
                del ws_clients[client_id]
        logging.info(f"WebSocket client disconnected: {client_id}")

# ============================================================================
# SSE (Server-Sent Events) for live updates - more reliable than WebSocket
# SSE is a one-way connection from server to client, no threading issues
# ============================================================================
import queue as queue_module

# MK: SSE tokens - session IDs in URLs can leak to webserver logs
# MK: tokens are only valid for 5 min, way better than exposing the actual session
sse_tokens = {}  # token -> {'user': username, 'expires': timestamp, 'allowed_clusters': [...]}
sse_tokens_lock = threading.Lock()
SSE_TOKEN_TTL = 300  # 5 min

def create_sse_token(username: str, allowed_clusters: list) -> str:
    """Create SSE token - avoids session ID in URL"""
    token = base64.urlsafe_b64encode(os.urandom(24)).decode('utf-8')
    expires = time.time() + SSE_TOKEN_TTL
    
    with sse_tokens_lock:
        # cleanup expired
        now = time.time()
        expired = [t for t, data in sse_tokens.items() if data['expires'] < now]
        for t in expired:
            del sse_tokens[t]
        
        sse_tokens[token] = {
            'user': username,
            'expires': expires,
            'allowed_clusters': allowed_clusters
        }
    
    return token

def validate_sse_token(token: str) -> dict:
    """Validate an SSE token and return user info or None"""
    if not token:
        return None
    
    with sse_tokens_lock:
        token_data = sse_tokens.get(token)
        if not token_data:
            return None
        
        if token_data['expires'] < time.time():
            del sse_tokens[token]
            return None
        
        return token_data

sse_clients = {}
sse_clients_lock = threading.Lock()

def broadcast_sse(update_type: str, data: dict, cluster_id: str = None):
    """Broadcast update to SSE clients
    
    For cluster-specific events (node_status, vm_update, etc.), only sends to clients
    subscribed to that cluster. Global events (update_type starting with 'global_')
    are sent to all clients.
    """
    try:
        message = json.dumps({
            'type': update_type,
            'data': data,
            'cluster_id': cluster_id,
            'timestamp': datetime.now().isoformat()
        })
        
        # Limit message size
        if len(message) > 500000:
            return
        
        # Determine if this is a cluster-specific event
        cluster_specific_events = ['node_status', 'vm_update', 'task_update', 'metrics', 
                                   'migration', 'maintenance', 'ha_event', 'alert']
        is_cluster_specific = update_type in cluster_specific_events or cluster_id is not None
        
        with sse_clients_lock:
            for client_id, client_info in list(sse_clients.items()):
                try:
                    q = client_info.get('queue')
                    subscribed = client_info.get('clusters')
                    
                    should_send = False
                    if not is_cluster_specific:
                        # Global event - send to everyone
                        should_send = True
                    elif cluster_id and subscribed and cluster_id in subscribed:
                        # Cluster-specific event and client is subscribed
                        should_send = True
                    elif cluster_id and subscribed is None:
                        # Client didn't specify clusters - skip cluster-specific events
                        # This prevents cross-cluster pollution
                        should_send = False
                    
                    if q and should_send:
                        try:
                            q.put_nowait(message)
                        except:
                            pass  # Queue full
                except:
                    pass
    except Exception as e:
        logging.error(f"SSE broadcast error: {e}")

@app.route('/api/sse/token', methods=['POST'])
@require_auth()
def get_sse_token():
    """Get SSE token for URL param auth"""
    user = request.session.get('user', 'unknown')
    users = load_users()
    user_data = users.get(user, {})
    allowed_clusters = get_user_clusters(user_data)
    
    token = create_sse_token(user, allowed_clusters)
    
    return jsonify({
        'token': token,
        'expires_in': SSE_TOKEN_TTL,
        'hint': 'Use this token in /api/sse/updates?token=...'
    })

@app.route('/api/sse/updates')
def sse_updates():
    """SSE endpoint for live updates
    
    NS: accepts ?token= (preferred) or ?session= (legacy)
    MK: token is better because session IDs in URLs can leak to logs
    """
    # token auth first (preferred)
    sse_token = request.args.get('token')
    session_id = request.args.get('session')
    
    user = None
    allowed_clusters = None
    auth_method = None
    
    if sse_token:
        # Validate SSE token
        token_data = validate_sse_token(sse_token)
        if token_data:
            user = token_data['user']
            allowed_clusters = token_data['allowed_clusters']
            auth_method = 'token'
    
    if not user and session_id:
        # Fallback to session auth (legacy)
        session = validate_session(session_id)
        if session:
            user = session.get('user', 'unknown')
            users = load_users()
            user_data = users.get(user, {})
            allowed_clusters = get_user_clusters(user_data)
            auth_method = 'session'
    
    if not user:
        return jsonify({'error': 'Authentication required. Use token or session parameter.'}), 401
    
    client_id = str(uuid.uuid4())
    message_queue = queue_module.Queue(maxsize=100)
    
    # Get cluster subscription from query params
    clusters_param = request.args.get('clusters')
    requested_clusters = clusters_param.split(',') if clusters_param else None
    
    # MK: only let users subscribe to clusters they have access to
    if requested_clusters:
        if allowed_clusters is None:
            # admin - all clusters allowed
            subscribed_clusters = requested_clusters
        else:
            # filter to allowed only
            subscribed_clusters = [c for c in requested_clusters if c in allowed_clusters]
            if not subscribed_clusters:
                logging.warning(f"[SSE] User {user} tried to subscribe to unauthorized clusters")
                subscribed_clusters = allowed_clusters
    else:
        subscribed_clusters = allowed_clusters
    
    with sse_clients_lock:
        sse_clients[client_id] = {
            'queue': message_queue,
            'user': user,
            'clusters': subscribed_clusters,
            'connected_at': datetime.now().isoformat(),
            'auth_method': auth_method
        }
    
    logging.info(f"[SSE] Client connected: {client_id} (user: {user}, auth: {auth_method}) - Total: {len(sse_clients)}")
    
    def generate():
        try:
            # Send initial connected message
            yield f"data: {json.dumps({'type': 'connected', 'client_id': client_id})}\n\n"
            
            while True:
                try:
                    # Wait for message with timeout
                    message = message_queue.get(timeout=30)
                    yield f"data: {message}\n\n"
                except queue_module.Empty:
                    # Send keepalive
                    yield f": keepalive\n\n"
        except GeneratorExit:
            pass
        finally:
            with sse_clients_lock:
                if client_id in sse_clients:
                    del sse_clients[client_id]
            logging.info(f"[SSE] Client disconnected: {client_id} - Remaining clients: {len(sse_clients)}")
            logging.info(f"SSE client disconnected: {client_id}")
    
    response = Response(generate(), mimetype='text/event-stream')
    response.headers['Cache-Control'] = 'no-cache'
    response.headers['X-Accel-Buffering'] = 'no'
    response.headers['Connection'] = 'keep-alive'
    return response

# Background thread to broadcast resource updates via SSE
def broadcast_resources_loop():
    """Periodically broadcast resource updates to all connected SSE clients
    
    MK: Increased frequency for more responsive UI
    NS: Further optimized Jan 2026 - resources now every 2s instead of 4s
    """
    print("=" * 50)
    print("SSE BROADCAST LOOP STARTED")
    print("=" * 50)
    logging.info("SSE broadcast loop started")
    
    loop_count = 0
    while True:
        try:
            client_count = len(sse_clients)
            if not sse_clients:
                time.sleep(2)
                continue
            
            loop_count += 1
            
            if loop_count % 10 == 1:  # Log every 10th loop
                logging.debug(f"[SSE] Broadcasting to {client_count} clients (loop {loop_count})")
            
            for cluster_id, manager in list(cluster_managers.items()):
                try:
                    # SKIP OFFLINE CLUSTERS - Don't wait for timeouts!
                    if not manager.is_connected:
                        # Send empty data for offline clusters so UI knows
                        broadcast_sse('tasks', [], cluster_id)
                        continue
                    
                    # Get tasks every loop - always broadcast (even empty list)
                    tasks = manager.get_tasks(limit=50)
                    broadcast_sse('tasks', tasks or [], cluster_id)
                    
                    # Get metrics every loop
                    try:
                        metrics = manager.get_node_status()
                        if metrics:
                            broadcast_sse('metrics', metrics, cluster_id)
                    except:
                        pass
                    
                    # NS: Resources every loop now (was every 2nd loop)
                    # This makes VM status update much faster in the UI
                    try:
                        resources = manager.get_all_resources()
                        if resources:
                            broadcast_sse('resources', resources, cluster_id)
                    except:
                        pass
                        
                except Exception as e:
                    logging.debug(f"Error broadcasting updates for {cluster_id}: {e}")
            
            # NS: Reduced to 1.5 seconds for snappier updates
            # Proxmox API can handle this - it's just GET requests
            time.sleep(1.5)
                    
        except Exception as e:
            logging.error(f"Broadcast loop error: {e}")
            time.sleep(5)

# Start broadcast thread when module loads
_broadcast_thread = None

def start_broadcast_thread():
    global _broadcast_thread
    if _broadcast_thread is None or not _broadcast_thread.is_alive():
        _broadcast_thread = threading.Thread(target=broadcast_resources_loop, daemon=True)
        _broadcast_thread.start()


# =====================================================
# EMAIL STUFF - MK
# this was a nightmare to debug with all the different smtp servers
# =====================================================

def send_email(to_addresses: list, subject: str, body: str, html_body: str = None, 
               smtp_settings: dict = None) -> tuple:
    """send email via smtp"""
    # MK: spent way too long on this, every smtp server is different
    if smtp_settings:
        settings = smtp_settings
    else:
        settings = load_server_settings()
    
    if not smtp_settings and not settings.get('smtp_enabled'):
        return False, "SMTP not enabled"
    
    smtp_host = settings.get('smtp_host', '')
    smtp_port = int(settings.get('smtp_port', 587) or 587)
    smtp_user = settings.get('smtp_user', '')
    smtp_password = settings.get('smtp_password', '')
    from_email = settings.get('smtp_from_email', '')
    from_name = settings.get('smtp_from_name', '') or 'PegaProx'
    use_tls = settings.get('smtp_tls', True)
    use_ssl = settings.get('smtp_ssl', False)
    
    if not smtp_host:
        return False, "SMTP host not configured"
    if not from_email:
        return False, "From email not configured"
    
    if isinstance(to_addresses, str):
        to_addresses = [to_addresses]
    
    # print(f"sending to {to_addresses}")  # DEBUG - NS
    logging.info(f"[SMTP] sending to {to_addresses} via {smtp_host}:{smtp_port}")
    
    try:
        import smtplib
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        from email.utils import formatdate, make_msgid
        
        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = f"{from_name} <{from_email}>" if from_name else from_email
        msg['To'] = ', '.join(to_addresses)
        msg['Date'] = formatdate(localtime=True)
        msg['Message-ID'] = make_msgid(domain=from_email.split('@')[-1] if '@' in from_email else 'pegaprox.local')
        
        msg.attach(MIMEText(body, 'plain', 'utf-8'))
        
        if html_body:
            msg.attach(MIMEText(html_body, 'html', 'utf-8'))
        
        # Connect and send
        if use_ssl:
            logging.debug(f"[SMTP] Connecting with SSL to {smtp_host}:{smtp_port}")
            server = smtplib.SMTP_SSL(smtp_host, smtp_port, timeout=15)
        else:
            logging.debug(f"[SMTP] Connecting to {smtp_host}:{smtp_port}")
            server = smtplib.SMTP(smtp_host, smtp_port, timeout=15)
        
        # Debug mode disabled for production (would log passwords!)
        # server.set_debuglevel(1)
        
        # Identify ourselves to the server
        server.ehlo()
        
        if not use_ssl and use_tls:
            logging.debug("[SMTP] Starting TLS")
            server.starttls()
            server.ehlo()  # Re-identify after TLS
        
        # Authenticate if credentials provided
        if smtp_user and smtp_password and smtp_password != '********':
            logging.info(f"[SMTP] Authenticating as {smtp_user}")
            server.login(smtp_user, smtp_password)
        else:
            logging.warning(f"[SMTP] No authentication! user={bool(smtp_user)}, password={bool(smtp_password and smtp_password != '********')}")
        
        # Send the email
        refused = server.sendmail(from_email, to_addresses, msg.as_string())
        server.quit()
        
        if refused:
            logging.warning(f"[SMTP] Some recipients refused: {refused}")
            return False, f"Some recipients refused: {list(refused.keys())}"
        
        logging.info(f"[SMTP] Email sent successfully to {to_addresses}: {subject}")
        return True, None
        
    except smtplib.SMTPAuthenticationError as e:
        error = f"Authentication failed: Check username/password"
        logging.error(f"[SMTP] {error}: {e}")
        return False, error
    except smtplib.SMTPRecipientsRefused as e:
        # Get detailed error
        details = []
        for addr, (code, msg) in e.recipients.items():
            details.append(f"{addr}: {code} {msg.decode() if isinstance(msg, bytes) else msg}")
        error = f"Recipients refused: {'; '.join(details)}"
        logging.error(f"[SMTP] {error}")
        return False, error
    except smtplib.SMTPSenderRefused as e:
        error = f"Sender refused ({from_email}): {e.smtp_error.decode() if isinstance(e.smtp_error, bytes) else e.smtp_error}"
        logging.error(f"[SMTP] {error}")
        return False, error
    except smtplib.SMTPDataError as e:
        error = f"Data error: {e.smtp_error.decode() if isinstance(e.smtp_error, bytes) else e.smtp_error}"
        logging.error(f"[SMTP] {error}")
        return False, error
    except smtplib.SMTPConnectError as e:
        error = f"Connection failed to {smtp_host}:{smtp_port}"
        logging.error(f"[SMTP] {error}: {e}")
        return False, error
    except socket.timeout:
        error = f"Connection timeout to {smtp_host}:{smtp_port}"
        logging.error(f"[SMTP] {error}")
        return False, error
    except socket.gaierror as e:
        error = f"DNS resolution failed for {smtp_host}"
        logging.error(f"[SMTP] {error}: {e}")
        return False, error
    except ConnectionRefusedError:
        error = f"Connection refused by {smtp_host}:{smtp_port}"
        logging.error(f"[SMTP] {error}")
        return False, error
    except Exception as e:
        error = f"Failed to send email: {str(e)}"
        logging.error(f"[SMTP] {error}")
        return False, error


@app.route('/api/settings/smtp/test', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def test_smtp():
    """Send a test email to verify SMTP settings
    
    NS: Uses the same send_email function for consistency
    """
    data = request.json or {}
    test_email = data.get('email', '')
    
    logging.info(f"[SMTP Test] Received data: {list(data.keys())}")
    
    if not test_email:
        return jsonify({'error': 'Email address required'}), 400
    
    # Load saved settings first (we might need the real password)
    saved_settings = load_server_settings()
    
    # Build SMTP settings from request or use saved
    smtp_host = data.get('smtp_host', '')
    
    if smtp_host:
        # Use provided settings for testing (before save)
        # But if password is masked (********), use the saved password
        provided_password = data.get('smtp_password', '')
        if provided_password == '********' or not provided_password:
            # Use saved password
            real_password = saved_settings.get('smtp_password', '')
            logging.info("[SMTP Test] Using saved password (frontend sent masked value)")
        else:
            real_password = provided_password
        
        smtp_settings = {
            'smtp_host': smtp_host,
            'smtp_port': data.get('smtp_port', 587),
            'smtp_user': data.get('smtp_user', ''),
            'smtp_password': real_password,
            'smtp_from_email': data.get('smtp_from_email', ''),
            'smtp_from_name': data.get('smtp_from_name', 'PegaProx'),
            'smtp_tls': data.get('smtp_tls', True),
            'smtp_ssl': data.get('smtp_ssl', False),
        }
        
        if not smtp_settings['smtp_from_email']:
            return jsonify({'error': 'From email address is required'}), 400
        
        logging.info(f"[SMTP Test] Using settings: host={smtp_host}, user={smtp_settings['smtp_user']}, has_password={bool(real_password)}")
    else:
        # Use saved settings
        smtp_settings = None  # send_email will load from database
        if not saved_settings.get('smtp_enabled'):
            return jsonify({'error': 'SMTP not enabled. Please enable and save SMTP settings first.'}), 400
    
    # Send test email using the same function as alerts
    success, error = send_email(
        to_addresses=[test_email],
        subject='PegaProx Test Email',
        body='This is a test email from PegaProx to verify your SMTP settings are working correctly.',
        html_body='<h2>PegaProx Test Email</h2><p>This is a test email to verify your SMTP settings.</p><p style="color: green;">✓ Your SMTP configuration is working!</p>',
        smtp_settings=smtp_settings
    )
    
    if success:
        return jsonify({'success': True, 'message': f'Test email sent to {test_email}'})
    else:
        return jsonify({'error': error or 'Failed to send test email'}), 400


# =====================================================
# ALERTS SYSTEM
# =====================================================

# In-memory alert state (for cooldown tracking)
_alert_last_sent = {}  # alert_key -> timestamp

def load_alerts_config():
    """Load alerts configuration from SQLite database
    
    SQLite migration
    """
    defaults = {'alerts': [], 'enabled': True}
    
    try:
        db = get_db()
        alerts = db.get_all_alerts()
        
        if alerts:
            # Convert alerts dict to list format expected by the rest of the code
            alert_list = list(alerts.values())
            return {'alerts': alert_list, 'enabled': True}
    except Exception as e:
        logging.error(f"Error loading alerts from database: {e}")
        # Legacy fallback
        if os.path.exists(ALERTS_CONFIG_FILE):
            try:
                with open(ALERTS_CONFIG_FILE, 'r') as f:
                    return {**defaults, **json.load(f)}
            except:
                pass
    
    return defaults


def save_alerts_config(config):
    """Save alerts configuration to SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        
        # Convert alerts list to dict format for database
        alerts_dict = {}
        for alert in config.get('alerts', []):
            alert_id = alert.get('id', str(uuid.uuid4())[:8])
            alerts_dict[alert_id] = alert
        
        db.save_all_alerts(alerts_dict)
        return True
    except Exception as e:
        logging.error(f"Error saving alerts config: {e}")
        return False

def check_and_send_alerts():
    """Check all alert conditions and send notifications
    
    LW: This runs periodically in a background thread
    Checks CPU, RAM, Disk usage against thresholds
    """
    config = load_alerts_config()
    if not config.get('enabled'):
        return
    
    settings = load_server_settings()
    recipients = settings.get('alert_email_recipients', [])
    cooldown = settings.get('alert_cooldown', 300)
    
    if not recipients:
        return
    
    current_time = time.time()
    
    for alert in config.get('alerts', []):
        if not alert.get('enabled', True):
            continue
        
        alert_id = alert.get('id', '')
        cluster_id = alert.get('cluster_id', '')
        metric = alert.get('metric', '')  # cpu, memory, disk
        threshold = alert.get('threshold', 80)
        operator = alert.get('operator', '>')  # >, <, =
        target_type = alert.get('target_type', 'cluster')  # cluster, node, vm
        target_id = alert.get('target_id', '')  # node name or vmid
        
        # Check cooldown
        alert_key = f"{cluster_id}:{target_type}:{target_id}:{metric}"
        if alert_key in _alert_last_sent:
            if current_time - _alert_last_sent[alert_key] < cooldown:
                continue
        
        # Get current value
        current_value = None
        target_name = target_id
        
        if cluster_id in cluster_managers:
            manager = cluster_managers[cluster_id]
            
            if target_type == 'cluster':
                # Get cluster-wide metrics
                summary = manager.get_cluster_summary()
                if metric == 'cpu':
                    current_value = summary.get('cpu_usage', 0)
                elif metric == 'memory':
                    mem = summary.get('memory', {})
                    if mem.get('total', 0) > 0:
                        current_value = (mem.get('used', 0) / mem.get('total', 1)) * 100
                elif metric == 'disk':
                    storage = summary.get('storage', {})
                    if storage.get('total', 0) > 0:
                        current_value = (storage.get('used', 0) / storage.get('total', 1)) * 100
                target_name = manager.config.name
                
            elif target_type == 'node':
                node_summary = manager.get_node_summary(target_id)
                if metric == 'cpu':
                    current_value = node_summary.get('cpu', 0) * 100
                elif metric == 'memory':
                    mem = node_summary.get('memory', {})
                    if mem.get('total', 0) > 0:
                        current_value = (mem.get('used', 0) / mem.get('total', 1)) * 100
                elif metric == 'disk':
                    rootfs = node_summary.get('rootfs', {})
                    if rootfs.get('total', 0) > 0:
                        current_value = (rootfs.get('used', 0) / rootfs.get('total', 1)) * 100
                        
            elif target_type == 'vm':
                # Get VM metrics
                for res in manager.get_resources():
                    if str(res.get('vmid')) == str(target_id):
                        if metric == 'cpu':
                            current_value = res.get('cpu', 0) * 100
                        elif metric == 'memory':
                            if res.get('maxmem', 0) > 0:
                                current_value = (res.get('mem', 0) / res.get('maxmem', 1)) * 100
                        elif metric == 'disk':
                            if res.get('maxdisk', 0) > 0:
                                current_value = (res.get('disk', 0) / res.get('maxdisk', 1)) * 100
                        target_name = res.get('name', target_id)
                        break
        
        if current_value is None:
            continue
        
        # Check condition
        triggered = False
        if operator == '>' and current_value > threshold:
            triggered = True
        elif operator == '<' and current_value < threshold:
            triggered = True
        elif operator == '>=' and current_value >= threshold:
            triggered = True
        elif operator == '<=' and current_value <= threshold:
            triggered = True
        
        if triggered:
            # Send alert
            alert_name = alert.get('name', f'{metric} Alert')
            subject = f"[PegaProx Alert] {alert_name}"
            body = f"""
Alert: {alert_name}
Target: {target_type.capitalize()} - {target_name}
Metric: {metric.upper()}
Condition: {metric} {operator} {threshold}%
Current Value: {current_value:.1f}%
Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Cluster: {cluster_id}

This is an automated alert from PegaProx.
"""
            html_body = f"""
<h2 style="color: #e74c3c;">⚠️ PegaProx Alert: {alert_name}</h2>
<table style="border-collapse: collapse; width: 100%; max-width: 500px;">
<tr><td style="padding: 8px; border: 1px solid #ddd;"><strong>Target</strong></td><td style="padding: 8px; border: 1px solid #ddd;">{target_type.capitalize()} - {target_name}</td></tr>
<tr><td style="padding: 8px; border: 1px solid #ddd;"><strong>Metric</strong></td><td style="padding: 8px; border: 1px solid #ddd;">{metric.upper()}</td></tr>
<tr><td style="padding: 8px; border: 1px solid #ddd;"><strong>Condition</strong></td><td style="padding: 8px; border: 1px solid #ddd;">{metric} {operator} {threshold}%</td></tr>
<tr style="background-color: #fee2e2;"><td style="padding: 8px; border: 1px solid #ddd;"><strong>Current Value</strong></td><td style="padding: 8px; border: 1px solid #ddd;"><strong>{current_value:.1f}%</strong></td></tr>
<tr><td style="padding: 8px; border: 1px solid #ddd;"><strong>Time</strong></td><td style="padding: 8px; border: 1px solid #ddd;">{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</td></tr>
</table>
<p style="color: #666; font-size: 12px; margin-top: 20px;">This is an automated alert from PegaProx.</p>
"""
            
            success, error = send_email(recipients, subject, body, html_body)
            if success:
                _alert_last_sent[alert_key] = current_time
                logging.info(f"Alert sent: {alert_name} ({metric}={current_value:.1f}%)")
            elif error:
                logging.warning(f"Alert email failed: {error}")


# Alert check thread
_alert_thread = None
_alert_running = False

def alert_check_loop():
    """Background thread that checks alerts periodically"""
    global _alert_running
    _alert_running = True
    
    while _alert_running:
        try:
            check_and_send_alerts()
        except Exception as e:
            logging.error(f"Alert check error: {e}")
        
        # Check every 60 seconds
        time.sleep(60)

def start_alert_thread():
    global _alert_thread
    if _alert_thread is None or not _alert_thread.is_alive():
        _alert_thread = threading.Thread(target=alert_check_loop, daemon=True)
        _alert_thread.start()
        logging.info("Alert monitoring thread started")


# =====================================================
# PASSWORD EXPIRY CHECK - LW: Dec 2025
# Runs every 6 hours to send email warnings about expiring passwords
# NS: Compliance requirement for their compliance requirements
# =====================================================

_password_expiry_last_check = {}  # username -> last_notified_date

def check_password_expiry():
    """Check all users for expiring passwords and send email notifications
    
    MK: This runs periodically to warn users about expiring passwords
    We track last notification to avoid spamming users
    NS: Only sends once per day per user, even if check runs more often
    """
    global _password_expiry_last_check
    
    settings = load_server_settings()
    if not settings.get('password_expiry_enabled'):
        return  # feature disabled, nothing to do
    if not settings.get('password_expiry_email_enabled', True):
        return  # emails disabled but UI warning still works
    if not settings.get('smtp_enabled'):
        return  # can't send emails without SMTP config
    
    expiry_days = settings.get('password_expiry_days', 90)
    warning_days = settings.get('password_expiry_warning_days', 14)
    include_admins = settings.get('password_expiry_include_admins', False)
    
    users_db = load_users()
    today = datetime.now().date()
    
    for username, user in users_db.items():
        # skip admins unless include_admins is enabled
        # NS: this was a feature request from IT-Sec - they want admins to rotate too
        if user.get('role') == ROLE_ADMIN and not include_admins:
            continue
        if not user.get('enabled', True):
            continue
        
        email = user.get('email')
        if not email:
            continue  # no email to send to
        
        changed_at = user.get('password_changed_at')
        if not changed_at:
            # treat as expired, but dont spam about it
            continue
        
        try:
            changed_date = datetime.fromisoformat(changed_at.replace('Z', '+00:00'))
            if changed_date.tzinfo:
                changed_date = changed_date.replace(tzinfo=None)
            days_since = (datetime.now() - changed_date).days
            days_until_expiry = expiry_days - days_since
            
            # LW: Only send notifications at specific intervals, not every day
            # NS: Users complained about too many emails, so we limit it to:
            # - First warning (configured warning_days, e.g. 14 days before)
            # - 7 days before
            # - 3 days before  
            # - 1 day before
            # - When expired (then every 3 days as reminder)
            # MK: we use a set to avoid duplicates if warning_days happens to be 7, 3, or 1
            notification_days = {warning_days, 7, 3, 1}
            
            should_notify = False
            if days_until_expiry <= 0:
                # expired - check if we should remind (every 3 days)
                last_notified = _password_expiry_last_check.get(username)
                if last_notified is None:
                    should_notify = True  # first notification after expiry
                else:
                    # remind every 3 days after expiry
                    days_since_notification = (today - last_notified).days if isinstance(last_notified, date) else 999
                    if days_since_notification >= 3:
                        should_notify = True
            elif days_until_expiry in notification_days:
                # exact match on notification days (14, 7, 3, 1 before expiry)
                should_notify = True
            
            if not should_notify:
                continue
            
            # Check if we already notified today (safety check)
            last_notified = _password_expiry_last_check.get(username)
            if last_notified == today:
                continue
            
            # Send notification - bilingual DE/EN because we dont track user language
            # NS: some users might want english only but this works for everyone
            display_name = user.get('display_name', username)
            
            if days_until_expiry <= 0:
                subject = f"[PegaProx] Password expired / Passwort abgelaufen"
                body = f"""Hello {display_name},

Your PegaProx password has expired. Please change it as soon as possible.

You can still log in, but you will be prompted to change your password.

Username: {username}

Best regards,
Your PegaProx System

---

Hallo {display_name},

Ihr PegaProx-Passwort ist abgelaufen. Bitte ändern Sie es so bald wie möglich.

Sie können sich weiterhin anmelden, werden aber aufgefordert Ihr Passwort zu ändern.

Benutzername: {username}

Mit freundlichen Grüßen,
Ihr PegaProx System"""
            else:
                subject = f"[PegaProx] Password expires in {days_until_expiry} days / Passwort läuft ab"
                body = f"""Hello {display_name},

Your PegaProx password will expire in {days_until_expiry} days.

Please change your password in time to avoid any interruptions.

Username: {username}

Best regards,
Your PegaProx System

---

Hallo {display_name},

Ihr PegaProx-Passwort läuft in {days_until_expiry} Tagen ab.

Bitte ändern Sie Ihr Passwort rechtzeitig um Unterbrechungen zu vermeiden.

Benutzername: {username}

Mit freundlichen Grüßen,
Ihr PegaProx System"""
            
            success, error = send_email([email], subject, body)
            if success:
                _password_expiry_last_check[username] = today
                logging.info(f"Password expiry notification sent to {username} ({days_until_expiry} days)")
            elif error:
                logging.warning(f"Password expiry email failed for {username}: {error}")
                
        except Exception as e:
            logging.error(f"Error checking password expiry for {username}: {e}")

_password_expiry_thread = None
_password_expiry_running = False

def password_expiry_check_loop():
    """Background thread that checks password expiry daily"""
    global _password_expiry_running
    _password_expiry_running = True
    
    while _password_expiry_running:
        try:
            check_password_expiry()
        except Exception as e:
            logging.error(f"Password expiry check error: {e}")
        
        # Check once every 6 hours
        time.sleep(6 * 60 * 60)

def start_password_expiry_thread():
    global _password_expiry_thread
    if _password_expiry_thread is None or not _password_expiry_thread.is_alive():
        _password_expiry_thread = threading.Thread(target=password_expiry_check_loop, daemon=True)
        _password_expiry_thread.start()
        logging.info("Password expiry check thread started")


@app.route('/api/alerts', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_alerts():
    """Get all alert configurations"""
    return jsonify(load_alerts_config())

@app.route('/api/alerts', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_alert():
    """Create a new alert"""
    data = request.json or {}
    config = load_alerts_config()
    
    import uuid
    new_alert = {
        'id': str(uuid.uuid4())[:8],
        'name': data.get('name', 'New Alert'),
        'cluster_id': data.get('cluster_id', ''),
        'target_type': data.get('target_type', 'cluster'),
        'target_id': data.get('target_id', ''),
        'metric': data.get('metric', 'cpu'),
        'operator': data.get('operator', '>'),
        'threshold': data.get('threshold', 80),
        'enabled': data.get('enabled', True),
        'created': datetime.now().isoformat()
    }
    
    config['alerts'].append(new_alert)
    save_alerts_config(config)
    
    user = request.session.get('user', 'unknown')
    log_audit(user, 'alert.created', f"Created alert: {new_alert['name']}")
    
    return jsonify(new_alert), 201

@app.route('/api/alerts/<alert_id>', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def update_alert(alert_id):
    """Update an alert"""
    data = request.json or {}
    config = load_alerts_config()
    
    for alert in config['alerts']:
        if alert['id'] == alert_id:
            alert.update({
                'name': data.get('name', alert['name']),
                'cluster_id': data.get('cluster_id', alert['cluster_id']),
                'target_type': data.get('target_type', alert['target_type']),
                'target_id': data.get('target_id', alert['target_id']),
                'metric': data.get('metric', alert['metric']),
                'operator': data.get('operator', alert['operator']),
                'threshold': data.get('threshold', alert['threshold']),
                'enabled': data.get('enabled', alert['enabled']),
            })
            save_alerts_config(config)
            return jsonify(alert)
    
    return jsonify({'error': 'Alert not found'}), 404

@app.route('/api/alerts/<alert_id>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def delete_alert(alert_id):
    """Delete an alert"""
    config = load_alerts_config()
    config['alerts'] = [a for a in config['alerts'] if a['id'] != alert_id]
    save_alerts_config(config)
    
    user = request.session.get('user', 'unknown')
    log_audit(user, 'alert.deleted', f"Deleted alert: {alert_id}")
    
    return jsonify({'success': True})


# =====================================================
# SCHEDULED TASKS SYSTEM - MK: Dec 2025
# =====================================================

def load_scheduled_tasks():
    """Load scheduled tasks from SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM scheduled_tasks')
        
        tasks = []
        for row in cursor.fetchall():
            tasks.append({
                'id': row['id'],
                'cluster_id': row['cluster_id'],
                'name': row['name'],
                'task_type': row['task_type'],
                'schedule': row['schedule'],
                'config': json.loads(row['config'] or '{}'),
                'enabled': bool(row['enabled']),
                'last_run': row['last_run'],
                'next_run': row['next_run'],
            })
        
        return {'tasks': tasks}
    except Exception as e:
        logging.error(f"Error loading scheduled tasks from database: {e}")
        # Legacy fallback
        if os.path.exists(SCHEDULED_TASKS_FILE):
            try:
                with open(SCHEDULED_TASKS_FILE, 'r') as f:
                    return json.load(f)
            except:
                pass
    return {'tasks': []}


def save_scheduled_tasks(config):
    """Save scheduled tasks to SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        now = datetime.now().isoformat()
        
        # Clear existing tasks (simple approach)
        cursor.execute('DELETE FROM scheduled_tasks')
        
        for task in config.get('tasks', []):
            task_id = task.get('id', str(uuid.uuid4())[:8])
            cursor.execute('''
                INSERT INTO scheduled_tasks
                (id, cluster_id, name, task_type, schedule, config, 
                 enabled, last_run, next_run, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                task_id,
                task.get('cluster_id'),
                task.get('name', ''),
                task.get('task_type', task.get('action', '')),
                json.dumps({
                    'schedule_type': task.get('schedule_type', 'daily'),
                    'schedule_time': task.get('schedule_time', '02:00'),
                    'schedule_day': task.get('schedule_day', 0),
                }),
                json.dumps(task.get('config', {})),
                1 if task.get('enabled', True) else 0,
                task.get('last_run'),
                task.get('next_run'),
                now
            ))
        
        db.conn.commit()
        return True
    except Exception as e:
        logging.error(f"Error saving scheduled tasks: {e}")
        return False

def run_scheduled_tasks():
    """Check and execute due scheduled tasks
    
    LW: Runs every minute, checks if any tasks are due
    Supported actions: start, stop, restart, snapshot, backup
    """
    config = load_scheduled_tasks()
    current_time = datetime.now()
    
    for task in config.get('tasks', []):
        if not task.get('enabled', True):
            continue
        
        # Check if task is due
        schedule_type = task.get('schedule_type', 'daily')
        schedule_time = task.get('schedule_time', '02:00')
        schedule_day = task.get('schedule_day', 0)  # 0=Monday for weekly
        last_run = task.get('last_run')
        
        should_run = False
        
        try:
            hour, minute = map(int, schedule_time.split(':'))
            
            if schedule_type == 'hourly':
                # Run every hour at specified minute
                if current_time.minute == minute:
                    if not last_run or (datetime.fromisoformat(last_run) + timedelta(hours=1)) <= current_time:
                        should_run = True
                        
            elif schedule_type == 'daily':
                # Run once a day at specified time
                if current_time.hour == hour and current_time.minute == minute:
                    if not last_run or datetime.fromisoformat(last_run).date() < current_time.date():
                        should_run = True
                        
            elif schedule_type == 'weekly':
                # Run once a week on specified day and time
                if current_time.weekday() == schedule_day and current_time.hour == hour and current_time.minute == minute:
                    if not last_run or (datetime.fromisoformat(last_run) + timedelta(days=7)) <= current_time:
                        should_run = True
                        
            elif schedule_type == 'monthly':
                # Run on specified day of month
                if current_time.day == schedule_day and current_time.hour == hour and current_time.minute == minute:
                    if not last_run or datetime.fromisoformat(last_run).month != current_time.month:
                        should_run = True
                        
        except Exception as e:
            logging.error(f"Error parsing schedule for task {task.get('id')}: {e}")
            continue
        
        if should_run:
            execute_scheduled_task(task)
            # Update last_run
            task['last_run'] = current_time.isoformat()
            save_scheduled_tasks(config)

def execute_scheduled_task(task):
    """Execute a scheduled task"""
    cluster_id = task.get('cluster_id', '')
    action = task.get('action', '')
    target_type = task.get('target_type', 'vm')
    target_id = task.get('target_id', '')
    target_node = task.get('target_node', '')
    
    if cluster_id not in cluster_managers:
        logging.error(f"Scheduled task failed: Cluster {cluster_id} not found")
        return
    
    manager = cluster_managers[cluster_id]
    logging.info(f"Executing scheduled task: {task.get('name')} - {action} on {target_type}/{target_id}")
    
    try:
        if action == 'start':
            manager.start_vm(target_node, int(target_id), target_type)
        elif action == 'stop':
            manager.stop_vm(target_node, int(target_id), target_type)
        elif action == 'restart':
            manager.restart_vm(target_node, int(target_id), target_type)
        elif action == 'shutdown':
            manager.shutdown_vm(target_node, int(target_id), target_type)
        elif action == 'snapshot':
            snap_name = f"scheduled_{datetime.now().strftime('%Y%m%d_%H%M')}"
            manager.create_snapshot(target_node, int(target_id), target_type, snap_name, 'Scheduled snapshot', False)
        elif action == 'backup':
            # Trigger backup job
            storage = task.get('backup_storage', 'local')
            manager.backup_vm(target_node, int(target_id), target_type, storage)
        
        log_audit('scheduler', 'scheduled_task.executed', f"Task '{task.get('name')}' executed: {action} on {target_type}/{target_id}")
        
    except Exception as e:
        logging.error(f"Scheduled task failed: {e}")
        log_audit('scheduler', 'scheduled_task.failed', f"Task '{task.get('name')}' failed: {e}")

# Scheduler thread
_scheduler_thread = None
_scheduler_running = False

def scheduler_loop():
    """Background thread that runs scheduled tasks"""
    global _scheduler_running
    _scheduler_running = True
    
    while _scheduler_running:
        try:
            run_scheduled_tasks()
        except Exception as e:
            logging.error(f"Scheduler error: {e}")
        
        # Check every 60 seconds
        time.sleep(60)

def start_scheduler_thread():
    global _scheduler_thread
    if _scheduler_thread is None or not _scheduler_thread.is_alive():
        _scheduler_thread = threading.Thread(target=scheduler_loop, daemon=True)
        _scheduler_thread.start()
        logging.info("Task scheduler thread started")


@app.route('/api/scheduled-tasks', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_scheduled_tasks():
    """Get all scheduled tasks"""
    return jsonify(load_scheduled_tasks())

@app.route('/api/scheduled-tasks', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_scheduled_task():
    """Create a new scheduled task"""
    data = request.json or {}
    config = load_scheduled_tasks()
    
    import uuid
    new_task = {
        'id': str(uuid.uuid4())[:8],
        'name': data.get('name', 'New Task'),
        'cluster_id': data.get('cluster_id', ''),
        'target_type': data.get('target_type', 'qemu'),
        'target_id': data.get('target_id', ''),
        'target_node': data.get('target_node', ''),
        'action': data.get('action', 'snapshot'),  # start, stop, restart, snapshot, backup
        'schedule_type': data.get('schedule_type', 'daily'),  # hourly, daily, weekly, monthly
        'schedule_time': data.get('schedule_time', '02:00'),
        'schedule_day': data.get('schedule_day', 0),
        'backup_storage': data.get('backup_storage', 'local'),
        'enabled': data.get('enabled', True),
        'last_run': None,
        'created': datetime.now().isoformat()
    }
    
    config['tasks'].append(new_task)
    save_scheduled_tasks(config)
    
    user = request.session.get('user', 'unknown')
    log_audit(user, 'scheduled_task.created', f"Created scheduled task: {new_task['name']}")
    
    return jsonify(new_task), 201

@app.route('/api/scheduled-tasks/<task_id>', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def update_scheduled_task(task_id):
    """Update a scheduled task"""
    data = request.json or {}
    config = load_scheduled_tasks()
    
    for task in config['tasks']:
        if task['id'] == task_id:
            task.update({
                'name': data.get('name', task['name']),
                'cluster_id': data.get('cluster_id', task['cluster_id']),
                'target_type': data.get('target_type', task['target_type']),
                'target_id': data.get('target_id', task['target_id']),
                'target_node': data.get('target_node', task['target_node']),
                'action': data.get('action', task['action']),
                'schedule_type': data.get('schedule_type', task['schedule_type']),
                'schedule_time': data.get('schedule_time', task['schedule_time']),
                'schedule_day': data.get('schedule_day', task['schedule_day']),
                'backup_storage': data.get('backup_storage', task.get('backup_storage', 'local')),
                'enabled': data.get('enabled', task['enabled']),
            })
            save_scheduled_tasks(config)
            return jsonify(task)
    
    return jsonify({'error': 'Task not found'}), 404

@app.route('/api/scheduled-tasks/<task_id>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def delete_scheduled_task(task_id):
    """Delete a scheduled task"""
    config = load_scheduled_tasks()
    config['tasks'] = [t for t in config['tasks'] if t['id'] != task_id]
    save_scheduled_tasks(config)
    
    user = request.session.get('user', 'unknown')
    log_audit(user, 'scheduled_task.deleted', f"Deleted scheduled task: {task_id}")
    
    return jsonify({'success': True})

@app.route('/api/scheduled-tasks/<task_id>/run', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def run_scheduled_task_now(task_id):
    """Run a scheduled task immediately"""
    config = load_scheduled_tasks()
    
    for task in config['tasks']:
        if task['id'] == task_id:
            execute_scheduled_task(task)
            task['last_run'] = datetime.now().isoformat()
            save_scheduled_tasks(config)
            return jsonify({'success': True, 'message': f"Task '{task['name']}' executed"})
    
    return jsonify({'error': 'Task not found'}), 404


# =====================================================
# MIGRATION HISTORY - LW: Dec 2025
# SQLite version
# =====================================================

def load_migration_history():
    """Load migration history from SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        cursor.execute('SELECT * FROM migration_history ORDER BY timestamp DESC LIMIT 1000')
        
        migrations = []
        for row in cursor.fetchall():
            migrations.append({
                'id': row['id'],
                'cluster_id': row['cluster_id'],
                'vmid': row['vmid'],
                'vm_name': row['vm_name'],
                'source_node': row['source_node'],
                'target_node': row['target_node'],
                'reason': row['reason'],
                'status': row['status'],
                'duration': row['duration_seconds'],
                'timestamp': row['timestamp'],
            })
        
        return {'migrations': migrations}
    except Exception as e:
        logging.error(f"Error loading migration history from database: {e}")
        # Legacy fallback
        if os.path.exists(MIGRATION_HISTORY_FILE):
            try:
                with open(MIGRATION_HISTORY_FILE, 'r') as f:
                    return json.load(f)
            except:
                pass
    return {'migrations': []}


def save_migration_history(config):
    """Save migration history - now handled per-entry via log_migration()
    
    saves directly to db
    This function is kept for backwards compatibility
    """
    # In SQLite version, saving is handled per-entry in log_migration()
    pass


def log_migration(cluster_id: str, vmid: int, vm_name: str, vm_type: str, 
                  source_node: str, target_node: str, migration_type: str,
                  status: str, user: str = 'system', duration: float = 0):
    """Log a VM migration event to SQLite database
    
    MK: Called from migrate_vm and HA failover functions
    writes to db now
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()
        
        cursor.execute('''
            INSERT INTO migration_history
            (cluster_id, vmid, vm_name, source_node, target_node, 
             reason, status, duration_seconds, timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            cluster_id,
            vmid,
            vm_name,
            source_node,
            target_node,
            f"{migration_type} by {user}",
            status,
            duration,
            datetime.now().isoformat()
        ))
        
        db.conn.commit()
        
        # Cleanup old entries (keep last 1000)
        cursor.execute('''
            DELETE FROM migration_history 
            WHERE id NOT IN (
                SELECT id FROM migration_history 
                ORDER BY timestamp DESC LIMIT 1000
            )
        ''')
        db.conn.commit()
        
    except Exception as e:
        logging.error(f"Error logging migration: {e}")
    
    return {
        'cluster_id': cluster_id,
        'vmid': vmid,
        'vm_name': vm_name,
        'source_node': source_node,
        'target_node': target_node,
        'migration_type': migration_type,
        'status': status,
        'user': user,
        'duration': duration,
        'timestamp': datetime.now().isoformat()
    }
    return entry

@app.route('/api/migration-history', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_migration_history():
    """Get migration history"""
    config = load_migration_history()
    
    # Optional filters
    cluster_id = request.args.get('cluster_id')
    vmid = request.args.get('vmid')
    limit = int(request.args.get('limit', 100))
    
    migrations = config.get('migrations', [])
    
    if cluster_id:
        migrations = [m for m in migrations if m.get('cluster_id') == cluster_id]
    if vmid:
        migrations = [m for m in migrations if str(m.get('vmid')) == str(vmid)]
    
    return jsonify(migrations[:limit])

@app.route('/api/clusters/<cluster_id>/vms/<int:vmid>/migration-history', methods=['GET'])
@require_auth(perms=['vm.view'])
def get_vm_migration_history(cluster_id, vmid):
    """Get migration history for a specific VM"""
    config = load_migration_history()
    
    migrations = [m for m in config.get('migrations', []) 
                  if m.get('cluster_id') == cluster_id and m.get('vmid') == vmid]
    
    return jsonify(migrations)


# =====================================================
# AFFINITY RULES - keeps VMs together or apart
# =====================================================

def load_affinity_rules():
    """Load affinity rules from SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        rules_by_cluster = db.get_affinity_rules()
        
        # Convert to flat list format
        all_rules = []
        for cluster_id, rules in rules_by_cluster.items():
            for rule in rules:
                rule['cluster_id'] = cluster_id
                all_rules.append(rule)
        
        return {'rules': all_rules}
    except Exception as e:
        logging.error(f"Error loading affinity rules: {e}")
        # Legacy fallback
        if os.path.exists(AFFINITY_RULES_FILE):
            try:
                with open(AFFINITY_RULES_FILE, 'r') as f:
                    return json.load(f)
            except:
                pass
    return {'rules': []}


def save_affinity_rules(config):
    """Save affinity rules to SQLite database
    
    SQLite migration
    """
    try:
        db = get_db()
        
        # Group rules by cluster_id
        rules_by_cluster = {}
        for rule in config.get('rules', []):
            cluster_id = rule.get('cluster_id', 'default')
            if cluster_id not in rules_by_cluster:
                rules_by_cluster[cluster_id] = []
            rules_by_cluster[cluster_id].append(rule)
        
        db.save_all_affinity_rules(rules_by_cluster)
        return True
    except Exception as e:
        logging.error(f"Error saving affinity rules: {e}")
        return False

def check_affinity_violation(cluster_id: str, vmid: int, target_node: str) -> dict:
    """Check if moving a VM to a node would violate affinity rules
    
    Returns: {'violation': True/False, 'rule': rule_name, 'message': description}
    """
    config = load_affinity_rules()
    
    if cluster_id not in cluster_managers:
        return {'violation': False}
    
    manager = cluster_managers[cluster_id]
    resources = manager.get_resources()
    
    # Build map of VM -> current node
    vm_nodes = {}
    for res in resources:
        if res.get('type') in ['qemu', 'lxc']:
            vm_nodes[str(res.get('vmid'))] = res.get('node')
    
    for rule in config.get('rules', []):
        if rule.get('cluster_id') != cluster_id or not rule.get('enabled', True):
            continue
        
        rule_type = rule.get('type', 'together')  # together, separate
        vm_ids = [str(v) for v in rule.get('vm_ids', [])]
        
        if str(vmid) not in vm_ids:
            continue
        
        # Get nodes of other VMs in this rule
        other_nodes = set()
        for vid in vm_ids:
            if vid != str(vmid) and vid in vm_nodes:
                other_nodes.add(vm_nodes[vid])
        
        if rule_type == 'together':
            # All VMs should be on same node
            if other_nodes and target_node not in other_nodes:
                return {
                    'violation': True,
                    'rule': rule.get('name', 'Affinity Rule'),
                    'message': f"VM must stay with VMs {', '.join([v for v in vm_ids if v != str(vmid)])} on node {list(other_nodes)[0]}"
                }
                
        elif rule_type == 'separate':
            # VMs should be on different nodes
            if target_node in other_nodes:
                return {
                    'violation': True,
                    'rule': rule.get('name', 'Anti-Affinity Rule'),
                    'message': f"VM must not be on the same node as VMs {', '.join([v for v in vm_ids if v != str(vmid) and vm_nodes.get(v) == target_node])}"
                }
    
    return {'violation': False}

@app.route('/api/affinity-rules', methods=['GET'])
@require_auth(perms=['cluster.view'])
def get_affinity_rules():
    """Get all affinity rules"""
    return jsonify(load_affinity_rules())

@app.route('/api/affinity-rules', methods=['POST'])
@require_auth(roles=[ROLE_ADMIN])
def create_affinity_rule():
    """Create a new affinity rule"""
    data = request.json or {}
    config = load_affinity_rules()
    
    import uuid
    new_rule = {
        'id': str(uuid.uuid4())[:8],
        'name': data.get('name', 'New Rule'),
        'cluster_id': data.get('cluster_id', ''),
        'type': data.get('type', 'together'),  # together, separate
        'vm_ids': data.get('vm_ids', []),
        'enabled': data.get('enabled', True),
        'enforce': data.get('enforce', False),  # If true, block migrations that violate
        'created': datetime.now().isoformat()
    }
    
    config['rules'].append(new_rule)
    save_affinity_rules(config)
    
    user = request.session.get('user', 'unknown')
    log_audit(user, 'affinity_rule.created', f"Created affinity rule: {new_rule['name']}")
    
    return jsonify(new_rule), 201

@app.route('/api/affinity-rules/<rule_id>', methods=['PUT'])
@require_auth(roles=[ROLE_ADMIN])
def update_affinity_rule(rule_id):
    """Update an affinity rule"""
    data = request.json or {}
    config = load_affinity_rules()
    
    for rule in config['rules']:
        if rule['id'] == rule_id:
            rule.update({
                'name': data.get('name', rule['name']),
                'cluster_id': data.get('cluster_id', rule['cluster_id']),
                'type': data.get('type', rule['type']),
                'vm_ids': data.get('vm_ids', rule['vm_ids']),
                'enabled': data.get('enabled', rule['enabled']),
                'enforce': data.get('enforce', rule.get('enforce', False)),
            })
            save_affinity_rules(config)
            return jsonify(rule)
    
    return jsonify({'error': 'Rule not found'}), 404

@app.route('/api/affinity-rules/<rule_id>', methods=['DELETE'])
@require_auth(roles=[ROLE_ADMIN])
def delete_affinity_rule(rule_id):
    """Delete an affinity rule"""
    config = load_affinity_rules()
    config['rules'] = [r for r in config['rules'] if r['id'] != rule_id]
    save_affinity_rules(config)
    
    user = request.session.get('user', 'unknown')
    log_audit(user, 'affinity_rule.deleted', f"Deleted affinity rule: {rule_id}")
    
    return jsonify({'success': True})

@app.route('/api/clusters/<cluster_id>/vms/<int:vmid>/check-affinity/<target_node>', methods=['GET'])
@require_auth(perms=['vm.view'])
def check_vm_affinity(cluster_id, vmid, target_node):
    """Check if moving VM to target node would violate affinity rules"""
    result = check_affinity_violation(cluster_id, vmid, target_node)
    return jsonify(result)


def main(debug_mode=False):
    """
    Main entry point
    
    NS: Startup sequence:
    1. Load users & audit log
    2. Load sessions (now persisted!)
    3. Load cluster configs
    4. Start manager thread for each cluster
    5. Start WebSocket broadcast thread
    6. Start alert monitoring thread
    7. Start task scheduler thread
    8. Run Flask/gevent server
    
    Args:
        debug_mode: If True, show DEBUG level logs. Default is WARNING.
    """
    global users_db
    
    # Configure logging based on debug mode
    log_level = logging.DEBUG if debug_mode else logging.WARNING
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s [%(name)s] %(levelname)s: %(message)s' if debug_mode else '%(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Suppress noisy loggers unless in debug mode
    if not debug_mode:
        logging.getLogger('werkzeug').setLevel(logging.ERROR)
        logging.getLogger('gevent').setLevel(logging.ERROR)
        logging.getLogger('urllib3').setLevel(logging.ERROR)
    
    if debug_mode:
        print("="*50)
        print("DEBUG MODE ENABLED")
        print("="*50)
    
    # Check optional libraries
    print("\nChecking optional libraries...")
    missing_libs = []
    try:
        import websockets
        print("  ✓ websockets (VNC/SSH console)")
    except ImportError:
        missing_libs.append('websockets')
        print("  ✗ websockets - VNC/SSH console will NOT work!")
    
    try:
        import paramiko
        print("  ✓ paramiko (SSH features)")
    except ImportError:
        missing_libs.append('paramiko')
        print("  ✗ paramiko - SSH features disabled")
    
    if GEVENT_AVAILABLE:
        print("  ✓ gevent (high performance)")
    else:
        print("  ✗ gevent - using Flask dev server (slower)")
    
    if ARGON2_AVAILABLE:
        print("  ✓ argon2-cffi (secure password hashing)")
    else:
        print("  ⚠ argon2-cffi - using PBKDF2 fallback")
    
    if missing_libs:
        print(f"\n  Install missing: pip install {' '.join(missing_libs)}")
    print()
    
    # init user system
    print("Initializing user system...")
    users_db = load_users()
    print(f"Loaded {len(users_db)} users")
    
    # init audit log
    print("Initializing audit log...")
    load_audit_log()
    print(f"Loaded {len(audit_log)} audit entries (retention: {AUDIT_RETENTION_DAYS} days)")
    
    # Load sessions from disk (new in Dec 2025)
    # LW: sessions now survive server restarts, finally!
    print("Loading sessions...")
    load_sessions()
    print(f"Loaded {len(active_sessions)} active sessions")
    
    # Show default credentials hint if only default user exists
    if len(users_db) == 1 and 'pegaprox' in users_db:
        print("\n" + "="*50)
        print("DEFAULT LOGIN CREDENTIALS:")
        print("  Username: pegaprox")
        print("  Password: admin")
        print("  Please change the password after first login!")
        print("="*50 + "\n")
    
    # Load existing configuration
    config = load_config()
    
    # Start managers for existing clusters
    for cluster_id, cluster_data in config.items():
        config_obj = PegaProxConfig(cluster_data)
        manager = PegaProxManager(cluster_id, config_obj)
        manager.start()
        cluster_managers[cluster_id] = manager
        print(f"Started PegaProx manager for cluster: {cluster_data['name']}")
    
    # Start WebSocket broadcast thread
    start_broadcast_thread()
    print("Started WebSocket live updates broadcast thread")
    
    # Start alert monitoring thread
    start_alert_thread()
    print("Started alert monitoring thread")
    
    # Start task scheduler thread
    start_scheduler_thread()
    print("Started task scheduler thread")
    
    # Start password expiry check thread
    start_password_expiry_thread()
    print("Started password expiry check thread")
    
    # MK: Warm up pool membership cache for all clusters
    # This prevents API calls during the first VM actions
    def warmup_pool_cache():
        time.sleep(5)  # Wait for cluster connections to establish
        for cluster_id in cluster_managers:
            try:
                get_pool_membership_cache(cluster_id)
                print(f"  Pool cache warmed for cluster: {cluster_id}")
            except Exception as e:
                print(f"  Warning: Could not warm pool cache for {cluster_id}: {e}")
    
    threading.Thread(target=warmup_pool_cache, daemon=True).start()
    print("Started pool cache warmup thread")
    
    # Load server settings
    server_settings = load_server_settings()
    port = server_settings.get('port', 5000)  # default 5000
    ssl_enabled = server_settings.get('ssl_enabled', False)
    domain = server_settings.get('domain', '')
    app_name = server_settings.get('app_name', 'PegaProx')  # NS: For SSL cert
    # debug = server_settings.get('debug', False)  # not used rn
    
    # Check for SSL certificates
    ssl_context = None
    
    # First check custom SSL certificates from settings
    if ssl_enabled and os.path.exists(SSL_CERT_FILE) and os.path.exists(SSL_KEY_FILE):
        ssl_context = (SSL_CERT_FILE, SSL_KEY_FILE)
        print(f"Custom SSL certificates found - starting with HTTPS")
    else:
        # Fall back to default location
        # MK: this is for backwards compat with old installs
        cert_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'cert.pem')
        key_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'key.pem')
        
        if os.path.exists(cert_file) and os.path.exists(key_file):
            ssl_context = (cert_file, key_file)
            print(f"SSL certificates found - starting with HTTPS")
        else:
            # Generate self-signed certificate
            print("No SSL certificates found. Generating self-signed certificate...")
            try:
                from OpenSSL import crypto
                
                # Generate key
                key = crypto.PKey()
                key.generate_key(crypto.TYPE_RSA, 2048)
                
                # Generate certificate - NS: Uses app_name from settings
                cert = crypto.X509()
                cert.get_subject().C = "DE"
                cert.get_subject().ST = "State"
                cert.get_subject().L = "City"
                cert.get_subject().O = app_name or "PegaProx"
                cert.get_subject().OU = app_name or "PegaProx"
                # use domain if configured, otherwise app_name for better recognition
                cert.get_subject().CN = domain or app_name or "PegaProx"
                cert.set_serial_number(1000)
                cert.gmtime_adj_notBefore(0)
                cert.gmtime_adj_notAfter(365*24*60*60)  # 1 year
                cert.set_issuer(cert.get_subject())
                cert.set_pubkey(key)
                cert.sign(key, 'sha256')
                
                # Save certificate and key
                with open(cert_file, "wb") as f:
                    f.write(crypto.dump_certificate(crypto.FILETYPE_PEM, cert))
                with open(key_file, "wb") as f:
                    f.write(crypto.dump_privatekey(crypto.FILETYPE_PEM, key))
                
                ssl_context = (cert_file, key_file)
                print(f"Self-signed certificate generated: {cert_file}")
                
            except ImportError:
                print("WARNING: pyOpenSSL not installed. Run: pip install pyOpenSSL")
                print("Starting without HTTPS (noVNC may not work)")
            except Exception as e:
                print(f"WARNING: Could not generate SSL certificate: {e}")
                print("Starting without HTTPS (noVNC may not work)")
    
    # Start HTTP redirect server if SSL is enabled
    # configurable via settings or environment variable
    http_redirect_port = server_settings.get('http_redirect_port', 0)  # 0 = auto, -1 = disabled
    if http_redirect_port == 0:
        # Auto mode: try port 80 if root, otherwise skip
        http_redirect_port = 80 if os.geteuid() == 0 else -1
    
    # Allow override via env var
    http_redirect_port = int(os.environ.get('PEGAPROX_HTTP_PORT', http_redirect_port))
    
    if ssl_context and http_redirect_port > 0:
        def start_http_redirect():
            """Start a simple HTTP server that redirects to HTTPS using raw sockets"""
            import socket
            
            try:
                # Create socket
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                sock.bind(('0.0.0.0', http_redirect_port))
                sock.listen(100)
                sock.settimeout(1.0)  # Allow periodic checks
                
                print(f"HTTP redirect server listening on port {http_redirect_port}")
                
                while True:
                    try:
                        client, addr = sock.accept()
                        client.settimeout(5.0)
                        
                        try:
                            # Read HTTP request (we only need first line)
                            request = client.recv(4096).decode('utf-8', errors='ignore')
                            
                            # Parse path from request
                            path = '/'
                            if request:
                                first_line = request.split('\r\n')[0]
                                parts = first_line.split(' ')
                                if len(parts) >= 2:
                                    path = parts[1]
                            
                            # Parse host header - MK: Fixed to handle port in host
                            host_header = ''
                            for line in request.split('\r\n'):
                                if line.lower().startswith('host:'):
                                    host_value = line.split(':', 1)[1].strip()
                                    # Remove port if present
                                    if ':' in host_value:
                                        host_header = host_value.rsplit(':', 1)[0]
                                    else:
                                        host_header = host_value
                                    break
                            
                            # Use configured domain or host header
                            # MK: Also strip port from domain if present
                            redirect_host = host_header or 'localhost'
                            if domain:
                                if ':' in domain and not domain.startswith('['):
                                    redirect_host = domain.rsplit(':', 1)[0]
                                else:
                                    redirect_host = domain
                            
                            # Build redirect URL
                            if port == 443:
                                redirect_url = f'https://{redirect_host}{path}'
                            else:
                                redirect_url = f'https://{redirect_host}:{port}{path}'
                            
                            # Send redirect response
                            response = (
                                f"HTTP/1.1 301 Moved Permanently\r\n"
                                f"Location: {redirect_url}\r\n"
                                f"Content-Length: 0\r\n"
                                f"Connection: close\r\n"
                                f"\r\n"
                            )
                            client.sendall(response.encode())
                        except Exception as e:
                            pass  # Ignore client errors
                        finally:
                            try:
                                client.close()
                            except:
                                pass
                                
                    except socket.timeout:
                        continue  # Just check again
                    except Exception as e:
                        if 'Bad file descriptor' not in str(e):
                            logging.debug(f"HTTP redirect accept error: {e}")
                        continue
                        
            except PermissionError:
                print(f"WARNING: Cannot bind to port {http_redirect_port} (requires root). HTTP redirect not available.")
            except OSError as e:
                if 'Address already in use' in str(e):
                    print(f"WARNING: Port {http_redirect_port} already in use. HTTP redirect not available.")
                else:
                    print(f"WARNING: HTTP redirect server failed: {e}")
            except Exception as e:
                print(f"WARNING: HTTP redirect server failed: {e}")
        
        # Start redirect server in a thread
        redirect_thread = threading.Thread(target=start_http_redirect, daemon=True)
        redirect_thread.start()
        print(f"Started additional HTTP → HTTPS redirect on port {http_redirect_port}")
    # Note: We no longer print "HTTP redirect disabled" because protocol detection on main port handles same-port redirects
    
    # Determine number of workers based on CPU cores
    cpu_count = multiprocessing.cpu_count()
    workers = int(os.environ.get('PEGAPROX_WORKERS', min(cpu_count * 2, 8)))  # Max 8 workers
    
    # Check for production server mode
    use_gevent = os.environ.get('PEGAPROX_SERVER', 'auto').lower()
    
    print(f"System: {cpu_count} CPU cores detected")
    print(f"Memory optimization: Garbage collection tuned for {workers} workers")
    
    # Tune garbage collection for better memory usage
    gc.set_threshold(700, 10, 10)
    
    # Production server with Gevent (best for I/O bound operations like API calls)
    if use_gevent == 'gevent' or (use_gevent == 'auto' and GEVENT_AVAILABLE):
        if GEVENT_AVAILABLE:
            print(f"Starting PegaProx with Gevent WSGIServer ({workers} greenlets)")
            print("Mode: Production (async I/O optimized)")
            
            # NS: Suppress noisy errors from bots/scanners/disconnects
            import logging as log_module
            log_module.getLogger('gevent').setLevel(log_module.CRITICAL)
            log_module.getLogger('gevent.pywsgi').setLevel(log_module.CRITICAL)
            # MK: websockets module spams ERROR logs when bots hit the VNC endpoint
            log_module.getLogger('websockets').setLevel(log_module.CRITICAL)
            log_module.getLogger('websockets.server').setLevel(log_module.CRITICAL)
            log_module.getLogger('websockets.asyncio').setLevel(log_module.CRITICAL)
            
            # LW: Monkey-patch traceback to suppress SSL errors
            # gevent uses traceback.print_exception directly, bypassing logging
            import traceback as tb_module
            _original_print_exception = tb_module.print_exception
            _original_print_exc = tb_module.print_exc
            _original_format_exception = tb_module.format_exception
            
            def quiet_print_exception(exc, value=None, tb=None, limit=None, file=None, chain=True):
                # Check if this is an SSL error
                exc_type = exc if isinstance(exc, type) else type(exc)
                if exc_type and 'ssl' in exc_type.__name__.lower():
                    return  # Silently ignore
                if value and 'ssl' in str(value).lower():
                    return
                _original_print_exception(exc, value, tb, limit, file, chain)
            
            def quiet_print_exc(limit=None, file=None, chain=True):
                exc_type, exc_value, exc_tb = sys.exc_info()
                if exc_type and 'ssl' in exc_type.__name__.lower():
                    return
                _original_print_exc(limit, file, chain)
            
            def quiet_format_exception(exc, value=None, tb=None, limit=None, chain=True):
                exc_type = exc if isinstance(exc, type) else type(exc)
                if exc_type and 'ssl' in exc_type.__name__.lower():
                    return []  # Return empty list
                return _original_format_exception(exc, value, tb, limit, chain)
            
            tb_module.print_exception = quiet_print_exception
            tb_module.print_exc = quiet_print_exc
            tb_module.format_exception = quiet_format_exception
            
            # NS: Also filter stderr directly as last resort
            import io
            class SSLFilteredStderr:
                def __init__(self, original):
                    self._original = original
                    self._buffer = []
                    self._in_ssl_traceback = False
                
                def write(self, text):
                    # Check if this starts an SSL traceback
                    if 'Traceback (most recent call last):' in text:
                        self._in_ssl_traceback = False  # Reset
                        self._buffer = [text]
                        return len(text)
                    
                    if self._buffer:
                        self._buffer.append(text)
                        # Check if we've seen an SSL error
                        full_text = ''.join(self._buffer)
                        if 'SSLEOFError' in full_text or 'ssl.SSL' in full_text:
                            self._in_ssl_traceback = True
                        # If we hit end of traceback (line starting without space that's not Traceback)
                        if text.strip() and not text.startswith(' ') and not text.startswith('Traceback'):
                            if self._in_ssl_traceback:
                                self._buffer = []  # Discard SSL traceback
                                self._in_ssl_traceback = False
                                return len(text)
                            else:
                                # Not SSL, print buffered content
                                for line in self._buffer:
                                    self._original.write(line)
                                self._buffer = []
                        return len(text)
                    
                    # Normal output
                    return self._original.write(text)
                
                def flush(self):
                    # Flush buffer if not in SSL traceback
                    if self._buffer and not self._in_ssl_traceback:
                        for line in self._buffer:
                            self._original.write(line)
                    self._buffer = []
                    self._original.flush()
                
                def __getattr__(self, name):
                    return getattr(self._original, name)
            
            sys.stderr = SSLFilteredStderr(sys.stderr)
            
            # Import WebSocket handler for gevent
            try:
                from geventwebsocket.handler import WebSocketHandler
                use_websocket_handler = True
                print("WebSocket support: geventwebsocket enabled")
            except ImportError:
                use_websocket_handler = False
                print("WebSocket support: geventwebsocket NOT installed")
                print("  Install with: pip install gevent-websocket")
            
            # NS: Custom handler to suppress SSL error tracebacks completely
            # These happen when users close browser tabs - totally normal
            if use_websocket_handler:
                class QuietWebSocketHandler(WebSocketHandler):
                    def handle_one_response(self):
                        try:
                            return super().handle_one_response()
                        except Exception as e:
                            # Silently ignore SSL errors (user closed tab)
                            if 'ssl' in type(e).__name__.lower() or 'ssl' in str(e).lower():
                                return
                            raise
                    
                    def log_error(self, msg, *args):
                        # Suppress SSL-related errors
                        if 'ssl' in str(msg).lower() or 'eof' in str(msg).lower():
                            return
                        super().log_error(msg, *args)
            else:
                QuietWebSocketHandler = None
            
            # Custom error handler to suppress SSL errors (from bots/scanners/disconnects)
            class QuietWSGIServer(WSGIServer):
                def wrap_socket_and_handle(self, client_socket, address):
                    """Override to catch SSL errors during handshake"""
                    try:
                        return super().wrap_socket_and_handle(client_socket, address)
                    except Exception as e:
                        # Silently ignore ALL SSL-related errors
                        if 'ssl' in str(type(e).__name__).lower() or 'ssl' in str(e).lower():
                            pass
                        else:
                            raise
                
                def handle_error(self, *args):
                    """Suppress SSL errors - they're normal with self-signed certs"""
                    exc_info = sys.exc_info()
                    exc_type = exc_info[0]
                    if exc_type is not None:
                        # Suppress all SSL-related errors
                        if 'ssl' in exc_type.__name__.lower():
                            return
                    # Don't call super to avoid tracebacks
                    pass
                
                def log_error(self, msg, *args):
                    """Suppress SSL error logging"""
                    msg_lower = str(msg).lower()
                    if 'ssl' in msg_lower or 'eof' in msg_lower or 'broken pipe' in msg_lower:
                        return
                    # Only print non-SSL errors
                    print(f"[Server Error] {msg % args if args else msg}")
            
            # DualProtocolWSGIServer - HTTP and HTTPS on same port
            # If someone visits http://server:5000, they get redirected to https://server:5000
            # MK: Claude helped with the TLS detection logic - checking for 0x16/0x80 bytes
            class DualProtocolWSGIServer(QuietWSGIServer):
                """WSGI Server that detects HTTP vs HTTPS and redirects HTTP to HTTPS
                
                The idea is simple: peek at the first byte of the connection.
                TLS always starts with 0x16 (handshake) or 0x80 (legacy SSLv2).
                Plain HTTP starts with an ASCII letter like 'G' (GET) or 'P' (POST).
                """
                
                def __init__(self, *args, redirect_domain=None, **kwargs):
                    self._redirect_domain = redirect_domain
                    super().__init__(*args, **kwargs)
                
                def wrap_socket_and_handle(self, client_socket, address):
                    """Peek at first bytes to detect protocol"""
                    if self.ssl_context is None:
                        # No SSL, just handle normally
                        return super().wrap_socket_and_handle(client_socket, address)
                    
                    try:
                        # Peek at first byte to detect protocol
                        # TLS ClientHello always starts with 0x16 (handshake) or 0x80 (SSLv2)
                        first_byte = client_socket.recv(1, socket.MSG_PEEK)
                        
                        if not first_byte:
                            client_socket.close()
                            return
                        
                        if first_byte[0] == 0x16 or first_byte[0] == 0x80:
                            # This is TLS/SSL, proceed with normal SSL handling
                            return super().wrap_socket_and_handle(client_socket, address)
                        else:
                            # This is plain HTTP, send redirect to HTTPS
                            self._handle_http_redirect(client_socket, address)
                            return
                            
                    except Exception as e:
                        # On any error, try normal handling
                        if 'ssl' in str(type(e).__name__).lower():
                            return
                        try:
                            return super().wrap_socket_and_handle(client_socket, address)
                        except:
                            pass
                
                def _handle_http_redirect(self, client_socket, address):
                    """Send HTTP 301 redirect to HTTPS version"""
                    try:
                        # Set timeout for reading
                        client_socket.settimeout(5.0)
                        
                        # Read the HTTP request (we need the path and Host header)
                        request_data = b''
                        while b'\r\n\r\n' not in request_data and len(request_data) < 8192:
                            chunk = client_socket.recv(1024)
                            if not chunk:
                                break
                            request_data += chunk
                        
                        request = request_data.decode('utf-8', errors='ignore')
                        
                        # Parse path from first line (e.g., "GET /path HTTP/1.1")
                        path = '/'
                        if request:
                            first_line = request.split('\r\n')[0]
                            parts = first_line.split(' ')
                            if len(parts) >= 2:
                                path = parts[1]
                        
                        # Parse Host header - MK: Fixed double port bug
                        host = self._redirect_domain or 'localhost'
                        host_port = None  # Port from Host header (if any)
                        
                        for line in request.split('\r\n'):
                            if line.lower().startswith('host:'):
                                host_value = line.split(':', 1)[1].strip()
                                # Check if host contains a port (but be careful with IPv6)
                                if host_value.startswith('['):
                                    # IPv6 address like [::1]:5000
                                    if ']:' in host_value:
                                        host = host_value.rsplit(':', 1)[0]
                                        host_port = host_value.rsplit(':', 1)[1]
                                    else:
                                        host = host_value
                                elif ':' in host_value:
                                    # IPv4 or hostname with port like 192.168.1.1:5000
                                    host = host_value.rsplit(':', 1)[0]
                                    host_port = host_value.rsplit(':', 1)[1]
                                else:
                                    host = host_value
                                break
                        
                        # Use configured domain if set (also strip port if present)
                        if self._redirect_domain:
                            domain = self._redirect_domain
                            if ':' in domain and not domain.startswith('['):
                                host = domain.rsplit(':', 1)[0]
                            else:
                                host = domain
                        
                        # Build redirect URL - use server port
                        server_port = self.server_port
                        if server_port == 443:
                            redirect_url = f'https://{host}{path}'
                        else:
                            redirect_url = f'https://{host}:{server_port}{path}'
                        
                        # Send redirect response
                        response = (
                            f"HTTP/1.1 301 Moved Permanently\r\n"
                            f"Location: {redirect_url}\r\n"
                            f"Content-Type: text/html\r\n"
                            f"Content-Length: 0\r\n"
                            f"Connection: close\r\n"
                            f"\r\n"
                        )
                        client_socket.sendall(response.encode())
                        
                    except Exception as e:
                        pass  # Ignore errors during redirect
                    finally:
                        try:
                            client_socket.close()
                        except:
                            pass
            
            # Server args - add WebSocket handler if available
            server_kwargs = {'log': None}
            if use_websocket_handler and QuietWebSocketHandler:
                # LW: Use our quiet handler that suppresses SSL errors
                server_kwargs['handler_class'] = QuietWebSocketHandler
            
            # NS: Set environment to suppress gevent traceback printing
            os.environ['GEVENT_DEBUG'] = 'off'
            
            if ssl_context:
                print(f"HTTPS on https://0.0.0.0:{port}")
                # MK: Temporarily disabled DualProtocolWSGIServer due to connection issues
                # HTTP→HTTPS redirect on same port is disabled for now
                # Use separate port 80 redirect if needed (see http_redirect_port setting)
                ssl_ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
                ssl_ctx.load_cert_chain(ssl_context[0], ssl_context[1])
                
                # Use regular HTTPS server (no auto-detect HTTP on same port)
                http_server = QuietWSGIServer(
                    ('0.0.0.0', port), app, 
                    ssl_context=ssl_ctx,
                    **server_kwargs
                )
            else:
                print(f"HTTP on http://0.0.0.0:{port}")
                print("WARNING: Running without HTTPS - noVNC console may not work!")
                http_server = QuietWSGIServer(('0.0.0.0', port), app, **server_kwargs)
            
            # VNC on port+1, Shell on port+2 (simple, no config needed)
            vnc_ws_port = port + 1
            ssh_ws_port = port + 2
            
            # Start dedicated VNC WebSocket server
            # Separate server needed because Flask-Sock doesn't work with Gevent in production
            if ssl_context:
                start_vnc_websocket_server(vnc_ws_port, ssl_cert=ssl_context[0], ssl_key=ssl_context[1])
            else:
                start_vnc_websocket_server(vnc_ws_port)
            
            # Start dedicated SSH WebSocket server
            if ssl_context:
                start_ssh_websocket_server(ssh_ws_port, ssl_cert=ssl_context[0], ssl_key=ssl_context[1])
            else:
                start_ssh_websocket_server(ssh_ws_port)
            
            # Handle graceful shutdown
            def signal_handler(signum, frame):
                print("\nShutting down gracefully...")
                http_server.stop()
                sys.exit(0)
            
            signal.signal(signal.SIGINT, signal_handler)
            signal.signal(signal.SIGTERM, signal_handler)
            
            # NS: these errors are mostly from bots/scanners, not real issues
            print("SSL/WebSocket errors (bots, scanners, disconnects) are suppressed")
            http_server.serve_forever()
        else:
            print("WARNING: Gevent not installed. Install with: pip install gevent")
            print("Falling back to Flask development server")
    
    # Fallback to Flask development server
    print(f"Starting PegaProx with Flask development server")
    print("WARNING: Not recommended for production with 100+ clusters!")
    print("Install gevent for better performance: pip install gevent")
    
    # VNC on port+1, Shell on port+2 (simple, no config needed)
    vnc_ws_port = port + 1
    ssh_ws_port = port + 2
    
    if ssl_context:
        print(f"Starting VNC WebSocket server on port {vnc_ws_port} (with SSL)")
        start_vnc_websocket_server(vnc_ws_port, ssl_cert=ssl_context[0], ssl_key=ssl_context[1])
        print(f"Starting SSH WebSocket server on port {ssh_ws_port} (with SSL)")
        start_ssh_websocket_server(ssh_ws_port, ssl_cert=ssl_context[0], ssl_key=ssl_context[1])
    else:
        print(f"Starting VNC WebSocket server on port {vnc_ws_port}")
        start_vnc_websocket_server(vnc_ws_port)
        print(f"Starting SSH WebSocket server on port {ssh_ws_port}")
        start_ssh_websocket_server(ssh_ws_port)
    
    if ssl_context:
        print(f"HTTPS on https://0.0.0.0:{port}")
        if domain:
            print(f"Configured domain: {domain}")
        app.run(host='0.0.0.0', port=port, debug=False, ssl_context=ssl_context, threaded=True)
    else:
        print(f"HTTP on http://0.0.0.0:{port}")
        print("WARNING: Running without HTTPS - noVNC console may not work!")
        app.run(host='0.0.0.0', port=port, debug=False, threaded=True)


def print_system_requirements():
    """Print recommended system requirements"""
    print("""
╔══════════════════════════════════════════════════════════════════════════════╗
║                    PegaProx System Requirements Guide                         ║
║                           Version 0.6 Beta - Jan 2026                         ║
╠══════════════════════════════════════════════════════════════════════════════╣
║  Clusters │ Concurrent │  CPU    │  RAM   │  Disk  │  Notes                  ║
║           │   Users    │ Cores   │        │        │                         ║
╠══════════════════════════════════════════════════════════════════════════════╣
║  1-5      │  1-5       │ 1 core  │  1 GB  │  1 GB  │  Testing/Home Lab       ║
║  5-20     │  5-10      │ 2 cores │  2 GB  │  5 GB  │  Small Production       ║
║  20-50    │  10-25     │ 4 cores │  4 GB  │ 10 GB  │  Medium Production      ║
║  50-100   │  25-50     │ 4 cores │  8 GB  │ 20 GB  │  Large Production       ║
║  100-200  │  50-100    │ 8 cores │ 16 GB  │ 50 GB  │  Enterprise             ║
║  200+     │  100+      │ 16 cores│ 32 GB  │100 GB  │  Large Enterprise       ║
╠══════════════════════════════════════════════════════════════════════════════╣
║  Performance Tips:                                                            ║
║  • Install gevent: pip install gevent (2-3x better concurrency)              ║
║  • Set workers: PEGAPROX_WORKERS=<cpu_count>                                 ║
║  • Use SSD for config storage (faster JSON read/write)                       ║
║  • Place behind nginx/haproxy for SSL termination & load balancing           ║
║  • Enable gzip compression in reverse proxy                                  ║
║  • Use Redis for session storage in multi-node setups (future)               ║
╠══════════════════════════════════════════════════════════════════════════════╣
║  Network Requirements:                                                        ║
║  • Port 5000: Main API & Web UI (configurable via PEGAPROX_PORT)             ║
║  • Port 5001: VNC WebSocket (noVNC console) - auto: main_port + 1            ║
║  • Port 5002: SSH WebSocket (Node shell) - auto: main_port + 2               ║
║  • HTTPS recommended (--ssl-cert/--ssl-key or auto-generated)                ║
║  • Access to all Proxmox nodes on port 8006                                  ║
║  • Self-signed certs: Users must accept cert on ports 5001/5002 separately   ║
╠══════════════════════════════════════════════════════════════════════════════╣
║  Dependencies:                                                                ║
║  • Python 3.8+ (3.10+ recommended)                                           ║
║  • Flask, flask-sock, requests, urllib3                                      ║
║  • paramiko (for SSH shell)                                                  ║
║  • websockets (for VNC and SSH WebSocket servers)                            ║
║  • gevent (optional, for better performance)                                 ║
║  • websocket-client (for Proxmox VNC proxy)                                  ║
╚══════════════════════════════════════════════════════════════════════════════╝
""")


def download_static_files():
    """Download all required static files for offline operation."""
    import urllib.request
    import ssl
    
    print("=" * 60)
    print("PegaProx Static Files Downloader")
    print("=" * 60)
    print()
    
    # Files to download - NS: all libs needed for offline mode
    # Note: noVNC is loaded from Proxmox directly (no download needed)
    # Tailwind CDN is a JIT compiler - for offline we use a pre-built CSS subset
    # NS: switched from unpkg to jsdelivr - much faster!
    static_files = {
        'js': [
            ('react.production.min.js', 'https://cdn.jsdelivr.net/npm/react@18/umd/react.production.min.js'),
            ('react-dom.production.min.js', 'https://cdn.jsdelivr.net/npm/react-dom@18/umd/react-dom.production.min.js'),
            ('babel.min.js', 'https://cdn.jsdelivr.net/npm/@babel/standalone@7/babel.min.js'),
            ('xterm.min.js', 'https://cdn.jsdelivr.net/npm/xterm@5.3.0/lib/xterm.min.js'),
            ('xterm-addon-fit.min.js', 'https://cdn.jsdelivr.net/npm/xterm-addon-fit@0.8.0/lib/xterm-addon-fit.min.js'),
        ],
        'css': [
            ('xterm.min.css', 'https://cdn.jsdelivr.net/npm/xterm@5.3.0/css/xterm.min.css'),
        ]
    }
    
    # Create directories
    os.makedirs('static/js', exist_ok=True)
    os.makedirs('static/css', exist_ok=True)
    
    # SSL context (for corporate proxies)
    ctx = ssl.create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE
    
    success = 0
    failed = 0
    
    for subdir, files in static_files.items():
        print(f"Downloading {subdir} files...")
        for filename, url in files:
            dest = f'static/{subdir}/{filename}'
            print(f"  {filename}...", end=' ')
            try:
                req = urllib.request.Request(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                with urllib.request.urlopen(req, timeout=30, context=ctx) as response:
                    data = response.read()
                with open(dest, 'wb') as f:
                    f.write(data)
                print(f"OK ({len(data):,} bytes)")
                success += 1
            except Exception as e:
                print(f"FAILED: {e}")
                failed += 1
    
    # Create Tailwind CSS subset
    print("\nCreating Tailwind CSS subset...")
    tailwind_css = '''/* PegaProx Tailwind Offline CSS */
*, ::before, ::after { box-sizing: border-box; border-width: 0; border-style: solid; }
html { line-height: 1.5; font-family: ui-sans-serif, system-ui, -apple-system, sans-serif; }
body { margin: 0; }
.flex { display: flex; } .grid { display: grid; } .hidden { display: none; } .block { display: block; }
.inline-flex { display: inline-flex; } .flex-col { flex-direction: column; } .flex-1 { flex: 1 1 0%; }
.items-center { align-items: center; } .items-start { align-items: flex-start; }
.justify-center { justify-content: center; } .justify-between { justify-content: space-between; }
.gap-1 { gap: 0.25rem; } .gap-2 { gap: 0.5rem; } .gap-3 { gap: 0.75rem; } .gap-4 { gap: 1rem; } .gap-6 { gap: 1.5rem; }
.p-1 { padding: 0.25rem; } .p-2 { padding: 0.5rem; } .p-3 { padding: 0.75rem; } .p-4 { padding: 1rem; } .p-5 { padding: 1.25rem; } .p-6 { padding: 1.5rem; }
.px-2 { padding-left: 0.5rem; padding-right: 0.5rem; } .px-3 { padding-left: 0.75rem; padding-right: 0.75rem; } .px-4 { padding-left: 1rem; padding-right: 1rem; }
.py-1 { padding-top: 0.25rem; padding-bottom: 0.25rem; } .py-2 { padding-top: 0.5rem; padding-bottom: 0.5rem; } .py-3 { padding-top: 0.75rem; padding-bottom: 0.75rem; }
.m-0 { margin: 0; } .mx-auto { margin-left: auto; margin-right: auto; }
.mt-1 { margin-top: 0.25rem; } .mt-2 { margin-top: 0.5rem; } .mt-4 { margin-top: 1rem; }
.mb-1 { margin-bottom: 0.25rem; } .mb-2 { margin-bottom: 0.5rem; } .mb-4 { margin-bottom: 1rem; }
.w-full { width: 100%; } .w-auto { width: auto; } .w-4 { width: 1rem; } .w-5 { width: 1.25rem; } .w-6 { width: 1.5rem; } .w-8 { width: 2rem; }
.h-full { height: 100%; } .h-4 { height: 1rem; } .h-5 { height: 1.25rem; } .h-6 { height: 1.5rem; } .h-8 { height: 2rem; }
.min-h-screen { min-height: 100vh; }
.text-xs { font-size: 0.75rem; } .text-sm { font-size: 0.875rem; } .text-lg { font-size: 1.125rem; } .text-xl { font-size: 1.25rem; } .text-2xl { font-size: 1.5rem; }
.font-medium { font-weight: 500; } .font-semibold { font-weight: 600; } .font-bold { font-weight: 700; }
.font-mono { font-family: ui-monospace, monospace; }
.text-center { text-align: center; } .text-right { text-align: right; }
.uppercase { text-transform: uppercase; } .truncate { overflow: hidden; text-overflow: ellipsis; white-space: nowrap; }
.rounded { border-radius: 0.25rem; } .rounded-lg { border-radius: 0.5rem; } .rounded-xl { border-radius: 0.75rem; } .rounded-full { border-radius: 9999px; }
.border { border-width: 1px; } .border-2 { border-width: 2px; } .border-t { border-top-width: 1px; } .border-b { border-bottom-width: 1px; }
.opacity-50 { opacity: 0.5; } .opacity-75 { opacity: 0.75; }
.shadow { box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1); } .shadow-lg { box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1); }
.overflow-hidden { overflow: hidden; } .overflow-auto { overflow: auto; } .overflow-y-auto { overflow-y: auto; }
.relative { position: relative; } .absolute { position: absolute; } .fixed { position: fixed; }
.inset-0 { top: 0; right: 0; bottom: 0; left: 0; } .top-0 { top: 0; } .right-0 { right: 0; } .bottom-0 { bottom: 0; } .left-0 { left: 0; }
.z-10 { z-index: 10; } .z-50 { z-index: 50; }
.cursor-pointer { cursor: pointer; }
.transition { transition-property: all; transition-duration: 150ms; } .transition-all { transition-property: all; transition-duration: 150ms; }
.animate-spin { animation: spin 1s linear infinite; } .animate-pulse { animation: pulse 2s ease-in-out infinite; }
@keyframes spin { from { transform: rotate(0deg); } to { transform: rotate(360deg); } }
@keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
.grid-cols-1 { grid-template-columns: repeat(1, minmax(0, 1fr)); } .grid-cols-2 { grid-template-columns: repeat(2, minmax(0, 1fr)); } .grid-cols-3 { grid-template-columns: repeat(3, minmax(0, 1fr)); }
.col-span-2 { grid-column: span 2 / span 2; } .col-span-3 { grid-column: span 3 / span 3; }
.space-y-2 > :not([hidden]) ~ :not([hidden]) { margin-top: 0.5rem; } .space-y-4 > :not([hidden]) ~ :not([hidden]) { margin-top: 1rem; }
.divide-y > :not([hidden]) ~ :not([hidden]) { border-top-width: 1px; }
@media (min-width: 768px) { .md\\:grid-cols-2 { grid-template-columns: repeat(2, minmax(0, 1fr)); } }
@media (min-width: 1280px) { .xl\\:grid-cols-3 { grid-template-columns: repeat(3, minmax(0, 1fr)); } .xl\\:col-span-2 { grid-column: span 2 / span 2; } }
'''
    try:
        with open('static/css/tailwind.min.css', 'w') as f:
            f.write(tailwind_css)
        print("  tailwind.min.css... OK")
        success += 1
    except Exception as e:
        print(f"  tailwind.min.css... FAILED: {e}")
        failed += 1
    
    # Download noVNC for offline VNC console
    # NS: noVNC uses ES6 modules with relative imports, we need to rewrite them
    print("\nDownloading noVNC for offline VNC console...")
    novnc_base = 'https://cdn.jsdelivr.net/npm/@novnc/novnc@1.4.0'
    novnc_files = [
        # Core
        'core/rfb.js',
        'core/display.js',
        'core/inflator.js',
        'core/deflator.js',
        'core/websock.js',
        'core/encodings.js',
        'core/des.js',
        'core/ra2.js',
        'core/base64.js',
        # Decoders
        'core/decoders/copyrect.js',
        'core/decoders/hextile.js',
        'core/decoders/raw.js',
        'core/decoders/rre.js',
        'core/decoders/tight.js',
        'core/decoders/tightpng.js',
        'core/decoders/zrle.js',
        'core/decoders/jpeg.js',
        # Input
        'core/input/keyboard.js',
        'core/input/keysym.js',
        'core/input/keysymdef.js',
        'core/input/gesturehandler.js',
        'core/input/domkeytable.js',
        'core/input/util.js',
        'core/input/vkeys.js',
        'core/input/xtscancodes.js',
        'core/input/fixedkeys.js',
        # Util
        'core/util/browser.js',
        'core/util/cursor.js',
        'core/util/element.js',
        'core/util/events.js',
        'core/util/eventtarget.js',
        'core/util/int.js',
        'core/util/logging.js',
        'core/util/strings.js',
        'core/util/md5.js',
        # Vendor - pako zlib
        'vendor/pako/lib/zlib/inflate.js',
        'vendor/pako/lib/zlib/zstream.js',
        'vendor/pako/lib/zlib/deflate.js',
        'vendor/pako/lib/zlib/messages.js',
        'vendor/pako/lib/zlib/trees.js',
        'vendor/pako/lib/zlib/adler32.js',
        'vendor/pako/lib/zlib/crc32.js',
        'vendor/pako/lib/zlib/inffast.js',
        'vendor/pako/lib/zlib/inftrees.js',
        'vendor/pako/lib/utils/common.js',
    ]
    
    # Create noVNC directory structure
    for subdir in ['core', 'core/decoders', 'core/input', 'core/util', 
                   'vendor/pako/lib/zlib', 'vendor/pako/lib/utils']:
        os.makedirs(f'static/js/novnc/{subdir}', exist_ok=True)
    
    novnc_success = 0
    novnc_failed = 0
    
    for filepath in novnc_files:
        url = f"{novnc_base}/{filepath}"
        dest = f"static/js/novnc/{filepath}"
        filename = filepath.split('/')[-1]
        print(f"  {filename}...", end=' ')
        try:
            req = urllib.request.Request(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            with urllib.request.urlopen(req, timeout=30, context=ctx) as response:
                content = response.read().decode('utf-8')
            
            # Rewrite relative imports to absolute paths
            # from './xxx.js' -> from '/static/js/novnc/core/xxx.js'
            # from '../xxx.js' -> from '/static/js/novnc/xxx.js'
            import re
            
            # Get the directory of the current file for resolving relative paths
            file_dir = '/'.join(filepath.split('/')[:-1])  # e.g. 'core' or 'core/decoders'
            
            # Pattern to match: from './something.js' or from "../something.js"
            pattern = r'''from\s+(['"])(\.{1,2}/[^'"]+)\1'''
            
            def rewrite_import(match):
                quote = match.group(1)
                rel_path = match.group(2)
                
                # Resolve the relative path
                if rel_path.startswith('./'):
                    # Same directory
                    resolved = f"/static/js/novnc/{file_dir}/{rel_path[2:]}"
                elif rel_path.startswith('../'):
                    # Parent directory - need to go up
                    parts = file_dir.split('/') if file_dir else []
                    rest = rel_path
                    while rest.startswith('../'):
                        if parts:
                            parts.pop()
                        rest = rest[3:]
                    parent = '/'.join(parts)
                    resolved = f"/static/js/novnc/{parent}/{rest}" if parent else f"/static/js/novnc/{rest}"
                else:
                    resolved = rel_path
                
                # Clean up double slashes
                while '//' in resolved:
                    resolved = resolved.replace('//', '/')
                return f"from {quote}{resolved}{quote}"
            
            content = re.sub(pattern, rewrite_import, content)
            
            with open(dest, 'w') as f:
                f.write(content)
            print(f"OK")
            novnc_success += 1
            success += 1
        except Exception as e:
            print(f"FAILED: {e}")
            novnc_failed += 1
            failed += 1
    
    # Create a simple entry point that re-exports RFB
    rfb_entry = '''// noVNC entry point for PegaProx offline mode
// Auto-generated by --download-static
export { default } from '/static/js/novnc/core/rfb.js';
export * from '/static/js/novnc/core/rfb.js';
'''
    try:
        with open('static/js/novnc/rfb.min.js', 'w') as f:
            f.write(rfb_entry)
        print("  rfb.min.js (entry point)... OK")
        success += 1
    except Exception as e:
        print(f"  rfb.min.js... FAILED: {e}")
        failed += 1
    
    print(f"\n  noVNC: {novnc_success}/{len(novnc_files)} files downloaded")
    
    print()
    print("=" * 60)
    print(f"Done: {success} succeeded, {failed} failed")
    print("=" * 60)
    
    if failed == 0:
        print("\n✓ All static files downloaded!")
        print("  PegaProx can run fully offline now (including VNC console)")
    else:
        print("\n⚠ some downloads failed, will use cdn fallback")
    
    return failed == 0


# main entry point
# LW: keep this simple, dont add too much stuff here
if __name__ == '__main__':
    if '--requirements' in sys.argv:
        print_system_requirements()
    elif '--download-static' in sys.argv:
        download_static_files()
    elif '--help' in sys.argv or '-h' in sys.argv:
        print("""
PegaProx Server

Usage:
  python pegaprox_server.py [options]

Options:
  --debug           verbose logging
  --requirements    show requirements
  --download-static download js libs for offline mode (React, Babel, xterm, noVNC)
  --help, -h        this message

Env vars:
  PEGAPROX_ALLOWED_ORIGINS  cors origins
  PEGAPROX_MAX_REQUEST_SIZE max request size (default 10MB)
  PEGAPROX_HTTP_PORT        http port for redirect (default 80)
        """)
    else:
        debug_mode = '--debug' in sys.argv
        # print(f"starting with debug={debug_mode}")  # DEBUG
        main(debug_mode=debug_mode)

# thats it - NS